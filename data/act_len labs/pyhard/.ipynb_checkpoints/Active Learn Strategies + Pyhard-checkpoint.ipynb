{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning - Comparando estratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amostra por incerteza\n",
    "- Amostragem aleatória\n",
    "- Consulta por comitê\n",
    "- Aprendizado passivo\n",
    "- Redução do erro esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run set_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing_libraries.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets OpenML\n",
    "import openml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml.config.cache_directory = os.path.expanduser('./datasets/openML')\n",
    "openml_list = openml.datasets.list_datasets()\n",
    "\n",
    "datalist = pd.DataFrame.from_dict(openml_list, orient=\"index\")\n",
    "datalist = list(datalist[(datalist.NumberOfClasses.isnull() == False) & (datalist.NumberOfClasses != 0)][\"did\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostra por incerteza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertain_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][TRAIN][initial_idx]]\n",
    "    #X_test, y_test = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    #cls = which_classifier(classifier)\n",
    "    #cls.fit(X_train,y_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "        \n",
    "        idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "        X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "        \n",
    "        if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "            #print(\"IF\", learner.score(X_test, y_test))\n",
    "            sample_size = sample_size + len(X_train)\n",
    "            learner.teach(X_train, y_train)\n",
    "            uncertain_sample_score = learner.score(X_test, y_test)\n",
    "            performance_history.append(uncertain_sample_score)\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Uncertain Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostragem aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "        \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "\n",
    "    for i in range(1, cost+1):\n",
    "\n",
    "        #high = X_raw.shape[0] = qtd amostras no dataset\n",
    "        #training_indices = np.random.randint(low=0, high=len(X_raw[idx_data[idx_bag][TRAIN]]), size=k+i) #high = qtd elementos na bag\n",
    "        #sample_size = sample_size + len(training_indices)\n",
    "        #X_train = X_raw[idx_data[idx_bag][TRAIN][training_indices]] #ASK06\n",
    "        #y_train = y_raw[idx_data[idx_bag][TRAIN][training_indices]]\n",
    "        #X_test = np.delete(X_raw, idx_data[idx_bag][TRAIN][training_indices], axis=0)\n",
    "        #y_test = np.delete(y_raw, idx_data[idx_bag][TRAIN][training_indices], axis=0)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "        \n",
    "        cls = which_classifier(classifier)\n",
    "        cls.fit(X_train, y_train)\n",
    "\n",
    "        random_sampling_score = cls.score(X_test,y_test)\n",
    "        performance_history.append(random_sampling_score)\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw),\n",
    "             \"Strategy\": \"Random Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta por comitê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_by_committee(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.models import ActiveLearner, Committee\n",
    "    from modAL.disagreement import vote_entropy_sampling\n",
    "\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "\n",
    "    learner_list = []\n",
    "\n",
    "    for j in range(1, cost+1): # Loop para criação do comitê\n",
    "\n",
    "        X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "\n",
    "        # initializing learner\n",
    "        learner = ActiveLearner(\n",
    "            estimator= which_classifier(classifier),\n",
    "            X_training = X_train, y_training = y_train \n",
    "        )\n",
    "        learner_list.append(learner)\n",
    "\n",
    "    # assembling the committee\n",
    "    committee = Committee(\n",
    "        learner_list=learner_list,\n",
    "        query_strategy=vote_entropy_sampling)\n",
    "\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]]\n",
    "    \n",
    "    # query by committee\n",
    "    for idx in range(cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        query_idx, query_instance = committee.query(X_pool, n_instances = init_size+1)\n",
    "        sample_size = sample_size + len(query_idx)\n",
    "        \n",
    "        committee.teach(\n",
    "            X = X_pool[query_idx],\n",
    "            y = y_pool[query_idx]\n",
    "        )\n",
    "\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx)\n",
    "        \n",
    "        query_by_committee_score = committee.score(X_pool, y_pool)\n",
    "        performance_history.append(query_by_committee_score)\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw),\n",
    "             \"Strategy\": \"Query by Committee\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mX_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhich_arff_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "%run -i main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Error Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_error_reduction(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][TRAIN][initial_idx]]\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    exp_er_score = learner.score(X_pool, y_pool)\n",
    "    performance_history.append(exp_er_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = expected_error_reduction(learner, X_pool, 'binary', n_instances=init_size)[0]\n",
    "\n",
    "        learner.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        sample_size = sample_size + init_size\n",
    "    \n",
    "        X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, exp_error_idx)\n",
    "        \n",
    "        exp_er_score = learner.score(X_pool, y_pool)\n",
    "        performance_history.append(exp_er_score)\n",
    "        \n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Expected Error Reduction\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Model Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_model_change(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][0][initial_idx]]\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    \n",
    "#     performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = np.random.choice(range(len(X_pool)), size=init_size, replace=False)\n",
    "        aux = deepcopy(learner)\n",
    "\n",
    "        aux.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        score_aux = aux.score(X_pool, y_pool)\n",
    "        score_learner = learner.score(X_pool, y_pool)\n",
    "\n",
    "        if score_aux > score_learner:\n",
    "            learner = deepcopy(aux)\n",
    "            sample_size = sample_size + init_size\n",
    "        \n",
    "        X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, exp_error_idx, axis=0)\n",
    "        \n",
    "        exp_mo_score = learner.score(X_pool, y_pool)\n",
    "        performance_history.append(exp_mo_score)\n",
    "\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Expected Model Change\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query highest LSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_lsc_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure('LSC')\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by='feature_LSC', ascending=False)['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Highest LSC Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query highest Usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_usefulness_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    print(type(X_raw[idx_data[idx_bag][TRAIN]]))\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure('Usefulness')\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by='feature_Usefulness', ascending=False)['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Highest Usefulness Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query lowest Harmfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_harmfulness_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    print(type(X_raw[idx_data[idx_bag][TRAIN]]))\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure('Harmfulness')\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by='feature_Harmfulness', ascending=True)['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Lowest Harmfulness Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query lowest H and Highest U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_h_highest_u_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    print(type(X_raw[idx_data[idx_bag][TRAIN]]))\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure('U+H')\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    # selecionar amostras que sao ao mesmo tempo altos U e baixos H\n",
    "    idx = list(df.sort_values(by=['feature_Usefulness','feature_Harmfulness'], ascending=[False, True])['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Lowest H, Highest U Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query lowest N2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_n2_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure('N2')\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by='feature_N2', ascending=True)['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Lowest N2 Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_pyhard_measure(measure='LSC'):\n",
    "    import yaml\n",
    "    with open(r'config-template.yaml') as file:\n",
    "        configs_list = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "        if measure == 'LSC':\n",
    "            configs_list['measures_list'] = ['LSC']\n",
    "        elif measure == 'Harmfulness':\n",
    "            configs_list['measures_list'] = ['Harmfulness']\n",
    "        elif measure == 'Usefulness':\n",
    "            configs_list['measures_list'] = ['Usefulness']\n",
    "        elif measure == 'U+H':\n",
    "            configs_list['measures_list'] = ['Harmfulness','Usefulness']\n",
    "        elif measure == 'N2':\n",
    "            configs_list['measures_list'] = ['N2']\n",
    "\n",
    "    with open(r'config.yaml', 'w') as file:\n",
    "        yaml.dump(configs_list, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_dataset(dataset = \"iris\", n_splits = 5):\n",
    "    \n",
    "    # Futuramente essa etapa será ajustada para receber qualquer dataset (ou lista com datasets)\n",
    "    if (dataset == \"iris\"):\n",
    "        data = load_iris()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "    \n",
    "    if (dataset == \"wine\"):\n",
    "        data = load_wine()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    if (dataset == \"digits\"):\n",
    "        data = load_digits()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_oml_dataset(dataset_id, n_splits = 5):\n",
    "    data = openml.datasets.get_dataset(dataset_id)\n",
    "    \n",
    "    X_raw, y_raw, categorical_indicator, attribute_names = data.get_data(\n",
    "    dataset_format=\"array\", target=data.default_target_attribute)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_raw)\n",
    "    y_raw = le.transform(y_raw)\n",
    "    \n",
    "    X_raw = np.nan_to_num(X_raw)\n",
    "    \n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_arff_dataset(dataset, n_splits = 5):\n",
    "   \n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    \n",
    "    data = arff.loadarff('datasets/luis/' + dataset)\n",
    "    data = pd.DataFrame(data[0])\n",
    "\n",
    "    X_raw = data[data.columns[:-1]].to_numpy()\n",
    "    y_raw = data[data.columns[-1]].to_numpy()\n",
    "    \n",
    "    lex = preprocessing.OrdinalEncoder()\n",
    "    lex.fit(X_raw)\n",
    "    X_raw = lex.transform(X_raw)\n",
    "        \n",
    "    ley = preprocessing.LabelEncoder()\n",
    "    ley.fit(y_raw)\n",
    "    y_raw = ley.transform(y_raw)\n",
    "    \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    data_cv.get_n_splits(X_raw,y_raw)\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw, y_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_classifier(classifier = '5NN'):\n",
    "    \n",
    "    if (classifier == '5NN'):\n",
    "        return KNeighborsClassifier(5)\n",
    "    elif (classifier == 'C4.5'):\n",
    "        return tree.DecisionTreeClassifier()\n",
    "    elif (classifier == 'NB'):\n",
    "        return GaussianNB()\n",
    "    elif (classifier == 'SVM'):\n",
    "        return SVC(probability=True, gamma='auto')\n",
    "    elif (classifier == 'RF'):\n",
    "        return RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datasets(dataset):\n",
    "    \n",
    "    data = arff.loadarff('./datasets/luis/' + dataset)\n",
    "    metadata = data[1]\n",
    "    data = pd.DataFrame(data[0])\n",
    "    \n",
    "    instances = len(data)\n",
    "    classes = len(data.iloc[:,-1].value_counts())\n",
    "    attributes = len(data.columns)- 1\n",
    "    nominal_attributes = str(metadata).count(\"nominal\")\n",
    "    \n",
    "    proportion = data.iloc[:,-1].value_counts()\n",
    "    proportion = proportion.map(lambda x: round(x/instances*100,2))\n",
    "\n",
    "    majority = max(proportion)\n",
    "    minority = min(proportion)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"name\": dataset[:-5],\n",
    "        \"instances\": instances,\n",
    "        \"classes\": classes,\n",
    "        \"attributes\": attributes,\n",
    "        \"nominal attributes\": nominal_attributes,\n",
    "        \"majority\": majority,\n",
    "        \"minority\": minority\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Teste\n",
    "\n",
    "total_performance_history = []\n",
    "\n",
    "classifiers = ['SVM']\n",
    "ds = '3_kr-vs-kp.arff'\n",
    "for classifier in classifiers:\n",
    "    X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "    for idx_bag in range(n_splits):\n",
    "        print(ds, classifier, \" \", idx_bag, \" \", n_splits, \" uncertain_sampling\")\n",
    "        result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "        result['dataset'] = ds\n",
    "        total_performance_history.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('./datasets/luis')\n",
    "classifiers = ['5NN', 'C4.5', 'NB','RF']\n",
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['61_iris.arff']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>instances</th>\n",
       "      <th>classes</th>\n",
       "      <th>attributes</th>\n",
       "      <th>nominal attributes</th>\n",
       "      <th>majority</th>\n",
       "      <th>minority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61_iris</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>33.33</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  instances  classes  attributes  nominal attributes  majority  \\\n",
       "0  61_iris        150        3           4                   1     33.33   \n",
       "\n",
       "   minority  \n",
       "0     33.33  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = []\n",
    "\n",
    "for ds in datasets:\n",
    "    metadata.append(fetch_datasets(ds))\n",
    "\n",
    "metadata = pd.DataFrame.from_dict(metadata)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for ds in tqdm(datasets):\n",
    "    for classifier in tqdm(classifiers):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" uncertain_sampling\")\n",
    "            result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" random sampling\")\n",
    "            result = random_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" query_by_committee\")\n",
    "            result = query_by_committee(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" exp error reduction\")\n",
    "            result = exp_error_reduction(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" exp model change\")\n",
    "            result = exp_model_change(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee1f3726f0c44d686ea06ac1707c47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dataset'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d861727b709e449c85478baed0683818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Classifier'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277538756c0f4bfba7d95ae66ee41ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Bag'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "\r",
      "Testando: 61_iris 5NN 0/5 exp_error_reduction\n",
      "\t Size of X_pool: 45\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(1) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testando: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_bag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" exp_error_reduction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_error_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_bag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtotal_performance_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/main.py\u001b[0m in \u001b[0;36mexp_error_reduction\u001b[0;34m(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mexp_error_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected_error_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_instances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp_error_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp_error_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minit_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/modAL/models/learners.py\u001b[0m in \u001b[0;36mteach\u001b[0;34m(self, X, y, bootstrap, only_new, **fit_kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfit\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0monly_new\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_to_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/modAL/models/base.py\u001b[0m in \u001b[0;36m_add_training_data\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mclassifier\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m         check_X_y(X, y, accept_sparse=True, ensure_2d=False, allow_nd=True, multi_output=True, dtype=None,\n\u001b[0m\u001b[1;32m     89\u001b[0m                   force_all_finite=self.force_all_finite)\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    821\u001b[0m                     estimator=estimator)\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n\u001b[0m\u001b[1;32m    824\u001b[0m                         ensure_2d=False, dtype=None)\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0m\u001b[1;32m    203\u001b[0m                             \" a valid collection.\" % x)\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(1) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "#tqdm_datasets = tqdm(datasets, desc=\" Dataset: \"+ str(ds[:-5]))\n",
    "#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in tqdm(classifiers,  desc =\"Classifier\"):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in tqdm(range(n_splits),  desc =\"Bag\"):\n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" uncertain_sampling\")\n",
    "#             result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" uncertain_sampling\")\n",
    "            \n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" random_sampling\")\n",
    "#             result = random_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" random_sampling\")\n",
    "            \n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" query_by_committee\")\n",
    "#             result = query_by_committee(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" query_by_committee\")\n",
    "\n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_error_reduction\")\n",
    "            result = exp_error_reduction(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_error_reduction\")\n",
    "            \n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_model_change\")\n",
    "#             result = exp_model_change(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_model_change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['RF']\n",
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1a2d92fb9747f49655f68109b87f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dataset'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c31df1b2e649df941179cb411e99c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Classifier'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03466832a050436b99517da9c2afeb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Bag'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando: 61_iris RF 0/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:32:33,011 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:32:33,012 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:32:33,014 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:32:33,014 - Building metadata.\n",
      "[INFO] 2021-04-13 22:32:34,871 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 22:32:34,896 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:32:34,897 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:32:34,897 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:32:34,897 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:32:34,898 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.31trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-13 22:32:43,642 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:32:43,642 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:32:43,642 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.39trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:32:49,767 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:32:49,767 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:32:49,767 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.45trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 22:32:55,644 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:32:55,644 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:32:55,645 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.53trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:33:01,399 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:33:01,399 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:33:01,400 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.40trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 22:33:07,507 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:33:07,507 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:33:07,507 - Mean accuracy on test instances (iteration #1): 0.9329\n",
      "[INFO] 2021-04-13 22:33:07,515 - Total elapsed time: 34.5s\n",
      "[INFO] 2021-04-13 22:33:07,515 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 0/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 0/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:33:11,138 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:33:11,139 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:33:11,141 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:33:11,141 - Building metadata.\n",
      "[INFO] 2021-04-13 22:33:12,740 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:33:12,741 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:33:12,742 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:33:12,742 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:33:12,742 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:33:12,743 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.22trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-13 22:33:19,144 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:33:19,144 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:33:19,145 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.24trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:33:25,447 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:33:25,447 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:33:25,448 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.74trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 22:33:32,817 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:33:32,817 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:33:32,818 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.84trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:33:39,978 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:33:39,979 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:33:39,979 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.29trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 22:33:46,214 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:33:46,214 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:33:46,215 - Mean accuracy on test instances (iteration #1): 0.9329\n",
      "[INFO] 2021-04-13 22:33:46,222 - Total elapsed time: 35.1s\n",
      "[INFO] 2021-04-13 22:33:46,222 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 0/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 0/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:33:49,966 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:33:49,966 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:33:49,969 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:33:49,969 - Building metadata.\n",
      "[INFO] 2021-04-13 22:33:52,073 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:33:52,075 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:33:52,075 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:33:52,075 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:33:52,076 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:33:52,077 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.53trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-13 22:34:00,095 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:34:00,095 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:34:00,096 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.34trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:34:06,200 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:34:06,200 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:34:06,201 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.43trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 22:34:12,095 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:34:12,095 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:34:12,096 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:06<00:00,  3.13trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 22:34:18,732 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:34:18,732 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:34:18,732 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.73trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 22:34:26,235 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:34:26,235 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:34:26,236 - Mean accuracy on test instances (iteration #1): 0.9329\n",
      "[INFO] 2021-04-13 22:34:26,245 - Total elapsed time: 36.3s\n",
      "[INFO] 2021-04-13 22:34:26,245 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 0/5 lowest_harmfulness_sampling\n",
      "Testando: 61_iris RF 0/5 lowest_h_highest_u_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:34:30,082 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:34:30,082 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:34:30,084 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:34:30,085 - Building metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-13 22:34:32,033 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:34:32,034 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:34:32,036 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:34:32,036 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:34:32,036 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:34:32,037 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:34:32,037 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.39trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:34:40,616 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:34:40,616 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:34:40,617 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.78trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:34:47,902 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:34:47,902 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:34:47,903 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.81trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 22:34:55,337 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:34:55,337 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:34:55,337 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.94trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:35:02,222 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:35:02,223 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:35:02,223 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.39trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 22:35:08,293 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:35:08,293 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:35:08,293 - Mean accuracy on test instances (iteration #1): 0.9329\n",
      "[INFO] 2021-04-13 22:35:08,301 - Total elapsed time: 38.2s\n",
      "[INFO] 2021-04-13 22:35:08,301 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 0/5 lowest_h_highest_u_sampling\n",
      "Testando: 61_iris RF 0/5 lowest_n2_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:35:12,170 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:35:12,170 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:35:12,173 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:35:12,173 - Building metadata.\n",
      "[INFO] 2021-04-13 22:35:14,185 - Calculating measure 'N2'\n",
      "[INFO] 2021-04-13 22:35:14,251 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:35:14,251 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:35:14,251 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:35:14,252 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:35:14,252 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.64trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-13 22:35:22,012 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:35:22,012 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:35:22,013 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.33trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:35:28,152 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:35:28,152 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:35:28,153 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.51trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 22:35:36,221 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:35:36,221 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:35:36,222 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.98trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:35:46,415 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:35:46,415 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:35:46,417 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.97trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 22:35:56,876 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:35:56,876 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:35:56,876 - Mean accuracy on test instances (iteration #1): 0.9329\n",
      "[INFO] 2021-04-13 22:35:56,888 - Total elapsed time: 44.7s\n",
      "[INFO] 2021-04-13 22:35:56,888 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 0/5 lowest_n2_sampling\n",
      "Testando: 61_iris RF 1/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:36:01,820 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:36:01,820 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:36:01,823 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:36:01,823 - Building metadata.\n",
      "[INFO] 2021-04-13 22:36:04,397 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 22:36:04,439 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:36:04,439 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:36:04,440 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:36:04,440 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:36:04,441 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:12<00:00,  1.63trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 22:36:16,968 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:36:16,968 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:36:16,969 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.48trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 22:36:25,058 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:36:25,058 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:36:25,058 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.61trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:36:32,814 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:36:32,815 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:36:32,816 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.14trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 22:36:42,478 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:36:42,478 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:36:42,478 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.32trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 22:36:51,489 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 22:36:51,489 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:36:51,490 - Mean accuracy on test instances (iteration #1): 0.961\n",
      "[INFO] 2021-04-13 22:36:51,500 - Total elapsed time: 49.7s\n",
      "[INFO] 2021-04-13 22:36:51,500 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 1/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 1/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:36:55,993 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:36:55,994 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:36:55,997 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:36:55,997 - Building metadata.\n",
      "[INFO] 2021-04-13 22:36:58,378 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:36:58,381 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:36:58,381 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:36:58,381 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:36:58,382 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:36:58,383 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:10<00:00,  1.89trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 22:37:09,279 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:37:09,279 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:37:09,280 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.26trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 22:37:18,208 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:37:18,209 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:37:18,209 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.30trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:37:26,937 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:37:26,937 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:37:26,938 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.80trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:37:34,111 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:37:34,111 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:37:34,112 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.56trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 22:37:42,026 - Test fold mean accuracy: 0.85\n",
      "[INFO] 2021-04-13 22:37:42,026 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:37:42,026 - Mean accuracy on test instances (iteration #1): 0.951\n",
      "[INFO] 2021-04-13 22:37:42,035 - Total elapsed time: 46.0s\n",
      "[INFO] 2021-04-13 22:37:42,035 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 1/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 1/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:37:46,361 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:37:46,361 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:37:46,365 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:37:46,365 - Building metadata.\n",
      "[INFO] 2021-04-13 22:37:48,636 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:37:48,638 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:37:48,638 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:37:48,638 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:37:48,639 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:37:48,640 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:08<00:00,  2.23trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 22:37:57,711 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:37:57,711 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:37:57,712 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.53trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 22:38:05,658 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:38:05,658 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:38:05,659 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.87trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:38:12,707 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:38:12,707 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:38:12,708 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.08trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 22:38:19,462 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:38:19,462 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:38:19,463 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.75trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 22:38:26,831 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 22:38:26,831 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:38:26,832 - Mean accuracy on test instances (iteration #1): 0.961\n",
      "[INFO] 2021-04-13 22:38:26,842 - Total elapsed time: 40.5s\n",
      "[INFO] 2021-04-13 22:38:26,842 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 1/5 lowest_harmfulness_sampling\n",
      "Testando: 61_iris RF 1/5 lowest_h_highest_u_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:38:31,392 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:38:31,393 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:38:31,396 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:38:31,396 - Building metadata.\n",
      "[INFO] 2021-04-13 22:38:34,202 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:38:34,204 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:38:34,206 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:38:34,207 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:38:34,207 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:38:34,207 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:38:34,208 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:09<00:00,  2.11trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 22:38:43,875 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:38:43,876 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:38:43,877 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.06trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 22:38:50,452 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:38:50,453 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:38:50,453 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.46trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:38:58,659 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:38:58,659 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:38:58,660 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.62trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:39:06,332 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:39:06,333 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:39:06,334 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.38trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 22:39:15,076 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 22:39:15,077 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:39:15,077 - Mean accuracy on test instances (iteration #1): 0.961\n",
      "[INFO] 2021-04-13 22:39:15,087 - Total elapsed time: 43.7s\n",
      "[INFO] 2021-04-13 22:39:15,087 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 1/5 lowest_h_highest_u_sampling\n",
      "Testando: 61_iris RF 1/5 lowest_n2_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:39:19,186 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:39:19,187 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:39:19,189 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:39:19,189 - Building metadata.\n",
      "[INFO] 2021-04-13 22:39:21,413 - Calculating measure 'N2'\n",
      "[INFO] 2021-04-13 22:39:21,495 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:39:21,495 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:39:21,495 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:39:21,496 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:39:21,497 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:09<00:00,  2.18trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 22:39:31,009 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:39:31,009 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:39:31,010 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.56trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 22:39:38,899 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:39:38,900 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:39:38,901 - Optimizing classifier hyper-parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 20/20 [00:09<00:00,  2.21trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:39:48,050 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:39:48,050 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:39:48,051 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.41trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 22:39:56,458 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:39:56,458 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:39:56,459 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.04trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 22:40:06,555 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 22:40:06,556 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:40:06,556 - Mean accuracy on test instances (iteration #1): 0.9419\n",
      "[INFO] 2021-04-13 22:40:06,566 - Total elapsed time: 47.4s\n",
      "[INFO] 2021-04-13 22:40:06,567 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 1/5 lowest_n2_sampling\n",
      "Testando: 61_iris RF 2/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:40:11,024 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:40:11,025 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:40:11,028 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:40:11,028 - Building metadata.\n",
      "[INFO] 2021-04-13 22:40:13,294 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 22:40:13,331 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:40:13,331 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:40:13,331 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:40:13,332 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:40:13,333 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.28trial/s, best loss: -0.9272486772486772]\n",
      "[INFO] 2021-04-13 22:40:22,190 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:40:22,190 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:40:22,191 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:06<00:00,  3.02trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 22:40:28,862 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:40:28,863 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:40:28,864 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:06<00:00,  2.97trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 22:40:35,795 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:40:35,796 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:40:35,797 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.88trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 22:40:42,802 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:40:42,802 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:40:42,803 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.55trial/s, best loss: -0.9166666666666666]\n",
      "[INFO] 2021-04-13 22:40:50,758 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:40:50,759 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:40:50,759 - Mean accuracy on test instances (iteration #1): 0.9238\n",
      "[INFO] 2021-04-13 22:40:50,769 - Total elapsed time: 39.8s\n",
      "[INFO] 2021-04-13 22:40:50,769 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 2/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 2/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:40:55,236 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:40:55,236 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:40:55,239 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:40:55,239 - Building metadata.\n",
      "[INFO] 2021-04-13 22:40:57,576 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:40:57,579 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:40:57,579 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:40:57,579 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:40:57,581 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:40:57,582 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.89trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 22:41:08,366 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:41:08,366 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:41:08,367 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:08<00:00,  2.24trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 22:41:17,340 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:41:17,340 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:41:17,341 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.59trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:41:25,132 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:41:25,132 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:41:25,133 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.58trial/s, best loss: -0.9629629629629629]\n",
      "[INFO] 2021-04-13 22:41:32,902 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:41:32,902 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:41:32,903 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.36trial/s, best loss: -0.9285714285714285]\n",
      "[INFO] 2021-04-13 22:41:41,470 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:41:41,470 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:41:41,470 - Mean accuracy on test instances (iteration #1): 0.9429\n",
      "[INFO] 2021-04-13 22:41:41,483 - Total elapsed time: 46.3s\n",
      "[INFO] 2021-04-13 22:41:41,483 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 2/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 2/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:41:46,145 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:41:46,146 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:41:46,149 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:41:46,149 - Building metadata.\n",
      "[INFO] 2021-04-13 22:41:48,538 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:41:48,540 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:41:48,540 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:41:48,540 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:41:48,541 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:41:48,542 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.15trial/s, best loss: -0.9272486772486772]\n",
      "[INFO] 2021-04-13 22:41:57,938 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:41:57,938 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:41:57,939 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:09<00:00,  2.21trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 22:42:07,170 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:42:07,170 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:42:07,171 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:08<00:00,  2.28trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 22:42:16,147 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:42:16,147 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:42:16,148 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.77trial/s, best loss: -0.9629629629629629]\n",
      "[INFO] 2021-04-13 22:42:23,399 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:42:23,399 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:42:23,399 - Optimizing classifier hyper-parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 20/20 [00:08<00:00,  2.48trial/s, best loss: -0.9285714285714285]\n",
      "[INFO] 2021-04-13 22:42:31,591 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:42:31,592 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:42:31,592 - Mean accuracy on test instances (iteration #1): 0.9333\n",
      "[INFO] 2021-04-13 22:42:31,602 - Total elapsed time: 45.5s\n",
      "[INFO] 2021-04-13 22:42:31,602 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 2/5 lowest_harmfulness_sampling\n",
      "Testando: 61_iris RF 2/5 lowest_h_highest_u_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:42:36,084 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:42:36,085 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:42:36,089 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:42:36,089 - Building metadata.\n",
      "[INFO] 2021-04-13 22:42:38,522 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:42:38,524 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:42:38,526 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:42:38,526 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:42:38,526 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:42:38,527 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:42:38,528 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.50trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 22:42:46,620 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:42:46,621 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:42:46,621 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:07<00:00,  2.62trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 22:42:54,440 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:42:54,440 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:42:54,441 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.18trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 22:43:03,638 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:43:03,638 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:43:03,639 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.49trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 22:43:11,773 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:43:11,773 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:43:11,774 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.27trial/s, best loss: -0.9285714285714285]\n",
      "[INFO] 2021-04-13 22:43:20,660 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:43:20,660 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:43:20,661 - Mean accuracy on test instances (iteration #1): 0.9429\n",
      "[INFO] 2021-04-13 22:43:20,671 - Total elapsed time: 44.6s\n",
      "[INFO] 2021-04-13 22:43:20,671 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 2/5 lowest_h_highest_u_sampling\n",
      "Testando: 61_iris RF 2/5 lowest_n2_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:43:25,047 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:43:25,047 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:43:25,050 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:43:25,050 - Building metadata.\n",
      "[INFO] 2021-04-13 22:43:27,418 - Calculating measure 'N2'\n",
      "[INFO] 2021-04-13 22:43:27,502 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:43:27,502 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:43:27,502 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:43:27,503 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:43:27,503 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.58trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 22:43:35,389 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:43:35,389 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:43:35,390 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:07<00:00,  2.71trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 22:43:42,810 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:43:42,810 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:43:42,811 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.40trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:43:48,758 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:43:48,758 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:43:48,759 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:07<00:00,  2.72trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 22:43:56,141 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:43:56,141 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:43:56,142 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.03trial/s, best loss: -0.9285714285714285]\n",
      "[INFO] 2021-04-13 22:44:02,797 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:44:02,797 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:44:02,797 - Mean accuracy on test instances (iteration #1): 0.9429\n",
      "[INFO] 2021-04-13 22:44:02,805 - Total elapsed time: 37.8s\n",
      "[INFO] 2021-04-13 22:44:02,805 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 2/5 lowest_n2_sampling\n",
      "Testando: 61_iris RF 3/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:44:07,152 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:44:07,152 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:44:07,155 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:44:07,155 - Building metadata.\n",
      "[INFO] 2021-04-13 22:44:09,410 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 22:44:09,446 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:44:09,446 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:44:09,446 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:44:09,447 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:44:09,447 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.39trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 22:44:18,009 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:44:18,009 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:44:18,010 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.50trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 22:44:26,177 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:44:26,177 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:44:26,178 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.82trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:44:33,439 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:44:33,440 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:44:33,440 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.47trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:44:41,753 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:44:41,753 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:44:41,754 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.47trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 22:44:50,044 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:44:50,044 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:44:50,045 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 22:44:50,053 - Total elapsed time: 42.9s\n",
      "[INFO] 2021-04-13 22:44:50,053 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 3/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 3/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:44:54,139 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:44:54,139 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:44:54,142 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:44:54,142 - Building metadata.\n",
      "[INFO] 2021-04-13 22:44:56,371 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:44:56,374 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:44:56,374 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:44:56,374 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:44:56,374 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:44:56,375 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.20trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 22:45:05,523 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:45:05,524 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:45:05,525 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.31trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 22:45:14,274 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:45:14,274 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:45:14,275 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.87trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:45:21,358 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:45:21,358 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:45:21,360 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.42trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:45:30,091 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:45:30,091 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:45:30,093 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.40trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 22:45:38,739 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:45:38,739 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:45:38,739 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 22:45:38,751 - Total elapsed time: 44.6s\n",
      "[INFO] 2021-04-13 22:45:38,751 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 3/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 3/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:45:43,151 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:45:43,151 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:45:43,154 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:45:43,154 - Building metadata.\n",
      "[INFO] 2021-04-13 22:45:45,541 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:45:45,543 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:45:45,544 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:45:45,544 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:45:45,545 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:45:45,545 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.87trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 22:45:56,544 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:45:56,544 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:45:56,545 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.85trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 22:46:03,963 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:46:03,963 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:46:03,964 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.44trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:46:12,293 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:46:12,293 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:46:12,294 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.66trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:46:20,201 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:46:20,201 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:46:20,202 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.24trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 22:46:29,348 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:46:29,348 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:46:29,348 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 22:46:29,360 - Total elapsed time: 46.2s\n",
      "[INFO] 2021-04-13 22:46:29,360 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 3/5 lowest_harmfulness_sampling\n",
      "Testando: 61_iris RF 3/5 lowest_h_highest_u_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:46:34,157 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:46:34,158 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:46:34,163 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:46:34,163 - Building metadata.\n",
      "[INFO] 2021-04-13 22:46:36,892 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:46:36,894 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:46:36,897 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:46:36,897 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:46:36,897 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:46:36,898 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:46:36,899 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.84trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 22:46:47,915 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:46:47,916 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:46:47,917 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.51trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 22:46:56,039 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:46:56,039 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:46:56,040 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.46trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:47:04,310 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:47:04,310 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:47:04,311 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.55trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:47:12,469 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:47:12,470 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:47:12,471 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.03trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 22:47:22,685 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:47:22,686 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:47:22,686 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 22:47:22,696 - Total elapsed time: 48.5s\n",
      "[INFO] 2021-04-13 22:47:22,696 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 3/5 lowest_h_highest_u_sampling\n",
      "Testando: 61_iris RF 3/5 lowest_n2_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:47:27,071 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:47:27,072 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:47:27,076 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:47:27,076 - Building metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-13 22:47:29,350 - Calculating measure 'N2'\n",
      "[INFO] 2021-04-13 22:47:29,427 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:47:29,428 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:47:29,428 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:47:29,429 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:47:29,429 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.30trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 22:47:38,188 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:47:38,189 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:47:38,190 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.63trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 22:47:45,955 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:47:45,955 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:47:45,956 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.90trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:47:53,048 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:47:53,048 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:47:53,049 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.61trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:48:00,926 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:48:00,926 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:48:00,927 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.85trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 22:48:08,276 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:48:08,276 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:48:08,276 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 22:48:08,285 - Total elapsed time: 41.2s\n",
      "[INFO] 2021-04-13 22:48:08,285 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 3/5 lowest_n2_sampling\n",
      "Testando: 61_iris RF 4/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:48:12,633 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:48:12,633 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:48:12,637 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:48:12,637 - Building metadata.\n",
      "[INFO] 2021-04-13 22:48:14,997 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 22:48:15,033 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:48:15,034 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:48:15,034 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:48:15,034 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:48:15,035 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.99trial/s, best loss: -0.9157848324514991]\n",
      "[INFO] 2021-04-13 22:48:25,393 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:48:25,394 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:48:25,395 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.28trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 22:48:34,308 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:48:34,308 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:48:34,310 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.56trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:48:42,509 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:48:42,509 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:48:42,510 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:07<00:00,  2.58trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 22:48:50,471 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:48:50,472 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:48:50,472 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.18trial/s, best loss: -0.9047619047619048]\n",
      "[INFO] 2021-04-13 22:48:57,040 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:48:57,041 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:48:57,041 - Mean accuracy on test instances (iteration #1): 0.9619\n",
      "[INFO] 2021-04-13 22:48:57,050 - Total elapsed time: 44.4s\n",
      "[INFO] 2021-04-13 22:48:57,050 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 4/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 4/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:49:01,063 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:49:01,064 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:49:01,066 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:49:01,066 - Building metadata.\n",
      "[INFO] 2021-04-13 22:49:03,255 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:49:03,258 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:49:03,259 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:49:03,259 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:49:03,259 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:49:03,260 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.23trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:49:12,296 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:49:12,296 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:49:12,297 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.30trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 22:49:18,414 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:49:18,414 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:49:18,415 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.27trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:49:27,516 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:49:27,516 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:49:27,517 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:07<00:00,  2.67trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 22:49:35,043 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:49:35,044 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:49:35,045 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.42trial/s, best loss: -0.9047619047619048]\n",
      "[INFO] 2021-04-13 22:49:43,428 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:49:43,428 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:49:43,429 - Mean accuracy on test instances (iteration #1): 0.9619\n",
      "[INFO] 2021-04-13 22:49:43,437 - Total elapsed time: 42.4s\n",
      "[INFO] 2021-04-13 22:49:43,437 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 4/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 4/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:49:47,648 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:49:47,648 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:49:47,651 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:49:47,651 - Building metadata.\n",
      "[INFO] 2021-04-13 22:49:49,916 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:49:49,918 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:49:49,918 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:49:49,918 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:49:49,919 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:49:49,919 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.42trial/s, best loss: -0.9281305114638448]\n",
      "[INFO] 2021-04-13 22:49:58,544 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 22:49:58,545 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:49:58,546 - Optimizing classifier hyper-parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 20/20 [00:07<00:00,  2.59trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 22:50:06,353 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:50:06,353 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:50:06,353 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.71trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:50:13,992 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:50:13,992 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:50:13,993 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.67trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:50:21,554 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:50:21,554 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:50:21,556 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.51trial/s, best loss: -0.9047619047619048]\n",
      "[INFO] 2021-04-13 22:50:29,697 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:50:29,697 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:50:29,697 - Mean accuracy on test instances (iteration #1): 0.9524\n",
      "[INFO] 2021-04-13 22:50:29,705 - Total elapsed time: 42.1s\n",
      "[INFO] 2021-04-13 22:50:29,705 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 4/5 lowest_harmfulness_sampling\n",
      "Testando: 61_iris RF 4/5 lowest_h_highest_u_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:50:33,764 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:50:33,765 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:50:33,767 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:50:33,767 - Building metadata.\n",
      "[INFO] 2021-04-13 22:50:35,962 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 22:50:35,963 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 22:50:35,965 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:50:35,965 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:50:35,965 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:50:35,966 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:50:35,967 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.08trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:50:45,643 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:50:45,643 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:50:45,644 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.67trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 22:50:53,243 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:50:53,243 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:50:53,244 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.49trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:50:59,173 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:50:59,173 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:50:59,174 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.64trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 22:51:06,794 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:51:06,795 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:51:06,795 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.89trial/s, best loss: -0.9047619047619048]\n",
      "[INFO] 2021-04-13 22:51:14,062 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:51:14,062 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:51:14,062 - Mean accuracy on test instances (iteration #1): 0.9619\n",
      "[INFO] 2021-04-13 22:51:14,071 - Total elapsed time: 40.3s\n",
      "[INFO] 2021-04-13 22:51:14,071 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 4/5 lowest_h_highest_u_sampling\n",
      "Testando: 61_iris RF 4/5 lowest_n2_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 22:51:18,497 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 22:51:18,497 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 22:51:18,500 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 22:51:18,500 - Building metadata.\n",
      "[INFO] 2021-04-13 22:51:20,985 - Calculating measure 'N2'\n",
      "[INFO] 2021-04-13 22:51:21,074 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 22:51:21,074 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 22:51:21,074 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 22:51:21,075 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 22:51:21,076 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.88trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-13 22:51:31,736 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 22:51:31,736 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 22:51:31,737 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.67trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 22:51:39,333 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:51:39,333 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 22:51:39,333 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.90trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 22:51:50,069 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 22:51:50,069 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 22:51:50,070 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.22trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 22:51:59,231 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 22:51:59,231 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 22:51:59,232 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.39trial/s, best loss: -0.9047619047619048]\n",
      "[INFO] 2021-04-13 22:52:07,686 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 22:52:07,686 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 22:52:07,686 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 22:52:07,696 - Total elapsed time: 49.2s\n",
      "[INFO] 2021-04-13 22:52:07,696 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 4/5 lowest_n2_sampling\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tqdm_datasets = tqdm(datasets, desc=\" Dataset: \"+ str(ds[:-5]))\n",
    "#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in tqdm(classifiers,  desc =\"Classifier\"):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "        \n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in tqdm(range(n_splits),  desc =\"Bag\"):\n",
    "\n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_lsc_sampling\")\n",
    "            result = highest_lsc_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_lsc_sampling\")\n",
    "            \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_usefulness_sampling\")\n",
    "            result = highest_usefulness_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_usefulness_sampling\")\n",
    "           \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_harmfulness_sampling\")\n",
    "            result = lowest_harmfulness_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_harmfulness_sampling\")\n",
    "            \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_h_highest_u_sampling\")\n",
    "            result = lowest_h_highest_u_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_h_highest_u_sampling\")\n",
    "  \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_n2_sampling\")\n",
    "            result = lowest_n2_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_n2_sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 38.14502305999849,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9278350515463918,\n",
       "  'time_elapsed': 38.67585848800081,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9381443298969072,\n",
       "  'time_elapsed': 40.001493839999966,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9896907216494846,\n",
       "  'time_elapsed': 42.09189175600113,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest H, Highest U Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8350515463917526,\n",
       "  'time_elapsed': 48.724058137999236,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest N2 Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 1.0,\n",
       "  'time_elapsed': 54.5501551010002,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 50.512893192000774,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8969072164948454,\n",
       "  'time_elapsed': 44.80764231499961,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 48.19530474000021,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest H, Highest U Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9175257731958762,\n",
       "  'time_elapsed': 51.51108995100003,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest N2 Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.7835051546391752,\n",
       "  'time_elapsed': 44.18138229500073,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 50.73035665399948,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9072164948453608,\n",
       "  'time_elapsed': 50.0920023409999,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 49.08500197600006,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest H, Highest U Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9175257731958762,\n",
       "  'time_elapsed': 42.09845532399959,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest N2 Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 47.2368223529993,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.845360824742268,\n",
       "  'time_elapsed': 48.75493334100065,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 50.662672071001,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.979381443298969,\n",
       "  'time_elapsed': 53.22239810099927,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest H, Highest U Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9175257731958762,\n",
       "  'time_elapsed': 45.60115093799868,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest N2 Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.979381443298969,\n",
       "  'time_elapsed': 48.72726974999932,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9381443298969072,\n",
       "  'time_elapsed': 46.40615703599906,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9072164948453608,\n",
       "  'time_elapsed': 46.249783908999234,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 44.374114215999725,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest H, Highest U Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8969072164948454,\n",
       "  'time_elapsed': 53.613774585999636,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest N2 Sampling',\n",
       "  'dataset': '61_iris'}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_performance_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(total_performance_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('performance_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performance_history</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>classifier</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.550155</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.989691</td>\n",
       "      <td>42.091892</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest H, Highest U Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.979381</td>\n",
       "      <td>53.222398</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest H, Highest U Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.979381</td>\n",
       "      <td>48.727270</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>50.730357</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>50.512893</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>48.195305</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest H, Highest U Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>49.085002</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest H, Highest U Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>50.662672</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>47.236822</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>38.145023</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>44.374114</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest H, Highest U Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.938144</td>\n",
       "      <td>46.406157</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.938144</td>\n",
       "      <td>40.001494</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.927835</td>\n",
       "      <td>38.675858</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.917526</td>\n",
       "      <td>45.601151</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest N2 Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.917526</td>\n",
       "      <td>42.098455</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest N2 Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.917526</td>\n",
       "      <td>51.511090</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest N2 Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.907216</td>\n",
       "      <td>46.249784</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.907216</td>\n",
       "      <td>50.092002</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.896907</td>\n",
       "      <td>44.807642</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.896907</td>\n",
       "      <td>53.613775</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest N2 Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.845361</td>\n",
       "      <td>48.754933</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.835052</td>\n",
       "      <td>48.724058</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest N2 Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.783505</td>\n",
       "      <td>44.181382</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    performance_history  time_elapsed classifier  sample_size  \\\n",
       "5              1.000000     54.550155         RF     0.066667   \n",
       "3              0.989691     42.091892         RF     0.066667   \n",
       "18             0.979381     53.222398         RF     0.066667   \n",
       "20             0.979381     48.727270         RF     0.066667   \n",
       "11             0.958763     50.730357         RF     0.066667   \n",
       "6              0.958763     50.512893         RF     0.066667   \n",
       "8              0.958763     48.195305         RF     0.066667   \n",
       "13             0.958763     49.085002         RF     0.066667   \n",
       "17             0.948454     50.662672         RF     0.066667   \n",
       "15             0.948454     47.236822         RF     0.066667   \n",
       "0              0.948454     38.145023         RF     0.066667   \n",
       "23             0.948454     44.374114         RF     0.066667   \n",
       "21             0.938144     46.406157         RF     0.066667   \n",
       "2              0.938144     40.001494         RF     0.066667   \n",
       "1              0.927835     38.675858         RF     0.066667   \n",
       "19             0.917526     45.601151         RF     0.066667   \n",
       "14             0.917526     42.098455         RF     0.066667   \n",
       "9              0.917526     51.511090         RF     0.066667   \n",
       "22             0.907216     46.249784         RF     0.066667   \n",
       "12             0.907216     50.092002         RF     0.066667   \n",
       "7              0.896907     44.807642         RF     0.066667   \n",
       "24             0.896907     53.613775         RF     0.066667   \n",
       "16             0.845361     48.754933         RF     0.066667   \n",
       "4              0.835052     48.724058         RF     0.066667   \n",
       "10             0.783505     44.181382         RF     0.066667   \n",
       "\n",
       "                        Strategy  dataset  \n",
       "5           Highest LSC Sampling  61_iris  \n",
       "3   Lowest H, Highest U Sampling  61_iris  \n",
       "18  Lowest H, Highest U Sampling  61_iris  \n",
       "20          Highest LSC Sampling  61_iris  \n",
       "11   Highest Usefulness Sampling  61_iris  \n",
       "6    Highest Usefulness Sampling  61_iris  \n",
       "8   Lowest H, Highest U Sampling  61_iris  \n",
       "13  Lowest H, Highest U Sampling  61_iris  \n",
       "17   Lowest Harmfulness Sampling  61_iris  \n",
       "15          Highest LSC Sampling  61_iris  \n",
       "0           Highest LSC Sampling  61_iris  \n",
       "23  Lowest H, Highest U Sampling  61_iris  \n",
       "21   Highest Usefulness Sampling  61_iris  \n",
       "2    Lowest Harmfulness Sampling  61_iris  \n",
       "1    Highest Usefulness Sampling  61_iris  \n",
       "19            Lowest N2 Sampling  61_iris  \n",
       "14            Lowest N2 Sampling  61_iris  \n",
       "9             Lowest N2 Sampling  61_iris  \n",
       "22   Lowest Harmfulness Sampling  61_iris  \n",
       "12   Lowest Harmfulness Sampling  61_iris  \n",
       "7    Lowest Harmfulness Sampling  61_iris  \n",
       "24            Lowest N2 Sampling  61_iris  \n",
       "16   Highest Usefulness Sampling  61_iris  \n",
       "4             Lowest N2 Sampling  61_iris  \n",
       "10          Highest LSC Sampling  61_iris  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Strategy != \"Query by Committee\"].sort_values('performance_history', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performance_history</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>classifier</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [performance_history, time_elapsed, classifier, sample_size, Strategy, dataset]\n",
       "Index: []"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Strategy == \"Expected Error Reduction\"].sort_values('time_elapsed', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25 entries, 0 to 24\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   performance_history  25 non-null     float64\n",
      " 1   time_elapsed         25 non-null     float64\n",
      " 2   classifier           25 non-null     object \n",
      " 3   sample_size          25 non-null     float64\n",
      " 4   Strategy             25 non-null     object \n",
      " 5   dataset              25 non-null     object \n",
      "dtypes: float64(3), object(3)\n",
      "memory usage: 1.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 546.875x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.relplot(\n",
    "    data= df,\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=5), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df[(df.Strategy != \"Uncertain Sampling\") & (df.Strategy != \"Query by Committee\")],\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=3), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df[(df.Strategy == \"Uncertain Sampling\") | (df.Strategy == \"Query by Committee\")],\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=2), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "p_bar = tqdm(datalist)\n",
    "for dataset_id in p_bar:\n",
    "    X_raw, y_raw, idx_data, dataset_name = which_oml_dataset(dataset_id)\n",
    "    p_bar.set_description(f'\"{dataset_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"1465_breast-tissue.arff\"\n",
    "\n",
    "X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "   \n",
    "from modAL.uncertainty import classifier_uncertainty\n",
    "\n",
    "print(len(np.unique(y_raw)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)), stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "print(y_train)\n",
    "\n",
    "learner = ActiveLearner (\n",
    "    estimator= which_classifier(classifier), #cls,\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    ")\n",
    "\n",
    "uncertain_sample_score = learner.score(X_test, y_test)\n",
    "\n",
    "total_of_samples = 1\n",
    "while (total_of_samples != cost):\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "        #print(\"IF\", learner.score(X_test, y_test))\n",
    "        learner.teach(X_train, y_train)\n",
    "        uncertain_sample_score = learner.score(X_test, y_test)\n",
    "        performance_history.append(uncertain_sample_score)\n",
    "    total_of_samples = total_of_samples + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size= len(np.unique(y_raw)) + init_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
