{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning - Comparando estratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amostra por incerteza\n",
    "- Amostragem aleatória\n",
    "- Consulta por comitê\n",
    "- Aprendizado passivo\n",
    "- Redução do erro esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run set_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing_libraries.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets OpenML\n",
    "import openml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml.config.cache_directory = os.path.expanduser('./datasets/openML')\n",
    "openml_list = openml.datasets.list_datasets()\n",
    "\n",
    "datalist = pd.DataFrame.from_dict(openml_list, orient=\"index\")\n",
    "datalist = list(datalist[(datalist.NumberOfClasses.isnull() == False) & (datalist.NumberOfClasses != 0)][\"did\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostra por incerteza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertain_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][TRAIN][initial_idx]]\n",
    "    #X_test, y_test = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    #cls = which_classifier(classifier)\n",
    "    #cls.fit(X_train,y_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "        \n",
    "        idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "        X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "        \n",
    "        if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "            #print(\"IF\", learner.score(X_test, y_test))\n",
    "            sample_size = sample_size + len(X_train)\n",
    "            learner.teach(X_train, y_train)\n",
    "            uncertain_sample_score = learner.score(X_test, y_test)\n",
    "            performance_history.append(uncertain_sample_score)\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Uncertain Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostragem aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "        \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "\n",
    "    for i in range(1, cost+1):\n",
    "\n",
    "        #high = X_raw.shape[0] = qtd amostras no dataset\n",
    "        #training_indices = np.random.randint(low=0, high=len(X_raw[idx_data[idx_bag][TRAIN]]), size=k+i) #high = qtd elementos na bag\n",
    "        #sample_size = sample_size + len(training_indices)\n",
    "        #X_train = X_raw[idx_data[idx_bag][TRAIN][training_indices]] #ASK06\n",
    "        #y_train = y_raw[idx_data[idx_bag][TRAIN][training_indices]]\n",
    "        #X_test = np.delete(X_raw, idx_data[idx_bag][TRAIN][training_indices], axis=0)\n",
    "        #y_test = np.delete(y_raw, idx_data[idx_bag][TRAIN][training_indices], axis=0)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "        \n",
    "        cls = which_classifier(classifier)\n",
    "        cls.fit(X_train, y_train)\n",
    "\n",
    "        random_sampling_score = cls.score(X_test,y_test)\n",
    "        performance_history.append(random_sampling_score)\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw),\n",
    "             \"Strategy\": \"Random Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta por comitê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_by_committee(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.models import ActiveLearner, Committee\n",
    "    from modAL.disagreement import vote_entropy_sampling\n",
    "\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "\n",
    "    learner_list = []\n",
    "\n",
    "    for j in range(1, cost+1): # Loop para criação do comitê\n",
    "\n",
    "        X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "\n",
    "        # initializing learner\n",
    "        learner = ActiveLearner(\n",
    "            estimator= which_classifier(classifier),\n",
    "            X_training = X_train, y_training = y_train \n",
    "        )\n",
    "        learner_list.append(learner)\n",
    "\n",
    "    # assembling the committee\n",
    "    committee = Committee(\n",
    "        learner_list=learner_list,\n",
    "        query_strategy=vote_entropy_sampling)\n",
    "\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]]\n",
    "    \n",
    "    # query by committee\n",
    "    for idx in range(cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        query_idx, query_instance = committee.query(X_pool, n_instances = init_size+1)\n",
    "        sample_size = sample_size + len(query_idx)\n",
    "        \n",
    "        committee.teach(\n",
    "            X = X_pool[query_idx],\n",
    "            y = y_pool[query_idx]\n",
    "        )\n",
    "\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx)\n",
    "        \n",
    "        query_by_committee_score = committee.score(X_pool, y_pool)\n",
    "        performance_history.append(query_by_committee_score)\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw),\n",
    "             \"Strategy\": \"Query by Committee\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mX_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhich_arff_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "%run -i main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Error Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_error_reduction(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][TRAIN][initial_idx]]\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    exp_er_score = learner.score(X_pool, y_pool)\n",
    "    performance_history.append(exp_er_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = expected_error_reduction(learner, X_pool, 'binary', n_instances=init_size)[0]\n",
    "\n",
    "        learner.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        sample_size = sample_size + init_size\n",
    "    \n",
    "        X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, exp_error_idx)\n",
    "        \n",
    "        exp_er_score = learner.score(X_pool, y_pool)\n",
    "        performance_history.append(exp_er_score)\n",
    "        \n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Expected Error Reduction\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Model Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_model_change(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][0][initial_idx]]\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    \n",
    "#     performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = np.random.choice(range(len(X_pool)), size=init_size, replace=False)\n",
    "        aux = deepcopy(learner)\n",
    "\n",
    "        aux.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        score_aux = aux.score(X_pool, y_pool)\n",
    "        score_learner = learner.score(X_pool, y_pool)\n",
    "\n",
    "        if score_aux > score_learner:\n",
    "            learner = deepcopy(aux)\n",
    "            sample_size = sample_size + init_size\n",
    "        \n",
    "        X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, exp_error_idx, axis=0)\n",
    "        \n",
    "        exp_mo_score = learner.score(X_pool, y_pool)\n",
    "        performance_history.append(exp_mo_score)\n",
    "\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Expected Model Change\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query highest LSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_lsc_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    print(type(X_raw[idx_data[idx_bag][TRAIN]]))\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by='feature_LSC', ascending=False)['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Highest LSC Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_dataset(dataset = \"iris\", n_splits = 5):\n",
    "    \n",
    "    # Futuramente essa etapa será ajustada para receber qualquer dataset (ou lista com datasets)\n",
    "    if (dataset == \"iris\"):\n",
    "        data = load_iris()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "    \n",
    "    if (dataset == \"wine\"):\n",
    "        data = load_wine()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    if (dataset == \"digits\"):\n",
    "        data = load_digits()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_oml_dataset(dataset_id, n_splits = 5):\n",
    "    data = openml.datasets.get_dataset(dataset_id)\n",
    "    \n",
    "    X_raw, y_raw, categorical_indicator, attribute_names = data.get_data(\n",
    "    dataset_format=\"array\", target=data.default_target_attribute)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_raw)\n",
    "    y_raw = le.transform(y_raw)\n",
    "    \n",
    "    X_raw = np.nan_to_num(X_raw)\n",
    "    \n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_arff_dataset(dataset, n_splits = 5):\n",
    "   \n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    \n",
    "    data = arff.loadarff('datasets/luis/' + dataset)\n",
    "    data = pd.DataFrame(data[0])\n",
    "\n",
    "    X_raw = data[data.columns[:-1]].to_numpy()\n",
    "    y_raw = data[data.columns[-1]].to_numpy()\n",
    "    \n",
    "    lex = preprocessing.OrdinalEncoder()\n",
    "    lex.fit(X_raw)\n",
    "    X_raw = lex.transform(X_raw)\n",
    "        \n",
    "    ley = preprocessing.LabelEncoder()\n",
    "    ley.fit(y_raw)\n",
    "    y_raw = ley.transform(y_raw)\n",
    "    \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    data_cv.get_n_splits(X_raw,y_raw)\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw, y_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_classifier(classifier = '5NN'):\n",
    "    \n",
    "    if (classifier == '5NN'):\n",
    "        return KNeighborsClassifier(5)\n",
    "    elif (classifier == 'C4.5'):\n",
    "        return tree.DecisionTreeClassifier()\n",
    "    elif (classifier == 'NB'):\n",
    "        return GaussianNB()\n",
    "    elif (classifier == 'SVM'):\n",
    "        return SVC(probability=True, gamma='auto')\n",
    "    elif (classifier == 'RF'):\n",
    "        return RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datasets(dataset):\n",
    "    \n",
    "    data = arff.loadarff('./datasets/luis/' + dataset)\n",
    "    metadata = data[1]\n",
    "    data = pd.DataFrame(data[0])\n",
    "    \n",
    "    instances = len(data)\n",
    "    classes = len(data.iloc[:,-1].value_counts())\n",
    "    attributes = len(data.columns)- 1\n",
    "    nominal_attributes = str(metadata).count(\"nominal\")\n",
    "    \n",
    "    proportion = data.iloc[:,-1].value_counts()\n",
    "    proportion = proportion.map(lambda x: round(x/instances*100,2))\n",
    "\n",
    "    majority = max(proportion)\n",
    "    minority = min(proportion)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"name\": dataset[:-5],\n",
    "        \"instances\": instances,\n",
    "        \"classes\": classes,\n",
    "        \"attributes\": attributes,\n",
    "        \"nominal attributes\": nominal_attributes,\n",
    "        \"majority\": majority,\n",
    "        \"minority\": minority\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Teste\n",
    "\n",
    "total_performance_history = []\n",
    "\n",
    "classifiers = ['SVM']\n",
    "ds = '3_kr-vs-kp.arff'\n",
    "for classifier in classifiers:\n",
    "    X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "    for idx_bag in range(n_splits):\n",
    "        print(ds, classifier, \" \", idx_bag, \" \", n_splits, \" uncertain_sampling\")\n",
    "        result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "        result['dataset'] = ds\n",
    "        total_performance_history.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('./datasets/luis')\n",
    "classifiers = ['5NN', 'C4.5', 'NB','RF']\n",
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['61_iris.arff']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>instances</th>\n",
       "      <th>classes</th>\n",
       "      <th>attributes</th>\n",
       "      <th>nominal attributes</th>\n",
       "      <th>majority</th>\n",
       "      <th>minority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61_iris</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>33.33</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  instances  classes  attributes  nominal attributes  majority  \\\n",
       "0  61_iris        150        3           4                   1     33.33   \n",
       "\n",
       "   minority  \n",
       "0     33.33  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = []\n",
    "\n",
    "for ds in datasets:\n",
    "    metadata.append(fetch_datasets(ds))\n",
    "\n",
    "metadata = pd.DataFrame.from_dict(metadata)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for ds in tqdm(datasets):\n",
    "    for classifier in tqdm(classifiers):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" uncertain_sampling\")\n",
    "            result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" random sampling\")\n",
    "            result = random_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" query_by_committee\")\n",
    "            result = query_by_committee(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" exp error reduction\")\n",
    "            result = exp_error_reduction(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" exp model change\")\n",
    "            result = exp_model_change(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee1f3726f0c44d686ea06ac1707c47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dataset'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d861727b709e449c85478baed0683818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Classifier'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277538756c0f4bfba7d95ae66ee41ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Bag'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "\r",
      "Testando: 61_iris 5NN 0/5 exp_error_reduction\n",
      "\t Size of X_pool: 45\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(1) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testando: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_bag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" exp_error_reduction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_error_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_bag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtotal_performance_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/main.py\u001b[0m in \u001b[0;36mexp_error_reduction\u001b[0;34m(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mexp_error_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected_error_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_instances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp_error_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp_error_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minit_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/modAL/models/learners.py\u001b[0m in \u001b[0;36mteach\u001b[0;34m(self, X, y, bootstrap, only_new, **fit_kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfit\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0monly_new\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_to_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/modAL/models/base.py\u001b[0m in \u001b[0;36m_add_training_data\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mclassifier\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m         check_X_y(X, y, accept_sparse=True, ensure_2d=False, allow_nd=True, multi_output=True, dtype=None,\n\u001b[0m\u001b[1;32m     89\u001b[0m                   force_all_finite=self.force_all_finite)\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    821\u001b[0m                     estimator=estimator)\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n\u001b[0m\u001b[1;32m    824\u001b[0m                         ensure_2d=False, dtype=None)\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0m\u001b[1;32m    203\u001b[0m                             \" a valid collection.\" % x)\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(1) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "#tqdm_datasets = tqdm(datasets, desc=\" Dataset: \"+ str(ds[:-5]))\n",
    "#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in tqdm(classifiers,  desc =\"Classifier\"):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in tqdm(range(n_splits),  desc =\"Bag\"):\n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" uncertain_sampling\")\n",
    "#             result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" uncertain_sampling\")\n",
    "            \n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" random_sampling\")\n",
    "#             result = random_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" random_sampling\")\n",
    "            \n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" query_by_committee\")\n",
    "#             result = query_by_committee(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" query_by_committee\")\n",
    "\n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_error_reduction\")\n",
    "            result = exp_error_reduction(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_error_reduction\")\n",
    "            \n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_model_change\")\n",
    "#             result = exp_model_change(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_model_change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['RF']\n",
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5d92e16c91467ab45a154e59abf1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dataset'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2effcbff0bcc4e07ac7c6a3c56d0af2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Classifier'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c418f1321843349703c88e932e4ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Bag'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando: 61_iris 5NN 0/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-12 23:53:04,859 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-12 23:53:04,860 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-12 23:53:04,862 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-12 23:53:04,862 - Building metadata.\n",
      "[INFO] 2021-04-12 23:53:06,619 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-12 23:53:06,647 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-12 23:53:06,647 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-12 23:53:06,647 - Estimating instance performance...\n",
      "[INFO] 2021-04-12 23:53:06,647 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-12 23:53:06,648 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.15trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-12 23:53:13,116 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:53:13,116 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-12 23:53:13,117 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.11trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-12 23:53:19,614 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:53:19,614 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-12 23:53:19,615 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.85trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-12 23:53:26,729 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:53:26,729 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-12 23:53:26,730 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:07<00:00,  2.69trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-12 23:53:34,385 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-12 23:53:34,385 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-12 23:53:34,385 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.63trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-12 23:53:40,146 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-12 23:53:40,146 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-12 23:53:40,146 - Mean accuracy on test instances (iteration #1): 0.9329\n",
      "[INFO] 2021-04-12 23:53:40,153 - Total elapsed time: 35.3s\n",
      "[INFO] 2021-04-12 23:53:40,153 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 0/5 highest_lsc_sampling\n",
      "Testando: 61_iris 5NN 1/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-12 23:53:44,090 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-12 23:53:44,090 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-12 23:53:44,093 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-12 23:53:44,093 - Building metadata.\n",
      "[INFO] 2021-04-12 23:53:45,694 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-12 23:53:45,721 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-12 23:53:45,721 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-12 23:53:45,722 - Estimating instance performance...\n",
      "[INFO] 2021-04-12 23:53:45,722 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-12 23:53:45,723 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:06<00:00,  2.94trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-12 23:53:52,628 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-12 23:53:52,628 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-12 23:53:52,629 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.56trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-12 23:53:58,267 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-12 23:53:58,267 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-12 23:53:58,268 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.25trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-12 23:54:04,493 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:54:04,493 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-12 23:54:04,493 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.92trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-12 23:54:11,598 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:54:11,598 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-12 23:54:11,599 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.52trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-12 23:54:19,724 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-12 23:54:19,724 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-12 23:54:19,725 - Mean accuracy on test instances (iteration #1): 0.961\n",
      "[INFO] 2021-04-12 23:54:19,736 - Total elapsed time: 35.7s\n",
      "[INFO] 2021-04-12 23:54:19,736 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 1/5 highest_lsc_sampling\n",
      "Testando: 61_iris 5NN 2/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-12 23:54:23,923 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-12 23:54:23,924 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-12 23:54:23,927 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-12 23:54:23,927 - Building metadata.\n",
      "[INFO] 2021-04-12 23:54:26,717 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-12 23:54:26,765 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-12 23:54:26,765 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-12 23:54:26,765 - Estimating instance performance...\n",
      "[INFO] 2021-04-12 23:54:26,766 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-12 23:54:26,768 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:13<00:00,  1.51trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-12 23:54:40,564 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-12 23:54:40,564 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-12 23:54:40,566 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:07<00:00,  2.69trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-12 23:54:48,055 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:54:48,055 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-12 23:54:48,056 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.17trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-12 23:54:57,355 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-12 23:54:57,356 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-12 23:54:57,357 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.18trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-12 23:55:06,610 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-12 23:55:06,610 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-12 23:55:06,611 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.57trial/s, best loss: -0.9285714285714285]\n",
      "[INFO] 2021-04-12 23:55:14,462 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-12 23:55:14,463 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-12 23:55:14,463 - Mean accuracy on test instances (iteration #1): 0.9333\n",
      "[INFO] 2021-04-12 23:55:14,473 - Total elapsed time: 50.6s\n",
      "[INFO] 2021-04-12 23:55:14,474 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 2/5 highest_lsc_sampling\n",
      "Testando: 61_iris 5NN 3/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-12 23:55:18,347 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-12 23:55:18,347 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-12 23:55:18,350 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-12 23:55:18,350 - Building metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-12 23:55:20,723 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-12 23:55:20,762 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-12 23:55:20,762 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-12 23:55:20,762 - Estimating instance performance...\n",
      "[INFO] 2021-04-12 23:55:20,763 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-12 23:55:20,764 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.96trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-12 23:55:31,105 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-12 23:55:31,105 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-12 23:55:31,106 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.61trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-12 23:55:39,062 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-12 23:55:39,062 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-12 23:55:39,063 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.88trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-12 23:55:49,988 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:55:49,988 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-12 23:55:49,989 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.27trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-12 23:55:59,070 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:55:59,070 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-12 23:55:59,071 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.63trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-12 23:56:06,822 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-12 23:56:06,822 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-12 23:56:06,822 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-12 23:56:06,834 - Total elapsed time: 48.5s\n",
      "[INFO] 2021-04-12 23:56:06,834 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 3/5 highest_lsc_sampling\n",
      "Testando: 61_iris 5NN 4/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-12 23:56:12,523 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-12 23:56:12,524 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-12 23:56:12,528 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-12 23:56:12,528 - Building metadata.\n",
      "[INFO] 2021-04-12 23:56:15,897 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-12 23:56:15,953 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-12 23:56:15,953 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-12 23:56:15,953 - Estimating instance performance...\n",
      "[INFO] 2021-04-12 23:56:15,954 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-12 23:56:15,956 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:12<00:00,  1.61trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-12 23:56:28,462 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:56:28,462 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-12 23:56:28,463 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:11<00:00,  1.74trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-12 23:56:40,132 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-12 23:56:40,132 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-12 23:56:40,133 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.29trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-12 23:56:49,157 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-12 23:56:49,157 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-12 23:56:49,158 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.37trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-12 23:56:57,619 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:56:57,619 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-12 23:56:57,620 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.26trial/s, best loss: -0.9047619047619048]\n",
      "[INFO] 2021-04-12 23:57:06,712 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-12 23:57:06,712 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-12 23:57:06,713 - Mean accuracy on test instances (iteration #1): 0.9524\n",
      "[INFO] 2021-04-12 23:57:06,723 - Total elapsed time: 54.2s\n",
      "[INFO] 2021-04-12 23:57:06,723 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 4/5 highest_lsc_sampling\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59477bea21a44f5a4b6a035109ec8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Bag'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando: 61_iris C4.5 0/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-12 23:57:10,600 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-12 23:57:10,600 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-12 23:57:10,603 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-12 23:57:10,603 - Building metadata.\n",
      "[INFO] 2021-04-12 23:57:13,202 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-12 23:57:13,244 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-12 23:57:13,244 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-12 23:57:13,245 - Estimating instance performance...\n",
      "[INFO] 2021-04-12 23:57:13,245 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-12 23:57:13,246 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:11<00:00,  1.81trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-12 23:57:24,412 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:57:24,412 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-12 23:57:24,413 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.43trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-12 23:57:32,815 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:57:32,815 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-12 23:57:32,816 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.54trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-12 23:57:40,822 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:57:40,822 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-12 23:57:40,823 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.23trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-12 23:57:49,935 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-12 23:57:49,935 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-12 23:57:49,936 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.26trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-12 23:57:59,020 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-12 23:57:59,020 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-12 23:57:59,020 - Mean accuracy on test instances (iteration #1): 0.9424\n",
      "[INFO] 2021-04-12 23:57:59,030 - Total elapsed time: 48.4s\n",
      "[INFO] 2021-04-12 23:57:59,030 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris C4.5 0/5 highest_lsc_sampling\n",
      "Testando: 61_iris C4.5 1/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-12 23:58:03,097 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-12 23:58:03,098 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-12 23:58:03,101 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-12 23:58:03,101 - Building metadata.\n",
      "[INFO] 2021-04-12 23:58:05,669 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-12 23:58:05,713 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-12 23:58:05,713 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-12 23:58:05,713 - Estimating instance performance...\n",
      "[INFO] 2021-04-12 23:58:05,714 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-12 23:58:05,715 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:08<00:00,  2.40trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-12 23:58:14,180 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-12 23:58:14,180 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-12 23:58:14,181 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.92trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-12 23:58:24,662 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:58:24,662 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-12 23:58:24,663 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:11<00:00,  1.70trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-12 23:58:36,505 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:58:36,505 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-12 23:58:36,506 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:13<00:00,  1.53trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-12 23:58:50,278 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:58:50,278 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-12 23:58:50,280 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:15<00:00,  1.31trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-12 23:59:05,983 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-12 23:59:05,983 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-12 23:59:05,984 - Mean accuracy on test instances (iteration #1): 0.9514\n",
      "[INFO] 2021-04-12 23:59:06,002 - Total elapsed time: 1m2s\n",
      "[INFO] 2021-04-12 23:59:06,002 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris C4.5 1/5 highest_lsc_sampling\n",
      "Testando: 61_iris C4.5 2/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-12 23:59:13,009 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-12 23:59:13,010 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-12 23:59:13,014 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-12 23:59:13,015 - Building metadata.\n",
      "[INFO] 2021-04-12 23:59:16,479 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-12 23:59:16,535 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-12 23:59:16,535 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-12 23:59:16,535 - Estimating instance performance...\n",
      "[INFO] 2021-04-12 23:59:16,536 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-12 23:59:16,538 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:12<00:00,  1.63trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-12 23:59:29,025 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-12 23:59:29,026 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-12 23:59:29,027 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:08<00:00,  2.40trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-12 23:59:37,396 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-12 23:59:37,396 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-12 23:59:37,397 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.97trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-12 23:59:44,192 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-12 23:59:44,193 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-12 23:59:44,193 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.83trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-12 23:59:51,270 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-12 23:59:51,271 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-12 23:59:51,271 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.71trial/s, best loss: -0.9285714285714285]\n",
      "[INFO] 2021-04-12 23:59:58,699 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-12 23:59:58,699 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-12 23:59:58,699 - Mean accuracy on test instances (iteration #1): 0.9333\n",
      "[INFO] 2021-04-12 23:59:58,708 - Total elapsed time: 45.7s\n",
      "[INFO] 2021-04-12 23:59:58,708 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris C4.5 2/5 highest_lsc_sampling\n",
      "Testando: 61_iris C4.5 3/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:00:02,342 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:00:02,343 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:00:02,346 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:00:02,346 - Building metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-13 00:00:04,622 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:00:04,658 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:00:04,659 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:00:04,659 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:00:04,659 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:00:04,660 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.43trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 00:00:13,149 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:00:13,149 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:00:13,150 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.48trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 00:00:19,083 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:00:19,084 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:00:19,084 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.25trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 00:00:28,159 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:00:28,159 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:00:28,160 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.04trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 00:00:38,185 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:00:38,185 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:00:38,186 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.09trial/s, best loss: -0.9642857142857144]\n",
      "[INFO] 2021-04-13 00:00:47,808 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 00:00:47,808 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:00:47,808 - Mean accuracy on test instances (iteration #1): 0.9419\n",
      "[INFO] 2021-04-13 00:00:47,819 - Total elapsed time: 45.5s\n",
      "[INFO] 2021-04-13 00:00:47,819 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris C4.5 3/5 highest_lsc_sampling\n",
      "Testando: 61_iris C4.5 4/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:00:52,195 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:00:52,195 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:00:52,198 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:00:52,198 - Building metadata.\n",
      "[INFO] 2021-04-13 00:00:54,840 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:00:54,882 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:00:54,882 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:00:54,882 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:00:54,883 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:00:54,884 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.92trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 00:01:05,444 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:01:05,445 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:01:05,446 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.46trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 00:01:13,705 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:01:13,705 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:01:13,706 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.28trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 00:01:22,721 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 00:01:22,721 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:01:22,722 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.03trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 00:01:32,924 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:01:32,925 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:01:32,926 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.55trial/s, best loss: -0.9166666666666666]\n",
      "[INFO] 2021-04-13 00:01:40,814 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 00:01:40,815 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:01:40,815 - Mean accuracy on test instances (iteration #1): 0.9424\n",
      "[INFO] 2021-04-13 00:01:40,825 - Total elapsed time: 48.6s\n",
      "[INFO] 2021-04-13 00:01:40,825 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris C4.5 4/5 highest_lsc_sampling\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df911d56f344ab4ab233d8b2ecd141c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Bag'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando: 61_iris NB 0/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:01:44,826 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:01:44,826 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:01:44,829 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:01:44,829 - Building metadata.\n",
      "[INFO] 2021-04-13 00:01:47,223 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:01:47,263 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:01:47,264 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:01:47,264 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:01:47,265 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:01:47,266 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.85trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-13 00:01:58,264 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:01:58,264 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:01:58,266 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.04trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 00:02:08,439 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:02:08,439 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:02:08,440 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.24trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 00:02:17,542 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:02:17,542 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:02:17,543 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:11<00:00,  1.75trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 00:02:29,109 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 00:02:29,109 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:02:29,110 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.24trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 00:02:38,197 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 00:02:38,197 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:02:38,198 - Mean accuracy on test instances (iteration #1): 0.9329\n",
      "[INFO] 2021-04-13 00:02:38,209 - Total elapsed time: 53.4s\n",
      "[INFO] 2021-04-13 00:02:38,210 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris NB 0/5 highest_lsc_sampling\n",
      "Testando: 61_iris NB 1/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:02:43,026 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:02:43,027 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:02:43,031 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:02:43,031 - Building metadata.\n",
      "[INFO] 2021-04-13 00:02:45,572 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:02:45,613 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:02:45,613 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:02:45,613 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:02:45,614 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:02:45,615 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:08<00:00,  2.42trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 00:02:53,982 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:02:53,982 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:02:53,983 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.37trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 00:02:59,971 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:02:59,972 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:02:59,972 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.46trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 00:03:08,188 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:03:08,188 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:03:08,189 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.90trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 00:03:15,448 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:03:15,448 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:03:15,449 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.29trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 00:03:24,338 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 00:03:24,338 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:03:24,339 - Mean accuracy on test instances (iteration #1): 0.961\n",
      "[INFO] 2021-04-13 00:03:24,349 - Total elapsed time: 41.3s\n",
      "[INFO] 2021-04-13 00:03:24,349 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris NB 1/5 highest_lsc_sampling\n",
      "Testando: 61_iris NB 2/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:03:28,916 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:03:28,917 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:03:28,920 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:03:28,920 - Building metadata.\n",
      "[INFO] 2021-04-13 00:03:31,650 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:03:31,694 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:03:31,695 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:03:31,695 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:03:31,695 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:03:31,696 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.06trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 00:03:41,813 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:03:41,813 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:03:41,814 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:08<00:00,  2.46trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 00:03:50,059 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:03:50,059 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:03:50,060 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:09<00:00,  2.19trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 00:03:59,253 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:03:59,253 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:03:59,254 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.43trial/s, best loss: -0.9629629629629629]\n",
      "[INFO] 2021-04-13 00:04:07,510 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:04:07,511 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:04:07,511 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.41trial/s, best loss: -0.9166666666666666]\n",
      "[INFO] 2021-04-13 00:04:15,904 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:04:15,904 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:04:15,904 - Mean accuracy on test instances (iteration #1): 0.9333\n",
      "[INFO] 2021-04-13 00:04:15,914 - Total elapsed time: 47.0s\n",
      "[INFO] 2021-04-13 00:04:15,914 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris NB 2/5 highest_lsc_sampling\n",
      "Testando: 61_iris NB 3/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:04:20,231 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:04:20,231 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:04:20,234 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:04:20,234 - Building metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-13 00:04:22,536 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:04:22,573 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:04:22,573 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:04:22,574 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:04:22,574 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:04:22,575 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.21trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 00:04:31,820 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:04:31,821 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:04:31,821 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.45trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 00:04:40,125 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:04:40,125 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:04:40,126 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.27trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 00:04:46,450 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:04:46,450 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:04:46,450 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.66trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 00:04:54,316 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:04:54,317 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:04:54,317 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.20trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 00:05:03,487 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 00:05:03,487 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:05:03,488 - Mean accuracy on test instances (iteration #1): 0.9419\n",
      "[INFO] 2021-04-13 00:05:03,497 - Total elapsed time: 43.3s\n",
      "[INFO] 2021-04-13 00:05:03,497 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris NB 3/5 highest_lsc_sampling\n",
      "Testando: 61_iris NB 4/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:05:07,416 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:05:07,416 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:05:07,419 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:05:07,419 - Building metadata.\n",
      "[INFO] 2021-04-13 00:05:09,676 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:05:09,711 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:05:09,711 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:05:09,711 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:05:09,712 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:05:09,713 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.18trial/s, best loss: -0.9157848324514991]\n",
      "[INFO] 2021-04-13 00:05:19,103 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:05:19,103 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:05:19,104 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.67trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 00:05:26,682 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:05:26,682 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:05:26,683 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.53trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 00:05:32,526 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 00:05:32,526 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:05:32,527 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.95trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 00:05:39,339 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:05:39,339 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:05:39,340 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.54trial/s, best loss: -0.9166666666666666]\n",
      "[INFO] 2021-04-13 00:05:45,042 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 00:05:45,042 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:05:45,043 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 00:05:45,053 - Total elapsed time: 37.6s\n",
      "[INFO] 2021-04-13 00:05:45,053 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris NB 4/5 highest_lsc_sampling\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f47004d6054d7484f2547344cdd48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Bag'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando: 61_iris RF 0/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:05:49,183 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:05:49,184 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:05:49,187 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:05:49,187 - Building metadata.\n",
      "[INFO] 2021-04-13 00:05:51,625 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:05:51,665 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:05:51,666 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:05:51,666 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:05:51,666 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:05:51,667 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:11<00:00,  1.75trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-13 00:06:03,225 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:06:03,225 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:06:03,226 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.88trial/s, best loss: -0.9281305114638448]\n",
      "[INFO] 2021-04-13 00:06:13,896 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 00:06:13,896 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:06:13,898 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.07trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 00:06:23,680 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:06:23,681 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:06:23,682 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.90trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 00:06:34,359 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:06:34,359 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:06:34,360 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.30trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 00:06:43,513 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 00:06:43,513 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:06:43,514 - Mean accuracy on test instances (iteration #1): 0.9233\n",
      "[INFO] 2021-04-13 00:06:43,527 - Total elapsed time: 54.4s\n",
      "[INFO] 2021-04-13 00:06:43,527 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 0/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 1/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:06:48,680 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:06:48,681 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:06:48,684 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:06:48,684 - Building metadata.\n",
      "[INFO] 2021-04-13 00:06:51,492 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:06:51,535 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:06:51,536 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:06:51,536 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:06:51,537 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:06:51,538 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:10<00:00,  1.82trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 00:07:02,882 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:07:02,883 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:07:02,884 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.35trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 00:07:11,442 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:07:11,442 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:07:11,443 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.27trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 00:07:20,298 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:07:20,298 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:07:20,299 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.94trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 00:07:27,145 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:07:27,145 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:07:27,146 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.88trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 00:07:34,285 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 00:07:34,285 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:07:34,285 - Mean accuracy on test instances (iteration #1): 0.9514\n",
      "[INFO] 2021-04-13 00:07:34,295 - Total elapsed time: 45.6s\n",
      "[INFO] 2021-04-13 00:07:34,295 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 1/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 2/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:07:38,715 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:07:38,715 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:07:38,718 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:07:38,718 - Building metadata.\n",
      "[INFO] 2021-04-13 00:07:40,951 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:07:40,988 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:07:40,988 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:07:40,988 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:07:40,989 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:07:40,990 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.96trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 00:07:51,495 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:07:51,496 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:07:51,497 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:06<00:00,  2.90trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 00:07:58,456 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:07:58,457 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:07:58,457 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:07<00:00,  2.61trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 00:08:06,326 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:08:06,327 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:08:06,328 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.37trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 00:08:14,802 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:08:14,802 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:08:14,803 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:09<00:00,  2.11trial/s, best loss: -0.9166666666666666]\n",
      "[INFO] 2021-04-13 00:08:24,329 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:08:24,329 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:08:24,330 - Mean accuracy on test instances (iteration #1): 0.9333\n",
      "[INFO] 2021-04-13 00:08:24,340 - Total elapsed time: 45.6s\n",
      "[INFO] 2021-04-13 00:08:24,340 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 2/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 3/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:08:29,293 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:08:29,294 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:08:29,297 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:08:29,297 - Building metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-13 00:08:31,790 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:08:31,830 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:08:31,830 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:08:31,830 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:08:31,831 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:08:31,832 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.23trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 00:08:40,909 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:08:40,909 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:08:40,910 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.39trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 00:08:49,381 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:08:49,381 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:08:49,382 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:11<00:00,  1.76trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 00:09:01,022 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:09:01,022 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:09:01,023 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:10<00:00,  1.99trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 00:09:11,406 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 00:09:11,407 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:09:11,407 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.52trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 00:09:19,701 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 00:09:19,701 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:09:19,701 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 00:09:19,710 - Total elapsed time: 50.4s\n",
      "[INFO] 2021-04-13 00:09:19,710 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 3/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 4/5 highest_lsc_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 00:09:23,498 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 00:09:23,499 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 00:09:23,502 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 00:09:23,502 - Building metadata.\n",
      "[INFO] 2021-04-13 00:09:25,634 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 00:09:25,669 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 00:09:25,669 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 00:09:25,669 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 00:09:25,670 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 00:09:25,671 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.67trial/s, best loss: -0.9281305114638448]\n",
      "[INFO] 2021-04-13 00:09:33,266 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 00:09:33,266 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 00:09:33,267 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.76trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 00:09:40,654 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:09:40,654 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 00:09:40,655 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.91trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 00:09:47,606 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 00:09:47,606 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 00:09:47,607 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:08<00:00,  2.32trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 00:09:56,402 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:09:56,402 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 00:09:56,402 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:11<00:00,  1.70trial/s, best loss: -0.9047619047619048]\n",
      "[INFO] 2021-04-13 00:10:08,879 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 00:10:08,880 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 00:10:08,880 - Mean accuracy on test instances (iteration #1): 0.9524\n",
      "[INFO] 2021-04-13 00:10:08,897 - Total elapsed time: 45.4s\n",
      "[INFO] 2021-04-13 00:10:08,897 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 4/5 highest_lsc_sampling\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tqdm_datasets = tqdm(datasets, desc=\" Dataset: \"+ str(ds[:-5]))\n",
    "#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in tqdm(classifiers,  desc =\"Classifier\"):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "        \n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in tqdm(range(n_splits),  desc =\"Bag\"):\n",
    "\n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_lsc_sampling\")\n",
    "            result = highest_lsc_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_lsc_sampling\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'performance_history': 0.8556701030927835,\n",
       "  'time_elapsed': 37.909508335,\n",
       "  'classifier': '5NN',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.7938144329896907,\n",
       "  'time_elapsed': 103.27505292800001,\n",
       "  'classifier': '5NN',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8247422680412371,\n",
       "  'time_elapsed': 45.602540345999984,\n",
       "  'classifier': '5NN',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8247422680412371,\n",
       "  'time_elapsed': 38.688130755999964,\n",
       "  'classifier': '5NN',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8969072164948454,\n",
       "  'time_elapsed': 39.56893734999994,\n",
       "  'classifier': '5NN',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8865979381443299,\n",
       "  'time_elapsed': 54.707674052000016,\n",
       "  'classifier': '5NN',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8247422680412371,\n",
       "  'time_elapsed': 52.438550373,\n",
       "  'classifier': '5NN',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8865979381443299,\n",
       "  'time_elapsed': 59.81434118300001,\n",
       "  'classifier': '5NN',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.7835051546391752,\n",
       "  'time_elapsed': 52.23966101000008,\n",
       "  'classifier': 'C4.5',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.7731958762886598,\n",
       "  'time_elapsed': 67.26563275399997,\n",
       "  'classifier': 'C4.5',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9381443298969072,\n",
       "  'time_elapsed': 52.38666123500002,\n",
       "  'classifier': 'C4.5',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9690721649484536,\n",
       "  'time_elapsed': 49.155686253,\n",
       "  'classifier': 'C4.5',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 52.95006728399994,\n",
       "  'classifier': 'C4.5',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9381443298969072,\n",
       "  'time_elapsed': 57.398396261000016,\n",
       "  'classifier': 'NB',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 46.09494533299994,\n",
       "  'classifier': 'NB',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.6288659793814433,\n",
       "  'time_elapsed': 51.55088629499983,\n",
       "  'classifier': 'NB',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 47.56722559600007,\n",
       "  'classifier': 'NB',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9072164948453608,\n",
       "  'time_elapsed': 41.54863791399998,\n",
       "  'classifier': 'NB',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9896907216494846,\n",
       "  'time_elapsed': 58.69899260700004,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 1.0,\n",
       "  'time_elapsed': 50.655236637000144,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8762886597938144,\n",
       "  'time_elapsed': 50.0774309200001,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 55.30010485499997,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.979381443298969,\n",
       "  'time_elapsed': 49.63691090399993,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Uncertain Sampling',\n",
       "  'dataset': '61_iris'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_performance_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(total_performance_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('performance_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performance_history</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>classifier</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.655237</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.989691</td>\n",
       "      <td>58.698993</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.979381</td>\n",
       "      <td>49.636911</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.969072</td>\n",
       "      <td>49.155686</td>\n",
       "      <td>C4.5</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>52.950067</td>\n",
       "      <td>C4.5</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>55.300105</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>47.567226</td>\n",
       "      <td>NB</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>46.094945</td>\n",
       "      <td>NB</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.938144</td>\n",
       "      <td>57.398396</td>\n",
       "      <td>NB</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.938144</td>\n",
       "      <td>52.386661</td>\n",
       "      <td>C4.5</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.907216</td>\n",
       "      <td>41.548638</td>\n",
       "      <td>NB</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.896907</td>\n",
       "      <td>39.568937</td>\n",
       "      <td>5NN</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.886598</td>\n",
       "      <td>54.707674</td>\n",
       "      <td>5NN</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.886598</td>\n",
       "      <td>59.814341</td>\n",
       "      <td>5NN</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.876289</td>\n",
       "      <td>50.077431</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.855670</td>\n",
       "      <td>37.909508</td>\n",
       "      <td>5NN</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.824742</td>\n",
       "      <td>38.688131</td>\n",
       "      <td>5NN</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.824742</td>\n",
       "      <td>45.602540</td>\n",
       "      <td>5NN</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.824742</td>\n",
       "      <td>52.438550</td>\n",
       "      <td>5NN</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.793814</td>\n",
       "      <td>103.275053</td>\n",
       "      <td>5NN</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.783505</td>\n",
       "      <td>52.239661</td>\n",
       "      <td>C4.5</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.773196</td>\n",
       "      <td>67.265633</td>\n",
       "      <td>C4.5</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.628866</td>\n",
       "      <td>51.550886</td>\n",
       "      <td>NB</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Uncertain Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    performance_history  time_elapsed classifier  sample_size  \\\n",
       "19             1.000000     50.655237         RF     0.066667   \n",
       "18             0.989691     58.698993         RF     0.066667   \n",
       "22             0.979381     49.636911         RF     0.066667   \n",
       "11             0.969072     49.155686       C4.5     0.066667   \n",
       "12             0.958763     52.950067       C4.5     0.066667   \n",
       "21             0.958763     55.300105         RF     0.066667   \n",
       "16             0.948454     47.567226         NB     0.066667   \n",
       "14             0.948454     46.094945         NB     0.066667   \n",
       "13             0.938144     57.398396         NB     0.066667   \n",
       "10             0.938144     52.386661       C4.5     0.066667   \n",
       "17             0.907216     41.548638         NB     0.066667   \n",
       "4              0.896907     39.568937        5NN     0.066667   \n",
       "5              0.886598     54.707674        5NN     0.066667   \n",
       "7              0.886598     59.814341        5NN     0.066667   \n",
       "20             0.876289     50.077431         RF     0.066667   \n",
       "0              0.855670     37.909508        5NN     0.066667   \n",
       "3              0.824742     38.688131        5NN     0.066667   \n",
       "2              0.824742     45.602540        5NN     0.066667   \n",
       "6              0.824742     52.438550        5NN     0.066667   \n",
       "1              0.793814    103.275053        5NN     0.066667   \n",
       "8              0.783505     52.239661       C4.5     0.066667   \n",
       "9              0.773196     67.265633       C4.5     0.066667   \n",
       "15             0.628866     51.550886         NB     0.066667   \n",
       "\n",
       "              Strategy  dataset  \n",
       "19  Uncertain Sampling  61_iris  \n",
       "18  Uncertain Sampling  61_iris  \n",
       "22  Uncertain Sampling  61_iris  \n",
       "11  Uncertain Sampling  61_iris  \n",
       "12  Uncertain Sampling  61_iris  \n",
       "21  Uncertain Sampling  61_iris  \n",
       "16  Uncertain Sampling  61_iris  \n",
       "14  Uncertain Sampling  61_iris  \n",
       "13  Uncertain Sampling  61_iris  \n",
       "10  Uncertain Sampling  61_iris  \n",
       "17  Uncertain Sampling  61_iris  \n",
       "4   Uncertain Sampling  61_iris  \n",
       "5   Uncertain Sampling  61_iris  \n",
       "7   Uncertain Sampling  61_iris  \n",
       "20  Uncertain Sampling  61_iris  \n",
       "0   Uncertain Sampling  61_iris  \n",
       "3   Uncertain Sampling  61_iris  \n",
       "2   Uncertain Sampling  61_iris  \n",
       "6   Uncertain Sampling  61_iris  \n",
       "1   Uncertain Sampling  61_iris  \n",
       "8   Uncertain Sampling  61_iris  \n",
       "9   Uncertain Sampling  61_iris  \n",
       "15  Uncertain Sampling  61_iris  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Strategy != \"Query by Committee\"].sort_values('performance_history', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Strategy == \"Expected Error Reduction\"].sort_values('time_elapsed', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 514x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.relplot(\n",
    "    data= df,\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=1), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df[(df.Strategy != \"Uncertain Sampling\") & (df.Strategy != \"Query by Committee\")],\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=3), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df[(df.Strategy == \"Uncertain Sampling\") | (df.Strategy == \"Query by Committee\")],\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=2), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "p_bar = tqdm(datalist)\n",
    "for dataset_id in p_bar:\n",
    "    X_raw, y_raw, idx_data, dataset_name = which_oml_dataset(dataset_id)\n",
    "    p_bar.set_description(f'\"{dataset_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"1465_breast-tissue.arff\"\n",
    "\n",
    "X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "   \n",
    "from modAL.uncertainty import classifier_uncertainty\n",
    "\n",
    "print(len(np.unique(y_raw)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)), stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "print(y_train)\n",
    "\n",
    "learner = ActiveLearner (\n",
    "    estimator= which_classifier(classifier), #cls,\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    ")\n",
    "\n",
    "uncertain_sample_score = learner.score(X_test, y_test)\n",
    "\n",
    "total_of_samples = 1\n",
    "while (total_of_samples != cost):\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "        #print(\"IF\", learner.score(X_test, y_test))\n",
    "        learner.teach(X_train, y_train)\n",
    "        uncertain_sample_score = learner.score(X_test, y_test)\n",
    "        performance_history.append(uncertain_sample_score)\n",
    "    total_of_samples = total_of_samples + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size= len(np.unique(y_raw)) + init_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
