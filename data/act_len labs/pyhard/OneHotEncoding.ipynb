{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atmospheric-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i set_environment\n",
    "%run -i importing_libraries\n",
    "%run -i classifiers\n",
    "%run -i importing_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nominated-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datasets(dataset):\n",
    "    \n",
    "    data = arff.loadarff('./datasets/luis/' + dataset)\n",
    "    metadata = data[1]\n",
    "    data = pd.DataFrame(data[0])\n",
    "    \n",
    "    instances = len(data)\n",
    "    classes = len(data.iloc[:,-1].value_counts())\n",
    "    attributes = len(data.columns)- 1\n",
    "    nominal_attributes = str(metadata).count(\"nominal\")\n",
    "    \n",
    "    proportion = data.iloc[:,-1].value_counts()\n",
    "    proportion = proportion.map(lambda x: round(x/instances*100,2))\n",
    "\n",
    "    majority = max(proportion)\n",
    "    minority = min(proportion)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"name\": dataset[:-5],\n",
    "        \"instances\": instances,\n",
    "        \"classes\": classes,\n",
    "        \"attributes\": attributes,\n",
    "        \"nominal attributes\": nominal_attributes,\n",
    "        \"majority\": majority,\n",
    "        \"minority\": minority\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "whole-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_arff_dataset(dataset, n_splits = 5):\n",
    "   \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    data = arff.loadarff('datasets/luis/' + dataset)\n",
    "    metadata = data[1]\n",
    "    data = pd.DataFrame(data[0])\n",
    "\n",
    "    X_raw = data[data.columns[:-1]]\n",
    "    y_raw = data[data.columns[-1]].to_numpy()\n",
    "    \n",
    "    metadataSplit = str(metadata).split('\\n')\n",
    "    \n",
    "    numericOrCategorical = []\n",
    "    for x in range(1,len(data.columns)):\n",
    "        if (metadataSplit[x].find('numeric')) == -1:\n",
    "            numericOrCategorical.append(0) # categorical\n",
    "        else:\n",
    "            numericOrCategorical.append(1) # numeric\n",
    "                               \n",
    "    print(numericOrCategorical)\n",
    "\n",
    "    print(X_raw)\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    enc.fit(X_raw)\n",
    "    print(X_raw)\n",
    "    print(enc.transform(X_raw))\n",
    "    X_raw2 = pd.DataFrame()\n",
    "    for x in range(len(data.columns)-1):\n",
    "        print(x)\n",
    "        if numericOrCategorical[x] == 0:\n",
    "            X_raw2.append(enc.transform(X_raw.iloc(x)))\n",
    "        else:\n",
    "            X_raw2[X_raw.columns[x]] = X_raw.iloc(x)\n",
    "\n",
    "#     lex = preprocessing.OrdinalEncoder()\n",
    "#     lex.fit(X_raw)\n",
    "#     X_raw = lex.transform(X_raw)\n",
    "\n",
    "    print(X_raw2)\n",
    "        \n",
    "    ley = preprocessing.LabelEncoder()\n",
    "    ley.fit(y_raw)\n",
    "    y_raw = ley.transform(y_raw)\n",
    "    \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.2, random_state=0) #n_splits\n",
    "    data_cv.get_n_splits(X_raw,y_raw)\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw, y_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "italian-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('./datasets/luis')\n",
    "classifiers = ['5NN', 'C4.5', 'NB','RF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "divided-scanner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>instances</th>\n",
       "      <th>classes</th>\n",
       "      <th>attributes</th>\n",
       "      <th>nominal attributes</th>\n",
       "      <th>majority</th>\n",
       "      <th>minority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1115_teachingAssistant</td>\n",
       "      <td>151</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>34.44</td>\n",
       "      <td>32.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name  instances  classes  attributes  nominal attributes  \\\n",
       "0  1115_teachingAssistant        151        3           6                   5   \n",
       "\n",
       "   majority  minority  \n",
       "0     34.44     32.45  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = []\n",
    "\n",
    "for ds in datasets:\n",
    "    metadata.append(fetch_datasets(ds))\n",
    "\n",
    "metadata = pd.DataFrame.from_dict(metadata)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "focal-syndrome",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 1]\n",
      "        ID EnglishSepaker courseInstructor course summer  classSize\n",
      "0      1.0           b'1'            b'23'   b'3'   b'1'       19.0\n",
      "1      2.0           b'2'            b'15'   b'3'   b'1'       17.0\n",
      "2      3.0           b'1'            b'23'   b'3'   b'2'       49.0\n",
      "3      4.0           b'1'             b'5'   b'2'   b'2'       33.0\n",
      "4      5.0           b'2'             b'7'  b'11'   b'2'       55.0\n",
      "..     ...            ...              ...    ...    ...        ...\n",
      "146  147.0           b'2'             b'3'   b'2'   b'2'       26.0\n",
      "147  148.0           b'2'            b'10'   b'3'   b'2'       12.0\n",
      "148  149.0           b'1'            b'18'   b'7'   b'2'       48.0\n",
      "149  150.0           b'2'            b'22'   b'1'   b'2'       51.0\n",
      "150  151.0           b'2'             b'2'  b'10'   b'2'       27.0\n",
      "\n",
      "[151 rows x 6 columns]\n",
      "        ID EnglishSepaker courseInstructor course summer  classSize\n",
      "0      1.0           b'1'            b'23'   b'3'   b'1'       19.0\n",
      "1      2.0           b'2'            b'15'   b'3'   b'1'       17.0\n",
      "2      3.0           b'1'            b'23'   b'3'   b'2'       49.0\n",
      "3      4.0           b'1'             b'5'   b'2'   b'2'       33.0\n",
      "4      5.0           b'2'             b'7'  b'11'   b'2'       55.0\n",
      "..     ...            ...              ...    ...    ...        ...\n",
      "146  147.0           b'2'             b'3'   b'2'   b'2'       26.0\n",
      "147  148.0           b'2'            b'10'   b'3'   b'2'       12.0\n",
      "148  149.0           b'1'            b'18'   b'7'   b'2'       48.0\n",
      "149  150.0           b'2'            b'22'   b'1'   b'2'       51.0\n",
      "150  151.0           b'2'             b'2'  b'10'   b'2'       27.0\n",
      "\n",
      "[151 rows x 6 columns]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=<pandas.core.indexing._iLocIndexer object at 0x7fdb6b64c230>.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/importing_datasets.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhich_arff_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/importing_datasets.py\u001b[0m in \u001b[0;36mwhich_arff_dataset\u001b[0;34m(dataset, n_splits)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumericOrCategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mX_raw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mX_raw2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown,\n\u001b[0;32m--> 462\u001b[0;31m                                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown, force_all_finite)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         X_list, n_samples, n_features = self._check_X(\n\u001b[0;32m--> 114\u001b[0;31m             X, force_all_finite=force_all_finite)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mX_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             X_temp = check_array(X, dtype=None,\n\u001b[0;32m---> 45\u001b[0;31m                                  force_all_finite=force_all_finite)\n\u001b[0m\u001b[1;32m     46\u001b[0m             if (not hasattr(X, 'dtype')\n\u001b[1;32m     47\u001b[0m                     and np.issubdtype(X_temp.dtype, np.str_)):\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=<pandas.core.indexing._iLocIndexer object at 0x7fdb6b64c230>.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "pyhard_strategies_names = ['H','U','H+U','LSC','N2','F3']\n",
    "\n",
    "for ds in datasets:\n",
    "    X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "46a641ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(a):\n",
    "    a = 12\n",
    "    return a\n",
    "    \n",
    "def test2():\n",
    "    test1()\n",
    "    a = 1243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2727ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_list_param(la):\n",
    "    test2('b')\n",
    "    la.append('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04675222",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = {\"test1\": \"test2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5407eb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "a = eval(list(teste.keys())[0] + \"(a)\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ef403f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertain Sampling uncertain_sampling\n",
      "Random Sampling random_sampling\n",
      "Query by Committee query_by_committee\n",
      "Expected Error Reduction exp_error_reduction\n",
      "Expected Model Change exp_model_change\n"
     ]
    }
   ],
   "source": [
    "modal_strategies = {\"Uncertain Sampling\":\"uncertain_sampling\", \"Random Sampling\":\"random_sampling\", \"Query by Committee\":\"query_by_committee\", \"Expected Error Reduction\":\"exp_error_reduction\", \"Expected Model Change\":\"exp_model_change\"}\n",
    "for k,v in modal_strategies.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51143ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
