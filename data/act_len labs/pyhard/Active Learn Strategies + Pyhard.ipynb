{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning - Comparando estratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amostra por incerteza\n",
    "- Amostragem aleatória\n",
    "- Consulta por comitê\n",
    "- Aprendizado passivo\n",
    "- Redução do erro esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run set_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing_libraries.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets OpenML\n",
    "import openml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml.config.cache_directory = os.path.expanduser('./datasets/openML')\n",
    "openml_list = openml.datasets.list_datasets()\n",
    "\n",
    "datalist = pd.DataFrame.from_dict(openml_list, orient=\"index\")\n",
    "datalist = list(datalist[(datalist.NumberOfClasses.isnull() == False) & (datalist.NumberOfClasses != 0)][\"did\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostra por incerteza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertain_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][TRAIN][initial_idx]]\n",
    "    #X_test, y_test = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    #cls = which_classifier(classifier)\n",
    "    #cls.fit(X_train,y_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "        \n",
    "        idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "        X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "        \n",
    "        if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "            #print(\"IF\", learner.score(X_test, y_test))\n",
    "            sample_size = sample_size + len(X_train)\n",
    "            learner.teach(X_train, y_train)\n",
    "            uncertain_sample_score = learner.score(X_test, y_test)\n",
    "            performance_history.append(uncertain_sample_score)\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Uncertain Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostragem aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "        \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "\n",
    "    for i in range(1, cost+1):\n",
    "\n",
    "        #high = X_raw.shape[0] = qtd amostras no dataset\n",
    "        #training_indices = np.random.randint(low=0, high=len(X_raw[idx_data[idx_bag][TRAIN]]), size=k+i) #high = qtd elementos na bag\n",
    "        #sample_size = sample_size + len(training_indices)\n",
    "        #X_train = X_raw[idx_data[idx_bag][TRAIN][training_indices]] #ASK06\n",
    "        #y_train = y_raw[idx_data[idx_bag][TRAIN][training_indices]]\n",
    "        #X_test = np.delete(X_raw, idx_data[idx_bag][TRAIN][training_indices], axis=0)\n",
    "        #y_test = np.delete(y_raw, idx_data[idx_bag][TRAIN][training_indices], axis=0)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "        \n",
    "        cls = which_classifier(classifier)\n",
    "        cls.fit(X_train, y_train)\n",
    "\n",
    "        random_sampling_score = cls.score(X_test,y_test)\n",
    "        performance_history.append(random_sampling_score)\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw),\n",
    "             \"Strategy\": \"Random Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta por comitê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_by_committee(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.models import ActiveLearner, Committee\n",
    "    from modAL.disagreement import vote_entropy_sampling\n",
    "\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "\n",
    "    learner_list = []\n",
    "\n",
    "    for j in range(1, cost+1): # Loop para criação do comitê\n",
    "\n",
    "        X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "\n",
    "        # initializing learner\n",
    "        learner = ActiveLearner(\n",
    "            estimator= which_classifier(classifier),\n",
    "            X_training = X_train, y_training = y_train \n",
    "        )\n",
    "        learner_list.append(learner)\n",
    "\n",
    "    # assembling the committee\n",
    "    committee = Committee(\n",
    "        learner_list=learner_list,\n",
    "        query_strategy=vote_entropy_sampling)\n",
    "\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]]\n",
    "    \n",
    "    # query by committee\n",
    "    for idx in range(cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        query_idx, query_instance = committee.query(X_pool, n_instances = init_size+1)\n",
    "        sample_size = sample_size + len(query_idx)\n",
    "        \n",
    "        committee.teach(\n",
    "            X = X_pool[query_idx],\n",
    "            y = y_pool[query_idx]\n",
    "        )\n",
    "\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx)\n",
    "        \n",
    "        query_by_committee_score = committee.score(X_pool, y_pool)\n",
    "        performance_history.append(query_by_committee_score)\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw),\n",
    "             \"Strategy\": \"Query by Committee\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mX_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhich_arff_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "%run -i main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Error Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_error_reduction(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][TRAIN][initial_idx]]\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    exp_er_score = learner.score(X_pool, y_pool)\n",
    "    performance_history.append(exp_er_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = expected_error_reduction(learner, X_pool, 'binary', n_instances=init_size)[0]\n",
    "\n",
    "        learner.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        sample_size = sample_size + init_size\n",
    "    \n",
    "        X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, exp_error_idx)\n",
    "        \n",
    "        exp_er_score = learner.score(X_pool, y_pool)\n",
    "        performance_history.append(exp_er_score)\n",
    "        \n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Expected Error Reduction\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Model Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_model_change(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][0][initial_idx]]\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    \n",
    "#     performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = np.random.choice(range(len(X_pool)), size=init_size, replace=False)\n",
    "        aux = deepcopy(learner)\n",
    "\n",
    "        aux.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        score_aux = aux.score(X_pool, y_pool)\n",
    "        score_learner = learner.score(X_pool, y_pool)\n",
    "\n",
    "        if score_aux > score_learner:\n",
    "            learner = deepcopy(aux)\n",
    "            sample_size = sample_size + init_size\n",
    "        \n",
    "        X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, exp_error_idx, axis=0)\n",
    "        \n",
    "        exp_mo_score = learner.score(X_pool, y_pool)\n",
    "        performance_history.append(exp_mo_score)\n",
    "\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Expected Model Change\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query highest LSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_lsc_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure('LSC')\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by='feature_LSC', ascending=False)['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Highest LSC Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query highest Usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_usefulness_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    print(type(X_raw[idx_data[idx_bag][TRAIN]]))\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure('Usefulness')\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by='feature_Usefulness', ascending=False)['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Highest Usefulness Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query lowest Harmfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_harmfulness_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    print(type(X_raw[idx_data[idx_bag][TRAIN]]))\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure('Harmfulness')\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by='feature_Harmfulness', ascending=True)['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Lowest Harmfulness Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_pyhard_measure(measure='LSC'):\n",
    "    import yaml\n",
    "    with open(r'config-template.yaml') as file:\n",
    "        configs_list = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "        if measure == 'LSC':\n",
    "            configs_list['measures_list'] = ['LSC']\n",
    "        elif measure == 'Harmfulness':\n",
    "            configs_list['measures_list'] = ['Harmfulness']\n",
    "        elif measure == 'Usefulness':\n",
    "            configs_list['measures_list'] = ['Usefulness']\n",
    "\n",
    "    with open(r'config.yaml', 'w') as file:\n",
    "        yaml.dump(configs_list, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_dataset(dataset = \"iris\", n_splits = 5):\n",
    "    \n",
    "    # Futuramente essa etapa será ajustada para receber qualquer dataset (ou lista com datasets)\n",
    "    if (dataset == \"iris\"):\n",
    "        data = load_iris()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "    \n",
    "    if (dataset == \"wine\"):\n",
    "        data = load_wine()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    if (dataset == \"digits\"):\n",
    "        data = load_digits()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_oml_dataset(dataset_id, n_splits = 5):\n",
    "    data = openml.datasets.get_dataset(dataset_id)\n",
    "    \n",
    "    X_raw, y_raw, categorical_indicator, attribute_names = data.get_data(\n",
    "    dataset_format=\"array\", target=data.default_target_attribute)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_raw)\n",
    "    y_raw = le.transform(y_raw)\n",
    "    \n",
    "    X_raw = np.nan_to_num(X_raw)\n",
    "    \n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_arff_dataset(dataset, n_splits = 5):\n",
    "   \n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    \n",
    "    data = arff.loadarff('datasets/luis/' + dataset)\n",
    "    data = pd.DataFrame(data[0])\n",
    "\n",
    "    X_raw = data[data.columns[:-1]].to_numpy()\n",
    "    y_raw = data[data.columns[-1]].to_numpy()\n",
    "    \n",
    "    lex = preprocessing.OrdinalEncoder()\n",
    "    lex.fit(X_raw)\n",
    "    X_raw = lex.transform(X_raw)\n",
    "        \n",
    "    ley = preprocessing.LabelEncoder()\n",
    "    ley.fit(y_raw)\n",
    "    y_raw = ley.transform(y_raw)\n",
    "    \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    data_cv.get_n_splits(X_raw,y_raw)\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw, y_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_classifier(classifier = '5NN'):\n",
    "    \n",
    "    if (classifier == '5NN'):\n",
    "        return KNeighborsClassifier(5)\n",
    "    elif (classifier == 'C4.5'):\n",
    "        return tree.DecisionTreeClassifier()\n",
    "    elif (classifier == 'NB'):\n",
    "        return GaussianNB()\n",
    "    elif (classifier == 'SVM'):\n",
    "        return SVC(probability=True, gamma='auto')\n",
    "    elif (classifier == 'RF'):\n",
    "        return RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datasets(dataset):\n",
    "    \n",
    "    data = arff.loadarff('./datasets/luis/' + dataset)\n",
    "    metadata = data[1]\n",
    "    data = pd.DataFrame(data[0])\n",
    "    \n",
    "    instances = len(data)\n",
    "    classes = len(data.iloc[:,-1].value_counts())\n",
    "    attributes = len(data.columns)- 1\n",
    "    nominal_attributes = str(metadata).count(\"nominal\")\n",
    "    \n",
    "    proportion = data.iloc[:,-1].value_counts()\n",
    "    proportion = proportion.map(lambda x: round(x/instances*100,2))\n",
    "\n",
    "    majority = max(proportion)\n",
    "    minority = min(proportion)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"name\": dataset[:-5],\n",
    "        \"instances\": instances,\n",
    "        \"classes\": classes,\n",
    "        \"attributes\": attributes,\n",
    "        \"nominal attributes\": nominal_attributes,\n",
    "        \"majority\": majority,\n",
    "        \"minority\": minority\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Teste\n",
    "\n",
    "total_performance_history = []\n",
    "\n",
    "classifiers = ['SVM']\n",
    "ds = '3_kr-vs-kp.arff'\n",
    "for classifier in classifiers:\n",
    "    X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "    for idx_bag in range(n_splits):\n",
    "        print(ds, classifier, \" \", idx_bag, \" \", n_splits, \" uncertain_sampling\")\n",
    "        result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "        result['dataset'] = ds\n",
    "        total_performance_history.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('./datasets/luis')\n",
    "classifiers = ['5NN', 'C4.5', 'NB','RF']\n",
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['61_iris.arff']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>instances</th>\n",
       "      <th>classes</th>\n",
       "      <th>attributes</th>\n",
       "      <th>nominal attributes</th>\n",
       "      <th>majority</th>\n",
       "      <th>minority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61_iris</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>33.33</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  instances  classes  attributes  nominal attributes  majority  \\\n",
       "0  61_iris        150        3           4                   1     33.33   \n",
       "\n",
       "   minority  \n",
       "0     33.33  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = []\n",
    "\n",
    "for ds in datasets:\n",
    "    metadata.append(fetch_datasets(ds))\n",
    "\n",
    "metadata = pd.DataFrame.from_dict(metadata)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for ds in tqdm(datasets):\n",
    "    for classifier in tqdm(classifiers):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" uncertain_sampling\")\n",
    "            result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" random sampling\")\n",
    "            result = random_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" query_by_committee\")\n",
    "            result = query_by_committee(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" exp error reduction\")\n",
    "            result = exp_error_reduction(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" exp model change\")\n",
    "            result = exp_model_change(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee1f3726f0c44d686ea06ac1707c47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dataset'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d861727b709e449c85478baed0683818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Classifier'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277538756c0f4bfba7d95ae66ee41ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Bag'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "\r",
      "Testando: 61_iris 5NN 0/5 exp_error_reduction\n",
      "\t Size of X_pool: 45\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(1) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testando: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_bag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" exp_error_reduction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_error_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_bag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtotal_performance_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/main.py\u001b[0m in \u001b[0;36mexp_error_reduction\u001b[0;34m(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mexp_error_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected_error_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_instances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp_error_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp_error_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minit_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/modAL/models/learners.py\u001b[0m in \u001b[0;36mteach\u001b[0;34m(self, X, y, bootstrap, only_new, **fit_kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfit\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0monly_new\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_to_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/modAL/models/base.py\u001b[0m in \u001b[0;36m_add_training_data\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mclassifier\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m         check_X_y(X, y, accept_sparse=True, ensure_2d=False, allow_nd=True, multi_output=True, dtype=None,\n\u001b[0m\u001b[1;32m     89\u001b[0m                   force_all_finite=self.force_all_finite)\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    821\u001b[0m                     estimator=estimator)\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n\u001b[0m\u001b[1;32m    824\u001b[0m                         ensure_2d=False, dtype=None)\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n",
      "\u001b[0;32m~/anaconda3/envs/ALEnv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0m\u001b[1;32m    203\u001b[0m                             \" a valid collection.\" % x)\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(1) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "#tqdm_datasets = tqdm(datasets, desc=\" Dataset: \"+ str(ds[:-5]))\n",
    "#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in tqdm(classifiers,  desc =\"Classifier\"):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in tqdm(range(n_splits),  desc =\"Bag\"):\n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" uncertain_sampling\")\n",
    "#             result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" uncertain_sampling\")\n",
    "            \n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" random_sampling\")\n",
    "#             result = random_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" random_sampling\")\n",
    "            \n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" query_by_committee\")\n",
    "#             result = query_by_committee(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" query_by_committee\")\n",
    "\n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_error_reduction\")\n",
    "            result = exp_error_reduction(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_error_reduction\")\n",
    "            \n",
    "#             tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_model_change\")\n",
    "#             result = exp_model_change(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "#             result['dataset'] = ds[:-5]\n",
    "#             total_performance_history.append(result)\n",
    "#             tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" exp_model_change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['RF']\n",
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d151677b61a445ebaa6cf6b930db768a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dataset'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215aedc76f6c49189a61c4cdd93bcb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Classifier'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417baba72081479cb21a398c210e42bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Bag'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando: 61_iris RF 0/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:00:08,750 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:00:08,751 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:00:08,753 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:00:08,753 - Building metadata.\n",
      "[INFO] 2021-04-13 21:00:10,108 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 21:00:10,130 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:00:10,130 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:00:10,130 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:00:10,131 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:00:10,131 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.37trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-13 21:00:16,224 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:00:16,225 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:00:16,225 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.41trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 21:00:20,987 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:00:20,987 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:00:20,988 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.82trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 21:00:26,306 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:00:26,306 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:00:26,307 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.03trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:00:31,352 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:00:31,353 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:00:31,353 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.36trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 21:00:36,105 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 21:00:36,105 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:00:36,106 - Mean accuracy on test instances (iteration #1): 0.9424\n",
      "[INFO] 2021-04-13 21:00:36,112 - Total elapsed time: 27.4s\n",
      "[INFO] 2021-04-13 21:00:36,112 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 0/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 0/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:00:38,987 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:00:38,988 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:00:38,990 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:00:38,990 - Building metadata.\n",
      "[INFO] 2021-04-13 21:00:40,415 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 21:00:40,417 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:00:40,417 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:00:40,417 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:00:40,417 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:00:40,418 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.00trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 21:00:47,191 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:00:47,191 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:00:47,191 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.40trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 21:00:51,920 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:00:51,920 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:00:51,920 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.95trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 21:00:57,038 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:00:57,038 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:00:57,039 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.88trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:01:02,256 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:01:02,256 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:01:02,257 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.07trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 21:01:07,256 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 21:01:07,256 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:01:07,257 - Mean accuracy on test instances (iteration #1): 0.9424\n",
      "[INFO] 2021-04-13 21:01:07,262 - Total elapsed time: 28.3s\n",
      "[INFO] 2021-04-13 21:01:07,262 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 0/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 0/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:01:10,171 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:01:10,171 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:01:10,173 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:01:10,173 - Building metadata.\n",
      "[INFO] 2021-04-13 21:01:11,691 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 21:01:11,692 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:01:11,692 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:01:11,692 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:01:11,693 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:01:11,693 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.28trial/s, best loss: -0.9157848324514992]\n",
      "[INFO] 2021-04-13 21:01:18,025 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:01:18,026 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:01:18,026 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.74trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 21:01:23,596 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:01:23,596 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:01:23,597 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.49trial/s, best loss: -0.9400352733686067]\n",
      "[INFO] 2021-04-13 21:01:28,114 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:01:28,114 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:01:28,115 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  4.00trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:01:33,170 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 21:01:33,170 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:01:33,171 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.51trial/s, best loss: -0.9285714285714287]\n",
      "[INFO] 2021-04-13 21:01:39,010 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 21:01:39,010 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:01:39,010 - Mean accuracy on test instances (iteration #1): 0.9329\n",
      "[INFO] 2021-04-13 21:01:39,015 - Total elapsed time: 28.8s\n",
      "[INFO] 2021-04-13 21:01:39,015 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 0/5 lowest_harmfulness_sampling\n",
      "Testando: 61_iris RF 1/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:01:41,934 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:01:41,934 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:01:41,936 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:01:41,936 - Building metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-13 21:01:43,515 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 21:01:43,538 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:01:43,538 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:01:43,538 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:01:43,539 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:01:43,539 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:05<00:00,  3.34trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 21:01:49,764 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:01:49,764 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:01:49,765 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.64trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 21:01:55,283 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:01:55,283 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:01:55,283 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.44trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 21:01:59,809 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:01:59,809 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:01:59,809 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.18trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 21:02:04,786 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:02:04,786 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:02:04,787 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.81trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 21:02:10,290 - Test fold mean accuracy: 0.85\n",
      "[INFO] 2021-04-13 21:02:10,290 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:02:10,291 - Mean accuracy on test instances (iteration #1): 0.951\n",
      "[INFO] 2021-04-13 21:02:10,297 - Total elapsed time: 28.4s\n",
      "[INFO] 2021-04-13 21:02:10,297 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 1/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 1/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:02:13,159 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:02:13,160 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:02:13,161 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:02:13,161 - Building metadata.\n",
      "[INFO] 2021-04-13 21:02:14,631 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 21:02:14,632 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:02:14,633 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:02:14,633 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:02:14,633 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:02:14,634 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:05<00:00,  3.52trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 21:02:20,435 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:02:20,435 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:02:20,435 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.34trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 21:02:25,079 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:02:25,080 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:02:25,080 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.68trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 21:02:30,544 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:02:30,544 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:02:30,545 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.65trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 21:02:36,251 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:02:36,252 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:02:36,252 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.23trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 21:02:41,071 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 21:02:41,071 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:02:41,071 - Mean accuracy on test instances (iteration #1): 0.961\n",
      "[INFO] 2021-04-13 21:02:41,076 - Total elapsed time: 27.9s\n",
      "[INFO] 2021-04-13 21:02:41,076 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 1/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 1/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:02:44,203 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:02:44,204 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:02:44,206 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:02:44,206 - Building metadata.\n",
      "[INFO] 2021-04-13 21:02:45,681 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 21:02:45,682 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:02:45,683 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:02:45,683 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:02:45,683 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:02:45,684 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:06<00:00,  3.25trial/s, best loss: -0.939594356261023]\n",
      "[INFO] 2021-04-13 21:02:51,900 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:02:51,900 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:02:51,900 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.38trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 21:02:56,500 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:02:56,500 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:02:56,501 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.78trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 21:03:01,832 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:03:01,832 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:03:01,833 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.14trial/s, best loss: -0.9519400352733686]\n",
      "[INFO] 2021-04-13 21:03:06,724 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:03:06,725 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:03:06,725 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.00trial/s, best loss: -0.9761904761904763]\n",
      "[INFO] 2021-04-13 21:03:11,975 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 21:03:11,975 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:03:11,975 - Mean accuracy on test instances (iteration #1): 0.9514\n",
      "[INFO] 2021-04-13 21:03:11,981 - Total elapsed time: 27.8s\n",
      "[INFO] 2021-04-13 21:03:11,981 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 1/5 lowest_harmfulness_sampling\n",
      "Testando: 61_iris RF 2/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:03:14,842 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:03:14,843 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:03:14,844 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:03:14,844 - Building metadata.\n",
      "[INFO] 2021-04-13 21:03:16,309 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 21:03:16,333 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:03:16,333 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:03:16,333 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:03:16,334 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:03:16,334 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.14trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:03:22,733 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:03:22,733 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:03:22,734 - Optimizing classifier hyper-parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 20/20 [00:05<00:00,  3.62trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 21:03:28,302 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:03:28,302 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:03:28,303 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:03<00:00,  5.11trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:03:32,256 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:03:32,256 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:03:32,257 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.77trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:03:37,588 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:03:37,589 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:03:37,589 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.02trial/s, best loss: -0.9166666666666666]\n",
      "[INFO] 2021-04-13 21:03:42,605 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:03:42,605 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:03:42,605 - Mean accuracy on test instances (iteration #1): 0.9429\n",
      "[INFO] 2021-04-13 21:03:42,611 - Total elapsed time: 27.8s\n",
      "[INFO] 2021-04-13 21:03:42,611 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 2/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 2/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:03:45,480 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:03:45,480 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:03:45,482 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:03:45,482 - Building metadata.\n",
      "[INFO] 2021-04-13 21:03:47,413 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 21:03:47,416 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:03:47,416 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:03:47,416 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:03:47,417 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:03:47,417 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.54trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:03:55,349 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:03:55,349 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:03:55,350 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.03trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:04:02,209 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:04:02,209 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:04:02,209 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:05<00:00,  3.36trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 21:04:08,294 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:04:08,294 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:04:08,294 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.78trial/s, best loss: -0.9272486772486772]\n",
      "[INFO] 2021-04-13 21:04:15,624 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:04:15,624 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:04:15,625 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.08trial/s, best loss: -0.9285714285714285]\n",
      "[INFO] 2021-04-13 21:04:22,175 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:04:22,175 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:04:22,176 - Mean accuracy on test instances (iteration #1): 0.9429\n",
      "[INFO] 2021-04-13 21:04:22,183 - Total elapsed time: 36.7s\n",
      "[INFO] 2021-04-13 21:04:22,183 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 2/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 2/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:04:25,447 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:04:25,447 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:04:25,449 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:04:25,449 - Building metadata.\n",
      "[INFO] 2021-04-13 21:04:27,157 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 21:04:27,159 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:04:27,159 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:04:27,159 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:04:27,160 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:04:27,160 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.22trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:04:33,420 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:04:33,420 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:04:33,421 - Optimizing classifier hyper-parameters\n",
      "100%|█████████| 20/20 [00:06<00:00,  3.07trial/s, best loss: -0.951058201058201]\n",
      "[INFO] 2021-04-13 21:04:40,054 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:04:40,054 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:04:40,054 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.09trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:04:46,590 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:04:46,590 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:04:46,591 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  2.96trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:04:53,376 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:04:53,376 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:04:53,377 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.20trial/s, best loss: -0.9285714285714285]\n",
      "[INFO] 2021-04-13 21:04:59,654 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:04:59,654 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:04:59,654 - Mean accuracy on test instances (iteration #1): 0.9333\n",
      "[INFO] 2021-04-13 21:04:59,660 - Total elapsed time: 34.2s\n",
      "[INFO] 2021-04-13 21:04:59,660 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 2/5 lowest_harmfulness_sampling\n",
      "Testando: 61_iris RF 3/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:05:03,045 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:05:03,045 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:05:03,047 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:05:03,047 - Building metadata.\n",
      "[INFO] 2021-04-13 21:05:04,777 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 21:05:04,815 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:05:04,815 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:05:04,815 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:05:04,816 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:05:04,816 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.02trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 21:05:11,670 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:05:11,671 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:05:11,671 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.43trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 21:05:16,424 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:05:16,424 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:05:16,424 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.15trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:05:21,315 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:05:21,315 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:05:21,316 - Optimizing classifier hyper-parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 20/20 [00:05<00:00,  3.37trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:05:27,488 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:05:27,488 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:05:27,489 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.32trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 21:05:33,782 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 21:05:33,782 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:05:33,782 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 21:05:33,788 - Total elapsed time: 30.7s\n",
      "[INFO] 2021-04-13 21:05:33,788 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 3/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 3/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:05:36,808 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:05:36,808 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:05:36,810 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:05:36,810 - Building metadata.\n",
      "[INFO] 2021-04-13 21:05:38,325 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 21:05:38,327 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:05:38,327 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:05:38,327 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:05:38,328 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:05:38,328 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.84trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 21:05:43,606 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:05:43,606 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:05:43,607 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.02trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 21:05:48,690 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:05:48,690 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:05:48,691 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.21trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:05:53,584 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:05:53,584 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:05:53,585 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.87trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:05:58,966 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:05:58,966 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:05:58,967 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.37trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 21:06:03,615 - Test fold mean accuracy: 0.9\n",
      "[INFO] 2021-04-13 21:06:03,615 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:06:03,615 - Mean accuracy on test instances (iteration #1): 0.9419\n",
      "[INFO] 2021-04-13 21:06:03,621 - Total elapsed time: 26.8s\n",
      "[INFO] 2021-04-13 21:06:03,621 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 3/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 3/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:06:06,603 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:06:06,603 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:06:06,605 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:06:06,605 - Building metadata.\n",
      "[INFO] 2021-04-13 21:06:08,129 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 21:06:08,130 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:06:08,130 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:06:08,130 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:06:08,131 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:06:08,131 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.33trial/s, best loss: -0.9268077601410935]\n",
      "[INFO] 2021-04-13 21:06:14,268 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:06:14,268 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:06:14,269 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.09trial/s, best loss: -0.9634038800705468]\n",
      "[INFO] 2021-04-13 21:06:20,944 - Test fold mean accuracy: 0.9047619047619048\n",
      "[INFO] 2021-04-13 21:06:20,944 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:06:20,944 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.65trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:06:26,592 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:06:26,592 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:06:26,593 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.50trial/s, best loss: -0.9514991181657848]\n",
      "[INFO] 2021-04-13 21:06:32,506 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:06:32,506 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:06:32,506 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.09trial/s, best loss: -0.9523809523809524]\n",
      "[INFO] 2021-04-13 21:06:37,627 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 21:06:37,628 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:06:37,628 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 21:06:37,634 - Total elapsed time: 31.0s\n",
      "[INFO] 2021-04-13 21:06:37,634 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 3/5 lowest_harmfulness_sampling\n",
      "Testando: 61_iris RF 4/5 highest_lsc_sampling\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:06:40,631 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:06:40,631 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:06:40,633 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:06:40,633 - Building metadata.\n",
      "[INFO] 2021-04-13 21:06:42,137 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-13 21:06:42,163 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:06:42,163 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:06:42,163 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:06:42,163 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:06:42,164 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.20trial/s, best loss: -0.9157848324514991]\n",
      "[INFO] 2021-04-13 21:06:48,494 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:06:48,494 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:06:48,495 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.88trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:06:53,762 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:06:53,762 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:06:53,763 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.97trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 21:06:58,899 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 21:06:58,899 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:06:58,900 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.21trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:07:05,155 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:07:05,155 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:07:05,156 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.89trial/s, best loss: -0.9047619047619048]\n",
      "[INFO] 2021-04-13 21:07:09,385 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:07:09,385 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:07:09,385 - Mean accuracy on test instances (iteration #1): 0.9619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-13 21:07:09,391 - Total elapsed time: 28.8s\n",
      "[INFO] 2021-04-13 21:07:09,391 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 4/5 highest_lsc_sampling\n",
      "Testando: 61_iris RF 4/5 highest_usefulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:07:12,384 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:07:12,384 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:07:12,386 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:07:12,386 - Building metadata.\n",
      "[INFO] 2021-04-13 21:07:13,906 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-13 21:07:13,908 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:07:13,908 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:07:13,908 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:07:13,908 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:07:13,909 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.75trial/s, best loss: -0.9157848324514991]\n",
      "[INFO] 2021-04-13 21:07:21,225 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:07:21,225 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:07:21,226 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.95trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:07:26,358 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:07:26,358 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:07:26,359 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.69trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 21:07:30,721 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 21:07:30,721 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:07:30,722 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.21trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 21:07:37,142 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:07:37,142 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:07:37,143 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.12trial/s, best loss: -0.9166666666666666]\n",
      "[INFO] 2021-04-13 21:07:43,602 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 21:07:43,602 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:07:43,602 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 21:07:43,610 - Total elapsed time: 31.2s\n",
      "[INFO] 2021-04-13 21:07:43,610 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 4/5 highest_usefulness_sampling\n",
      "Testando: 61_iris RF 4/5 lowest_harmfulness_sampling\n",
      "<class 'numpy.ndarray'>\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-13 21:07:47,407 - Configuration file: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-13 21:07:47,408 - Reading input dataset: '/Users/tiagocabral/Documents/UnB/EstudosEmIA/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-13 21:07:47,410 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-13 21:07:47,410 - Building metadata.\n",
      "[INFO] 2021-04-13 21:07:49,357 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-13 21:07:49,359 - Hyper parameter optimization enabled\n",
      "[INFO] 2021-04-13 21:07:49,359 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-13 21:07:49,359 - Estimating instance performance...\n",
      "[INFO] 2021-04-13 21:07:49,360 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-13 21:07:49,361 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:07<00:00,  2.59trial/s, best loss: -0.9157848324514991]\n",
      "[INFO] 2021-04-13 21:07:57,216 - Test fold mean accuracy: 0.9523809523809523\n",
      "[INFO] 2021-04-13 21:07:57,217 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-13 21:07:57,217 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:04<00:00,  4.20trial/s, best loss: -0.9391534391534391]\n",
      "[INFO] 2021-04-13 21:08:02,083 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:08:02,083 - Evaluating testing fold #3\n",
      "[INFO] 2021-04-13 21:08:02,084 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.88trial/s, best loss: -0.9638447971781305]\n",
      "[INFO] 2021-04-13 21:08:07,384 - Test fold mean accuracy: 0.8571428571428571\n",
      "[INFO] 2021-04-13 21:08:07,384 - Evaluating testing fold #4\n",
      "[INFO] 2021-04-13 21:08:07,384 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:06<00:00,  3.13trial/s, best loss: -0.9276895943562611]\n",
      "[INFO] 2021-04-13 21:08:13,870 - Test fold mean accuracy: 1.0\n",
      "[INFO] 2021-04-13 21:08:13,870 - Evaluating testing fold #5\n",
      "[INFO] 2021-04-13 21:08:13,871 - Optimizing classifier hyper-parameters\n",
      "100%|████████| 20/20 [00:05<00:00,  3.50trial/s, best loss: -0.9166666666666666]\n",
      "[INFO] 2021-04-13 21:08:19,624 - Test fold mean accuracy: 0.95\n",
      "[INFO] 2021-04-13 21:08:19,624 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-13 21:08:19,625 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-13 21:08:19,632 - Total elapsed time: 32.2s\n",
      "[INFO] 2021-04-13 21:08:19,632 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris RF 4/5 lowest_harmfulness_sampling\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tqdm_datasets = tqdm(datasets, desc=\" Dataset: \"+ str(ds[:-5]))\n",
    "#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in tqdm(classifiers,  desc =\"Classifier\"):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "        \n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in tqdm(range(n_splits),  desc =\"Bag\"):\n",
    "\n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_lsc_sampling\")\n",
    "            result = highest_lsc_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_lsc_sampling\")\n",
    "            \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_usefulness_sampling\")\n",
    "            result = highest_usefulness_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_usefulness_sampling\")\n",
    "           \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_harmfulness_sampling\")\n",
    "            result = lowest_harmfulness_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_harmfulness_sampling\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 46.410336223000286,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 1.0,\n",
       "  'time_elapsed': 43.47225970399995,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 39.61375497200015,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 36.04979556900025,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 40.71912481800018,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8969072164948454,\n",
       "  'time_elapsed': 41.20319744399967,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 35.85817156300027,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 1.0,\n",
       "  'time_elapsed': 30.77777824399982,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 29.739228350999838,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.845360824742268,\n",
       "  'time_elapsed': 29.89895658400019,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9072164948453608,\n",
       "  'time_elapsed': 31.921701797999958,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9278350515463918,\n",
       "  'time_elapsed': 30.75395939200007,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 30.22088341200015,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.845360824742268,\n",
       "  'time_elapsed': 31.03390942599981,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9896907216494846,\n",
       "  'time_elapsed': 30.443988546000583,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9278350515463918,\n",
       "  'time_elapsed': 29.637933414000145,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 1.0,\n",
       "  'time_elapsed': 31.364379308000025,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.979381443298969,\n",
       "  'time_elapsed': 28.613609151000674,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.979381443298969,\n",
       "  'time_elapsed': 29.84432696099975,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 31.64850817199931,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.865979381443299,\n",
       "  'time_elapsed': 30.306758783000078,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9175257731958762,\n",
       "  'time_elapsed': 31.11979010899995,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9072164948453608,\n",
       "  'time_elapsed': 31.75183160600045,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 1.0,\n",
       "  'time_elapsed': 31.28846474399961,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9072164948453608,\n",
       "  'time_elapsed': 30.771709380000175,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8865979381443299,\n",
       "  'time_elapsed': 30.900993131999712,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.979381443298969,\n",
       "  'time_elapsed': 30.632942664999973,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 39.67643311900065,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8865979381443299,\n",
       "  'time_elapsed': 37.43953559700003,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.8969072164948454,\n",
       "  'time_elapsed': 34.06668878900018,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9587628865979382,\n",
       "  'time_elapsed': 29.828584364000562,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 34.01663782800006,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.979381443298969,\n",
       "  'time_elapsed': 31.755980898000416,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest LSC Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9896907216494846,\n",
       "  'time_elapsed': 34.31822668299992,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Highest Usefulness Sampling',\n",
       "  'dataset': '61_iris'},\n",
       " {'performance_history': 0.9484536082474226,\n",
       "  'time_elapsed': 36.00320191300034,\n",
       "  'classifier': 'RF',\n",
       "  'sample_size': 0.06666666666666667,\n",
       "  'Strategy': 'Lowest Harmfulness Sampling',\n",
       "  'dataset': '61_iris'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_performance_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(total_performance_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('performance_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performance_history</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>classifier</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>43.472260</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.364379</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.288465</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.777778</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.989691</td>\n",
       "      <td>34.318227</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.989691</td>\n",
       "      <td>30.443989</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.979381</td>\n",
       "      <td>28.613609</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.979381</td>\n",
       "      <td>30.632943</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.979381</td>\n",
       "      <td>29.844327</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.979381</td>\n",
       "      <td>31.755981</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>31.648508</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>29.828584</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>39.676433</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>30.220883</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>29.739228</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>40.719125</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>36.049796</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.958763</td>\n",
       "      <td>39.613755</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>34.016638</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>46.410336</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>35.858172</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.948454</td>\n",
       "      <td>36.003202</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.927835</td>\n",
       "      <td>29.637933</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.927835</td>\n",
       "      <td>30.753959</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.917526</td>\n",
       "      <td>31.119790</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.907216</td>\n",
       "      <td>31.751832</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.907216</td>\n",
       "      <td>30.771709</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.907216</td>\n",
       "      <td>31.921702</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.896907</td>\n",
       "      <td>34.066689</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.896907</td>\n",
       "      <td>41.203197</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.886598</td>\n",
       "      <td>30.900993</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.886598</td>\n",
       "      <td>37.439536</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Lowest Harmfulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.865979</td>\n",
       "      <td>30.306759</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.845361</td>\n",
       "      <td>31.033909</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest Usefulness Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.845361</td>\n",
       "      <td>29.898957</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Highest LSC Sampling</td>\n",
       "      <td>61_iris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    performance_history  time_elapsed classifier  sample_size  \\\n",
       "1              1.000000     43.472260         RF     0.066667   \n",
       "16             1.000000     31.364379         RF     0.066667   \n",
       "23             1.000000     31.288465         RF     0.066667   \n",
       "7              1.000000     30.777778         RF     0.066667   \n",
       "33             0.989691     34.318227         RF     0.066667   \n",
       "14             0.989691     30.443989         RF     0.066667   \n",
       "17             0.979381     28.613609         RF     0.066667   \n",
       "26             0.979381     30.632943         RF     0.066667   \n",
       "18             0.979381     29.844327         RF     0.066667   \n",
       "32             0.979381     31.755981         RF     0.066667   \n",
       "19             0.958763     31.648508         RF     0.066667   \n",
       "30             0.958763     29.828584         RF     0.066667   \n",
       "27             0.958763     39.676433         RF     0.066667   \n",
       "12             0.958763     30.220883         RF     0.066667   \n",
       "8              0.958763     29.739228         RF     0.066667   \n",
       "4              0.958763     40.719125         RF     0.066667   \n",
       "3              0.958763     36.049796         RF     0.066667   \n",
       "2              0.958763     39.613755         RF     0.066667   \n",
       "31             0.948454     34.016638         RF     0.066667   \n",
       "0              0.948454     46.410336         RF     0.066667   \n",
       "6              0.948454     35.858172         RF     0.066667   \n",
       "34             0.948454     36.003202         RF     0.066667   \n",
       "15             0.927835     29.637933         RF     0.066667   \n",
       "11             0.927835     30.753959         RF     0.066667   \n",
       "21             0.917526     31.119790         RF     0.066667   \n",
       "22             0.907216     31.751832         RF     0.066667   \n",
       "24             0.907216     30.771709         RF     0.066667   \n",
       "10             0.907216     31.921702         RF     0.066667   \n",
       "29             0.896907     34.066689         RF     0.066667   \n",
       "5              0.896907     41.203197         RF     0.066667   \n",
       "25             0.886598     30.900993         RF     0.066667   \n",
       "28             0.886598     37.439536         RF     0.066667   \n",
       "20             0.865979     30.306759         RF     0.066667   \n",
       "13             0.845361     31.033909         RF     0.066667   \n",
       "9              0.845361     29.898957         RF     0.066667   \n",
       "\n",
       "                       Strategy  dataset  \n",
       "1          Highest LSC Sampling  61_iris  \n",
       "16         Highest LSC Sampling  61_iris  \n",
       "23         Highest LSC Sampling  61_iris  \n",
       "7          Highest LSC Sampling  61_iris  \n",
       "33  Highest Usefulness Sampling  61_iris  \n",
       "14         Highest LSC Sampling  61_iris  \n",
       "17  Highest Usefulness Sampling  61_iris  \n",
       "26         Highest LSC Sampling  61_iris  \n",
       "18         Highest LSC Sampling  61_iris  \n",
       "32         Highest LSC Sampling  61_iris  \n",
       "19  Highest Usefulness Sampling  61_iris  \n",
       "30  Highest Usefulness Sampling  61_iris  \n",
       "27  Highest Usefulness Sampling  61_iris  \n",
       "12         Highest LSC Sampling  61_iris  \n",
       "8          Highest LSC Sampling  61_iris  \n",
       "4          Highest LSC Sampling  61_iris  \n",
       "3          Highest LSC Sampling  61_iris  \n",
       "2          Highest LSC Sampling  61_iris  \n",
       "31  Lowest Harmfulness Sampling  61_iris  \n",
       "0          Highest LSC Sampling  61_iris  \n",
       "6          Highest LSC Sampling  61_iris  \n",
       "34  Lowest Harmfulness Sampling  61_iris  \n",
       "15  Highest Usefulness Sampling  61_iris  \n",
       "11  Highest Usefulness Sampling  61_iris  \n",
       "21  Highest Usefulness Sampling  61_iris  \n",
       "22  Lowest Harmfulness Sampling  61_iris  \n",
       "24  Highest Usefulness Sampling  61_iris  \n",
       "10         Highest LSC Sampling  61_iris  \n",
       "29         Highest LSC Sampling  61_iris  \n",
       "5          Highest LSC Sampling  61_iris  \n",
       "25  Lowest Harmfulness Sampling  61_iris  \n",
       "28  Lowest Harmfulness Sampling  61_iris  \n",
       "20         Highest LSC Sampling  61_iris  \n",
       "13  Highest Usefulness Sampling  61_iris  \n",
       "9          Highest LSC Sampling  61_iris  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Strategy != \"Query by Committee\"].sort_values('performance_history', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performance_history</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>classifier</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [performance_history, time_elapsed, classifier, sample_size, Strategy, dataset]\n",
       "Index: []"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Strategy == \"Expected Error Reduction\"].sort_values('time_elapsed', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35 entries, 0 to 34\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   performance_history  35 non-null     float64\n",
      " 1   time_elapsed         35 non-null     float64\n",
      " 2   classifier           35 non-null     object \n",
      " 3   sample_size          35 non-null     float64\n",
      " 4   Strategy             35 non-null     object \n",
      " 5   dataset              35 non-null     object \n",
      "dtypes: float64(3), object(3)\n",
      "memory usage: 1.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 546.875x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.relplot(\n",
    "    data= df,\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=3), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df[(df.Strategy != \"Uncertain Sampling\") & (df.Strategy != \"Query by Committee\")],\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=3), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df[(df.Strategy == \"Uncertain Sampling\") | (df.Strategy == \"Query by Committee\")],\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=2), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "p_bar = tqdm(datalist)\n",
    "for dataset_id in p_bar:\n",
    "    X_raw, y_raw, idx_data, dataset_name = which_oml_dataset(dataset_id)\n",
    "    p_bar.set_description(f'\"{dataset_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"1465_breast-tissue.arff\"\n",
    "\n",
    "X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "   \n",
    "from modAL.uncertainty import classifier_uncertainty\n",
    "\n",
    "print(len(np.unique(y_raw)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)), stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "print(y_train)\n",
    "\n",
    "learner = ActiveLearner (\n",
    "    estimator= which_classifier(classifier), #cls,\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    ")\n",
    "\n",
    "uncertain_sample_score = learner.score(X_test, y_test)\n",
    "\n",
    "total_of_samples = 1\n",
    "while (total_of_samples != cost):\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "        #print(\"IF\", learner.score(X_test, y_test))\n",
    "        learner.teach(X_train, y_train)\n",
    "        uncertain_sample_score = learner.score(X_test, y_test)\n",
    "        performance_history.append(uncertain_sample_score)\n",
    "    total_of_samples = total_of_samples + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size= len(np.unique(y_raw)) + init_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
