{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modAL + pyhard- Comparando estratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- modAL\n",
    "\n",
    "    - Amostra por incerteza\n",
    "    - Amostragem aleatória\n",
    "    - Consulta por comitê\n",
    "    - Aprendizado passivo\n",
    "    - Redução do erro esperado\n",
    "\n",
    "- Pyhard\n",
    "    - Harmfulness\n",
    "    - Usefulness\n",
    "    - H+U\n",
    "    - Local Set Cardinality \n",
    "    - N2\n",
    "    - F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i set_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i importing_libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i importing_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modal_framework(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy):\n",
    "\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    accuracy_history = []\n",
    "    f1_history = []\n",
    "    start = timer()    \n",
    "    \n",
    "    print(strategy[1])\n",
    "    if (strategy[1] == \"query_by_committee\"):\n",
    "        learner_list = []\n",
    "        for j in range(1, cost+1): # Loop para criação do comitê\n",
    "\n",
    "            X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "            sample_size = sample_size + len(X_train)\n",
    "            \n",
    "            # initializing learner\n",
    "            learner = ActiveLearner(\n",
    "                estimator= which_classifier(classifier),\n",
    "                X_training = X_train, y_training = y_train \n",
    "            )\n",
    "            learner_list.append(learner)\n",
    "            \n",
    "        # assembling the committee\n",
    "        committee = Committee(\n",
    "            learner_list=learner_list,\n",
    "            query_strategy=vote_entropy_sampling)\n",
    "            \n",
    "    else: \n",
    "        X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "\n",
    "        sample_size = sample_size + len(X_train)\n",
    "        committee = None\n",
    "\n",
    "        if (strategy[1] != \"random_sampling\"):\n",
    "            learner = ActiveLearner (\n",
    "                estimator= which_classifier(classifier), #cls,\n",
    "                query_strategy=uncertainty_sampling,\n",
    "                X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "            )\n",
    "        else:\n",
    "            learner = None\n",
    "     \n",
    "    if (strategy[1] != \"random_sampling\"):\n",
    "        accuracy_history.append(learner.score(X_pool, y_pool))\n",
    "        f1_history.append(compute_f1(learner, X_pool, y_pool, \"weighted\"))\n",
    "    \n",
    "    for i in range(cost):\n",
    "        sample_size, accuracy_history, f1_history = eval(strategy[1] + \"(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy, sample_size, accuracy_history, f1_history, X_train, X_pool, y_train, y_pool, learner, committee)\")\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return {\"accuracy_history\": accuracy_history,\n",
    "         \"f1_history\": f1_history,\n",
    "         \"package\": \"modAL\",\n",
    "         \"time_elapsed\": time_elapsed,\n",
    "         \"classifier\": classifier,\n",
    "         \"sample_size\": sample_size / len(X_raw),\n",
    "         \"strategy\": strategy[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostra por incerteza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertain_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy, sample_size, accuracy_history, f1_history, X_train, X_test, y_train, y_test, learner, committee = None):\n",
    "      \n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "        #print(\"IF\", learner.score(X_test, y_test))\n",
    "        sample_size = sample_size + len(X_train)\n",
    "        learner.teach(X_train, y_train)\n",
    "    accuracy_history.append(learner.score(X_test, y_test))\n",
    "    f1_history.append(compute_f1(learner, X_test, y_test, \"weighted\"))\n",
    "        \n",
    "    return sample_size, accuracy_history, f1_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostragem aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy, sample_size, accuracy_history, f1_history, X_train, X_test, y_train, y_test, learner = None, committee = None):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    cls = which_classifier(classifier)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    accuracy_history.append(cls.score(X_test,y_test))\n",
    "    f1_history.append(compute_f1(cls, X_test, y_test, \"weighted\"))\n",
    "        \n",
    "    return sample_size, accuracy_history, f1_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta por comitê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_by_committee(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy, sample_size, accuracy_history, f1_history, X_train, X_pool, y_train, y_pool, learner, committee):    \n",
    "\n",
    "    # print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "    query_idx, query_instance = committee.query(X_pool, n_instances = init_size)\n",
    "    sample_size = sample_size + len(query_idx)\n",
    "\n",
    "    committee.teach(\n",
    "        X = X_pool[query_idx],\n",
    "        y = y_pool[query_idx]\n",
    "    )\n",
    "\n",
    "    X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "    y_pool = np.delete(y_pool, query_idx)\n",
    "\n",
    "    accuracy_history.append(committee.score(X_pool, y_pool))\n",
    "    f1_history.append(compute_f1(committee, X_pool, y_pool, \"weighted\"))\n",
    "    \n",
    "    return sample_size, accuracy_history, f1_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Error Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_error_reduction(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy, sample_size, accuracy_history, f1_history, X_train, X_pool, y_train, y_pool, learner, committee = None):\n",
    "\n",
    "    # print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "    exp_error_idx = expected_error_reduction(learner, X_pool, 'binary', n_instances=init_size)\n",
    "\n",
    "    learner.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "    sample_size = sample_size + init_size\n",
    "\n",
    "    # X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "    # y_pool = np.delete(y_pool, exp_error_idx)\n",
    "\n",
    "    accuracy_history.append(learner.score(X_pool, y_pool))\n",
    "    f1_history.append(compute_f1(learner, X_pool, y_pool, \"weighted\"))\n",
    "    \n",
    "    return sample_size, accuracy_history, f1_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Model Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_model_change(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy, sample_size, accuracy_history, f1_history, X_train, X_pool, y_train, y_pool, learner, committee = None):\n",
    "\n",
    "    # print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "    exp_error_idx = np.random.choice(range(len(X_pool)), size=init_size, replace=False)\n",
    "    aux = deepcopy(learner)\n",
    "\n",
    "    aux.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "    score_aux = aux.score(X_pool, y_pool)\n",
    "    score_learner = learner.score(X_pool, y_pool)\n",
    "\n",
    "    if score_aux > score_learner:\n",
    "        learner = deepcopy(aux)\n",
    "        sample_size = sample_size + init_size\n",
    "\n",
    "    X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "    y_pool = np.delete(y_pool, exp_error_idx, axis=0)\n",
    "\n",
    "    accuracy_history.append(learner.score(X_pool, y_pool))\n",
    "    f1_history.append(compute_f1(learner, X_pool, y_pool, \"weighted\"))\n",
    "\n",
    "    return sample_size, accuracy_history, f1_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i setup_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyhard Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyhard_strategies(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    accuracy_history = []\n",
    "    f1_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    strategy = config(strategy)\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "\n",
    "    accuracy_history.append(learner.score(X_test, y_test))\n",
    "    f1_history.append(compute_f1(learner, X_test, y_test, \"weighted\"))\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure(strategy['measure'])\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by=strategy['sortby'], ascending=strategy['ascending'])['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    \n",
    "    accuracy_history.append(learner.score(X_test, y_test))\n",
    "    f1_history.append(compute_f1(learner, X_test, y_test, \"weighted\"))\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return { \"accuracy_history\": accuracy_history,\n",
    "         \"f1_history\": f1_history,\n",
    "         \"package\": \"Pyhard\",\n",
    "         \"time_elapsed\": time_elapsed,\n",
    "         \"classifier\": classifier,\n",
    "         \"sample_size\": sample_size / len(X_raw),\n",
    "         \"strategy\": strategy['name']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('./datasets/luis')\n",
    "classifiers = ['5NN', 'C4.5', 'NB','RF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata = []\n",
    "\n",
    "for ds in datasets:\n",
    "    metadata.append(fetch_datasets(ds))\n",
    "\n",
    "metadata = pd.DataFrame.from_dict(metadata)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhard_strategies_names = ['H','U','H+U','LSC','N2','F3']\n",
    "\n",
    "for ds in datasets:\n",
    "    for classifier in classifiers:\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in range(n_splits):\n",
    "            for ph_strategy in pyhard_strategies_names:\n",
    "                tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" \" + ph_strategy)\n",
    "                result = pyhard_strategies(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, init_size, cost, ph_strategy)\n",
    "                result['dataset'] = ds[:-5]\n",
    "                total_performance_history.append(result)\n",
    "                tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" \" + ph_strategy)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modal_strategies = {\"Uncertain Sampling\":\"uncertain_sampling\", \"Random Sampling\":\"random_sampling\", \"Query by Committee\":\"query_by_committee\", \"Expected Error Reduction\":\"exp_error_reduction\", \"Expected Model Change\":\"exp_model_change\"}\n",
    "parameters = \"(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, init_size, cost)\"\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in classifiers:\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds) # Seta aqui mesmo ou no loop do dataset?\n",
    "        for idx_bag in range(n_splits):\n",
    "            for key, value in modal_strategies.items():\n",
    "                tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag+1) + \"/\" + str(n_splits) + \" \" + key)\n",
    "                result = modal_framework(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, [key, value])\n",
    "                result['dataset'] = ds[:-5]\n",
    "                total_performance_history.append(result)\n",
    "                tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag+1) + \"/\" + str(n_splits) + \" \" + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_performance_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame.from_dict(total_performance_history)\n",
    "output = output.set_index(['package','time_elapsed','classifier','sample_size','Strategy','dataset']).apply(pd.Series.explode).reset_index()\n",
    "output.to_csv('total_performance_history.output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(total_performance_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Strategies on Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.unique(df['Strategy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('accuracy_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_mean = df2.groupby(['Strategy', 'classifier']).mean()\n",
    "performance_std = df2.groupby(['Strategy', 'classifier']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last loop results (last cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_result = df[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_result['f1_history'] = df_last_result['f1_history'].apply(lambda x: x[-1])\n",
    "df_last_result['auc_history'] = df_last_result['auc_history'].apply(lambda x: x[-1])\n",
    "df_last_result['accuracy_history'] = df_last_result['accuracy_history'].apply(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_last_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_result_mean = df_last_result.groupby(['Strategy', 'classifier']).mean()\n",
    "df_last_result_std = df_last_result.groupby(['Strategy', 'classifier']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_result_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing most time expensive strategies to improve visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Strategy != \"Query by Committee\"].sort_values('performance_history', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Strategy == \"Expected Error Reduction\"].sort_values('time_elapsed', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df,\n",
    "    x=\"accuracy_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=10), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df_last_result_mean,\n",
    "    x=\"accuracy_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\", style=\"classifier\", height=7,\n",
    "    palette=sns.color_palette(n_colors=len(pd.unique(df_last_result['Strategy']))), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=df_last_result, x=\"accuracy_history\", y=\"Strategy\", orient=\"h\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=df_last_result, x=\"f1_history\", y=\"Strategy\", orient=\"h\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeatMap = df_last_result.filter(['sample_size','f1_history', 'accuracy_history'], axis=1)\n",
    "ax = sns.heatmap(dfHeatMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby por dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(total_performance_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode pelas colunas accuracy_history e f1_history\n",
    "df = df.set_index(['package','time_elapsed','classifier','sample_size','Strategy','dataset']).apply(pd.Series.explode).reset_index()\n",
    "\n",
    "# explode retorna coluna como object, logo é necessário transformar em número\n",
    "df['f1_history'] = pd.to_numeric(df['f1_history'])\n",
    "df['accuracy_history'] = pd.to_numeric(df['accuracy_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['dataset','package','Strategy', 'classifier']).mean().sort_values(\"f1_history\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['dataset','package','Strategy', 'classifier']).std().sort_values(\"f1_history\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
