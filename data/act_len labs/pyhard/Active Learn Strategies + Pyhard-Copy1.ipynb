{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modAL + pyhard- Comparando estratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- modAL\n",
    "\n",
    "    - Amostra por incerteza\n",
    "    - Amostragem aleatória\n",
    "    - Consulta por comitê\n",
    "    - Aprendizado passivo\n",
    "    - Redução do erro esperado\n",
    "\n",
    "- Pyhard\n",
    "    - H\n",
    "    - U\n",
    "    - H+U\n",
    "    - LSC\n",
    "    - N2\n",
    "    - F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i set_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i importing_libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i importing_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostra por incerteza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertain_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    accuracy_history = []\n",
    "    f1_history = []\n",
    "    auc_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    accuracy_history.append(learner.score(X_test, y_test))\n",
    "    f1_history.append(compute_f1(learner, X_test, y_test, \"weighted\"))\n",
    "    auc_history.append(compute_auc(learner, X_test, y_test, \"weighted\",\"ovo\"))\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "        \n",
    "        idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "        X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "        \n",
    "        if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "            #print(\"IF\", learner.score(X_test, y_test))\n",
    "            sample_size = sample_size + len(X_train)\n",
    "            learner.teach(X_train, y_train)\n",
    "        accuracy_history.append(learner.score(X_test, y_test))\n",
    "        f1_history.append(compute_f1(learner, X_test, y_test, \"weighted\"))\n",
    "        auc_history.append(compute_auc(learner, X_test, y_test, \"weighted\", \"ovo\"))\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"accuracy_history\": accuracy_history,\n",
    "             \"f1_history\": f1_history,\n",
    "             \"auc_history\": auc_history,\n",
    "             \"package\": \"modAL\",\n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Uncertain Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostragem aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "        \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    accuracy_history = []\n",
    "    f1_history = []\n",
    "    auc_history = []\n",
    "    start = timer()\n",
    "\n",
    "    for i in range(1, cost+1):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "        \n",
    "        cls = which_classifier(classifier)\n",
    "        cls.fit(X_train, y_train)\n",
    "\n",
    "        accuracy_history.append(cls.score(X_test,y_test))\n",
    "        f1_history.append(compute_f1(cls, X_test, y_test, \"weighted\"))\n",
    "        auc_history.append(compute_auc(cls, X_test, y_test, \"weighted\", \"ovo\"))\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return { \"accuracy_history\": accuracy_history,\n",
    "         \"f1_history\": f1_history,\n",
    "         \"auc_history\": auc_history,\n",
    "         \"package\": \"modAL\",\n",
    "         \"time_elapsed\": time_elapsed,\n",
    "         \"classifier\": classifier,\n",
    "         \"sample_size\": sample_size / len(X_raw),\n",
    "         \"Strategy\": \"Random Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta por comitê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_by_committee(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.models import ActiveLearner, Committee\n",
    "    from modAL.disagreement import vote_entropy_sampling\n",
    "\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    accuracy_history = []\n",
    "    f1_history = []\n",
    "    auc_history = []\n",
    "    start = timer()\n",
    "\n",
    "    learner_list = []\n",
    "\n",
    "    for j in range(1, cost+1): # Loop para criação do comitê\n",
    "\n",
    "        X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "\n",
    "        # initializing learner\n",
    "        learner = ActiveLearner(\n",
    "            estimator= which_classifier(classifier),\n",
    "            X_training = X_train, y_training = y_train \n",
    "        )\n",
    "        learner_list.append(learner)\n",
    "\n",
    "    # assembling the committee\n",
    "    committee = Committee(\n",
    "        learner_list=learner_list,\n",
    "        query_strategy=vote_entropy_sampling)\n",
    "    \n",
    "    # COLOCAR OU NÃO O PRIMEIRO SCORE?\n",
    "    # accuracy_history.append(committee.score(X_pool, y_pool))\n",
    "    # f1_history.append(compute_f1(committee, X_pool, y_pool, \"weighted\"))\n",
    "    # auc_history.append(compute_auc(committee, X_pool, y_pool, \"weighted\", \"ovo\"))\n",
    "    \n",
    "    \n",
    "    # query by committee\n",
    "    for idx in range(cost):\n",
    "        # print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        query_idx, query_instance = committee.query(X_pool, n_instances = init_size+1)\n",
    "        sample_size = sample_size + len(query_idx)\n",
    "        \n",
    "        committee.teach(\n",
    "            X = X_pool[query_idx],\n",
    "            y = y_pool[query_idx]\n",
    "        )\n",
    "\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx)\n",
    "\n",
    "        accuracy_history.append(committee.score(X_pool, y_pool))\n",
    "        f1_history.append(compute_f1(committee, X_pool, y_pool, \"weighted\"))\n",
    "        auc_history.append(compute_auc(committee, X_pool, y_pool, \"weighted\", \"ovo\"))\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return { \"accuracy_history\": accuracy_history,\n",
    "         \"f1_history\": f1_history,\n",
    "         \"auc_history\": auc_history,\n",
    "         \"package\": \"modAL\",\n",
    "         \"time_elapsed\": time_elapsed,\n",
    "         \"classifier\": classifier,\n",
    "         \"sample_size\": sample_size / len(X_raw),\n",
    "         \"Strategy\": \"Query by Committee\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Error Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_error_reduction(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    accuracy_history = []\n",
    "    f1_history = []\n",
    "    auc_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][TRAIN][initial_idx]]\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    accuracy_history.append(learner.score(X_pool, y_pool))\n",
    "    f1_history.append(compute_f1(learner, X_pool, y_pool, \"weighted\"))\n",
    "    auc_history.append(compute_auc(learner, X_pool, y_pool, \"weighted\", \"ovo\"))\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        # print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = expected_error_reduction(learner, X_pool, 'binary', n_instances=init_size)\n",
    "\n",
    "        learner.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        sample_size = sample_size + init_size\n",
    "    \n",
    "        # X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        # y_pool = np.delete(y_pool, exp_error_idx)\n",
    "        \n",
    "        accuracy_history.append(learner.score(X_pool, y_pool))\n",
    "        f1_history.append(compute_f1(learner, X_pool, y_pool, \"weighted\"))\n",
    "        auc_history.append(compute_auc(learner, X_pool, y_pool, \"weighted\", \"ovo\"))\n",
    "        \n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "\n",
    "    return { \"accuracy_history\": accuracy_history,\n",
    "         \"f1_history\": f1_history,\n",
    "         \"auc_history\": auc_history,\n",
    "         \"package\": \"modAL\",\n",
    "         \"time_elapsed\": time_elapsed,\n",
    "         \"classifier\": classifier,\n",
    "         \"sample_size\": sample_size / len(X_raw),\n",
    "         \"Strategy\": \"Expected Error Reduction\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Model Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_model_change(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    accuracy_history = []\n",
    "    f1_history = []\n",
    "    auc_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    \n",
    "    accuracy_history.append(learner.score(X_pool, y_pool))\n",
    "    f1_history.append(compute_f1(learner, X_pool, y_pool, \"weighted\"))\n",
    "    auc_history.append(compute_auc(learner, X_pool, y_pool, \"weighted\", \"ovo\"))\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        # print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = np.random.choice(range(len(X_pool)), size=init_size, replace=False)\n",
    "        aux = deepcopy(learner)\n",
    "\n",
    "        aux.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        score_aux = aux.score(X_pool, y_pool)\n",
    "        score_learner = learner.score(X_pool, y_pool)\n",
    "\n",
    "        if score_aux > score_learner:\n",
    "            learner = deepcopy(aux)\n",
    "            sample_size = sample_size + init_size\n",
    "        \n",
    "        X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, exp_error_idx, axis=0)\n",
    "        \n",
    "        accuracy_history.append(learner.score(X_pool, y_pool))\n",
    "        f1_history.append(compute_f1(learner, X_pool, y_pool, \"weighted\"))\n",
    "        auc_history.append(compute_auc(learner, X_pool, y_pool, \"weighted\", \"ovo\"))\n",
    "\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return { \"accuracy_history\": accuracy_history,\n",
    "         \"f1_history\": f1_history,\n",
    "         \"auc_history\": auc_history,\n",
    "         \"package\": \"modAL\",\n",
    "         \"time_elapsed\": time_elapsed,\n",
    "         \"classifier\": classifier,\n",
    "         \"sample_size\": sample_size / len(X_raw),\n",
    "         \"Strategy\": \"Expected Model Change\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyhard Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config(section, filename='strategies.config'):\n",
    "    from configparser import ConfigParser\n",
    "\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(\"../\" + filename)\n",
    "    # get section, default to postgresql\n",
    "    strategy = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            strategy[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    "\n",
    "    # transformando texto em bool\n",
    "    strategy['ascending'] = list(map(lambda x: bool(0 if x == \"False\" else 1), strategy['ascending'].split(',')))\n",
    "    strategy['sortby'] = strategy['sortby'].split(',')\n",
    "    \n",
    "    print(strategy)\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyhard_strategies(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    accuracy_history = []\n",
    "    f1_history = []\n",
    "    auc_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    strategy = config(strategy)\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "\n",
    "    accuracy_history.append(learner.score(X_test, y_test))\n",
    "    f1_history.append(compute_f1(learner, X_test, y_test, \"weighted\"))\n",
    "    auc_history.append(compute_auc(learner, X_test, y_test, \"weighted\", \"ovo\"))\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure(strategy['measure'])\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by=strategy['sortby'], ascending=strategy['ascending'])['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    \n",
    "    accuracy_history.append(learner.score(X_test, y_test))\n",
    "    f1_history.append(compute_f1(learner, X_test, y_test, \"weighted\"))\n",
    "    auc_history.append(compute_auc(learner, X_test, y_test, \"weighted\", \"ovo\"))\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return { \"accuracy_history\": accuracy_history,\n",
    "         \"f1_history\": f1_history,\n",
    "         \"auc_history\": auc_history,\n",
    "         \"package\": \"Pyhard\",\n",
    "         \"time_elapsed\": time_elapsed,\n",
    "         \"classifier\": classifier,\n",
    "         \"sample_size\": sample_size / len(X_raw),\n",
    "         \"Strategy\": strategy['name']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(learner, X, y_true, average = None):\n",
    "    y_pred = learner.predict(X)\n",
    "    return metrics.f1_score(y_true, y_pred, average = average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(learner, X, y_true, average = None, multi_class = \"ovo\"):\n",
    "    y_pred = learner.predict_proba(X)\n",
    "    return metrics.roc_auc_score(y_true, y_pred, average = average, multi_class = multi_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_pyhard_measure(measure='LSC'):\n",
    "    import yaml\n",
    "    with open(r'config-template.yaml') as file:\n",
    "        configs_list = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "        if measure == 'LSC':\n",
    "            configs_list['measures_list'] = ['LSC']\n",
    "        elif measure == 'Harmfulness':\n",
    "            configs_list['measures_list'] = ['Harmfulness']\n",
    "        elif measure == 'Usefulness':\n",
    "            configs_list['measures_list'] = ['Usefulness']\n",
    "        elif measure == 'U+H':\n",
    "            configs_list['measures_list'] = ['Harmfulness','Usefulness']\n",
    "        elif measure == 'N2':\n",
    "            configs_list['measures_list'] = ['N2']\n",
    "        elif measure == 'F3':\n",
    "            configs_list['measures_list'] = ['F3']\n",
    "\n",
    "    with open(r'config.yaml', 'w') as file:\n",
    "        yaml.dump(configs_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_dataset(dataset = \"iris\", n_splits = 5):\n",
    "    \n",
    "    # Futuramente essa etapa será ajustada para receber qualquer dataset (ou lista com datasets)\n",
    "    if (dataset == \"iris\"):\n",
    "        data = load_iris()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "    \n",
    "    if (dataset == \"wine\"):\n",
    "        data = load_wine()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    if (dataset == \"digits\"):\n",
    "        data = load_digits()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_oml_dataset(dataset_id, n_splits = 5):\n",
    "    data = openml.datasets.get_dataset(dataset_id)\n",
    "    \n",
    "    X_raw, y_raw, categorical_indicator, attribute_names = data.get_data(\n",
    "    dataset_format=\"array\", target=data.default_target_attribute)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_raw)\n",
    "    y_raw = le.transform(y_raw)\n",
    "    \n",
    "    X_raw = np.nan_to_num(X_raw)\n",
    "    \n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_arff_dataset(dataset, n_splits = 5):\n",
    "   \n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    \n",
    "    data = arff.loadarff('datasets/luis/' + dataset)\n",
    "    data = pd.DataFrame(data[0])\n",
    "\n",
    "    X_raw = data[data.columns[:-1]].to_numpy()\n",
    "    y_raw = data[data.columns[-1]].to_numpy()\n",
    "    \n",
    "    lex = preprocessing.OrdinalEncoder()\n",
    "    lex.fit(X_raw)\n",
    "    X_raw = lex.transform(X_raw)\n",
    "        \n",
    "    ley = preprocessing.LabelEncoder()\n",
    "    ley.fit(y_raw)\n",
    "    y_raw = ley.transform(y_raw)\n",
    "    \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    data_cv.get_n_splits(X_raw,y_raw)\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw, y_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_classifier(classifier = '5NN'):\n",
    "    \n",
    "    if (classifier == '5NN'):\n",
    "        return KNeighborsClassifier(5)\n",
    "    elif (classifier == 'C4.5'):\n",
    "        return tree.DecisionTreeClassifier()\n",
    "    elif (classifier == 'NB'):\n",
    "        return GaussianNB()\n",
    "    elif (classifier == 'SVM'):\n",
    "        return SVC(probability=True, gamma='auto')\n",
    "    elif (classifier == 'RF'):\n",
    "        return RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datasets(dataset):\n",
    "    \n",
    "    data = arff.loadarff('./datasets/luis/' + dataset)\n",
    "    metadata = data[1]\n",
    "    data = pd.DataFrame(data[0])\n",
    "    \n",
    "    instances = len(data)\n",
    "    classes = len(data.iloc[:,-1].value_counts())\n",
    "    attributes = len(data.columns)- 1\n",
    "    nominal_attributes = str(metadata).count(\"nominal\")\n",
    "    \n",
    "    proportion = data.iloc[:,-1].value_counts()\n",
    "    proportion = proportion.map(lambda x: round(x/instances*100,2))\n",
    "\n",
    "    majority = max(proportion)\n",
    "    minority = min(proportion)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"name\": dataset[:-5],\n",
    "        \"instances\": instances,\n",
    "        \"classes\": classes,\n",
    "        \"attributes\": attributes,\n",
    "        \"nominal attributes\": nominal_attributes,\n",
    "        \"majority\": majority,\n",
    "        \"minority\": minority\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('./datasets/luis')\n",
    "classifiers = ['5NN', 'C4.5', 'NB','RF']\n",
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata = []\n",
    "\n",
    "for ds in datasets:\n",
    "    metadata.append(fetch_datasets(ds))\n",
    "\n",
    "metadata = pd.DataFrame.from_dict(metadata)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhard_strategies_names = ['H','U','H+U','LSC','N2','F3']\n",
    "\n",
    "for ds in datasets:\n",
    "    for classifier in classifiers:\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in range(n_splits):\n",
    "            for ph_strategy in pyhard_strategies_names:\n",
    "                tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" \" + ph_strategy)\n",
    "                result = pyhard_strategies(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost, ph_strategy)\n",
    "                result['dataset'] = ds[:-5]\n",
    "                total_performance_history.append(result)\n",
    "                tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" \" + ph_strategy)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "functions = [\"uncertain_sampling\", \"random_sampling\", \"query_by_committee\", \"exp_error_reduction\", \"exp_model_change\"]\n",
    "parameters = \"(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\"\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in classifiers:\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in range(n_splits):\n",
    "            for func in functions:\n",
    "                tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag+1) + \"/\" + str(n_splits) + \" \" + func)\n",
    "                result = eval(func+parameters)\n",
    "                result['dataset'] = ds[:-5]\n",
    "                total_performance_history.append(result)\n",
    "                tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag+1) + \"/\" + str(n_splits) + \" \" + func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_performance_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(total_performance_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Strategies on Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df['Strategy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('accuracy_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df\n",
    "df2.groupby(['Strategy', 'classifier']).agg({'performance_history':['mean','std'],'time_elapsed':['mean','std'], 'sample_size':['mean','std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_mean = df2.groupby(['Strategy', 'classifier']).mean()\n",
    "performance_std = df2.groupby(['Strategy', 'classifier']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last loop results (last cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_result = df[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_result['f1_history'] = df_last_result['f1_history'].apply(lambda x: x[-1])\n",
    "df_last_result['auc_history'] = df_last_result['auc_history'].apply(lambda x: x[-1])\n",
    "df_last_result['accuracy_history'] = df_last_result['accuracy_history'].apply(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_last_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_result_mean = df_last_result.groupby(['Strategy', 'classifier']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing most time expensive strategies to improve visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Strategy != \"Query by Committee\"].sort_values('performance_history', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Strategy == \"Expected Error Reduction\"].sort_values('time_elapsed', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df,\n",
    "    x=\"accuracy_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=10), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df_last_result,\n",
    "    x=\"auc_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\", style=\"classifier\",\n",
    "    palette=sns.color_palette(n_colors=len(pd.unique(df_last_result['Strategy']))), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
