{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modAL + pyhard- Comparando estratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amostra por incerteza\n",
    "- Amostragem aleatória\n",
    "- Consulta por comitê\n",
    "- Aprendizado passivo\n",
    "- Redução do erro esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i set_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i importing_libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i importing_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatratégias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostra por incerteza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertain_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][TRAIN][initial_idx]]\n",
    "    #X_test, y_test = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    #cls = which_classifier(classifier)\n",
    "    #cls.fit(X_train,y_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "        \n",
    "        idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "        X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "        \n",
    "        if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "            #print(\"IF\", learner.score(X_test, y_test))\n",
    "            sample_size = sample_size + len(X_train)\n",
    "            learner.teach(X_train, y_train)\n",
    "            uncertain_sample_score = learner.score(X_test, y_test)\n",
    "            performance_history.append(uncertain_sample_score)\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Uncertain Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostragem aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "        \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "\n",
    "    for i in range(1, cost+1):\n",
    "\n",
    "        #high = X_raw.shape[0] = qtd amostras no dataset\n",
    "        #training_indices = np.random.randint(low=0, high=len(X_raw[idx_data[idx_bag][TRAIN]]), size=k+i) #high = qtd elementos na bag\n",
    "        #sample_size = sample_size + len(training_indices)\n",
    "        #X_train = X_raw[idx_data[idx_bag][TRAIN][training_indices]] #ASK06\n",
    "        #y_train = y_raw[idx_data[idx_bag][TRAIN][training_indices]]\n",
    "        #X_test = np.delete(X_raw, idx_data[idx_bag][TRAIN][training_indices], axis=0)\n",
    "        #y_test = np.delete(y_raw, idx_data[idx_bag][TRAIN][training_indices], axis=0)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "        \n",
    "        cls = which_classifier(classifier)\n",
    "        cls.fit(X_train, y_train)\n",
    "\n",
    "        random_sampling_score = cls.score(X_test,y_test)\n",
    "        performance_history.append(random_sampling_score)\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw),\n",
    "             \"Strategy\": \"Random Sampling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta por comitê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_by_committee(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.models import ActiveLearner, Committee\n",
    "    from modAL.disagreement import vote_entropy_sampling\n",
    "\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "\n",
    "    learner_list = []\n",
    "\n",
    "    for j in range(1, cost+1): # Loop para criação do comitê\n",
    "\n",
    "        X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "        sample_size = sample_size + len(X_train)\n",
    "\n",
    "        # initializing learner\n",
    "        learner = ActiveLearner(\n",
    "            estimator= which_classifier(classifier),\n",
    "            X_training = X_train, y_training = y_train \n",
    "        )\n",
    "        learner_list.append(learner)\n",
    "\n",
    "    # assembling the committee\n",
    "    committee = Committee(\n",
    "        learner_list=learner_list,\n",
    "        query_strategy=vote_entropy_sampling)\n",
    "\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]]\n",
    "    \n",
    "    # query by committee\n",
    "    for idx in range(cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        query_idx, query_instance = committee.query(X_pool, n_instances = init_size+1)\n",
    "        sample_size = sample_size + len(query_idx)\n",
    "        \n",
    "        committee.teach(\n",
    "            X = X_pool[query_idx],\n",
    "            y = y_pool[query_idx]\n",
    "        )\n",
    "\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx)\n",
    "        \n",
    "        query_by_committee_score = committee.score(X_pool, y_pool)\n",
    "        performance_history.append(query_by_committee_score)\n",
    "\n",
    "        \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "\n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw),\n",
    "             \"Strategy\": \"Query by Committee\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Error Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_error_reduction(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][TRAIN][initial_idx]]\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    exp_er_score = learner.score(X_pool, y_pool)\n",
    "    performance_history.append(exp_er_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = expected_error_reduction(learner, X_pool, 'binary', n_instances=init_size)\n",
    "\n",
    "        learner.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        sample_size = sample_size + init_size\n",
    "    \n",
    "        X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, exp_error_idx)\n",
    "        \n",
    "        exp_er_score = learner.score(X_pool, y_pool)\n",
    "        performance_history.append(exp_er_score)\n",
    "        \n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Expected Error Reduction\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Model Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_model_change(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost):\n",
    "\n",
    "    from modAL.expected_error import expected_error_reduction\n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    #initial_idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    #X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][initial_idx]], y_raw[idx_data[idx_bag][0][initial_idx]]\n",
    "    #X_pool, y_pool = X_raw[idx_data[idx_bag][TEST]], y_raw[idx_data[idx_bag][TEST]]\n",
    "    \n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator = which_classifier(classifier),\n",
    "        X_training = X_train, y_training = y_train\n",
    "    )\n",
    "    \n",
    "#     performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "    while (total_of_samples != cost):\n",
    "        print(\"\\t Size of X_pool:\", len(X_pool))\n",
    "        exp_error_idx = np.random.choice(range(len(X_pool)), size=init_size, replace=False)[0]\n",
    "        aux = deepcopy(learner)\n",
    "\n",
    "        aux.teach(X_pool[exp_error_idx], y_pool[exp_error_idx])\n",
    "        score_aux = aux.score(X_pool, y_pool)\n",
    "        score_learner = learner.score(X_pool, y_pool)\n",
    "\n",
    "        if score_aux > score_learner:\n",
    "            learner = deepcopy(aux)\n",
    "            sample_size = sample_size + init_size\n",
    "        \n",
    "        X_pool = np.delete(X_pool, exp_error_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, exp_error_idx, axis=0)\n",
    "        \n",
    "        exp_mo_score = learner.score(X_pool, y_pool)\n",
    "        performance_history.append(exp_mo_score)\n",
    "\n",
    "        total_of_samples = total_of_samples + 1\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": \"Expected Model Change\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyhard Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config(section, filename='strategies.config'):\n",
    "    from configparser import ConfigParser\n",
    "\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(\"../\" + filename)\n",
    "    # get section, default to postgresql\n",
    "    strategy = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            strategy[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    "\n",
    "    # transformando texto em bool\n",
    "    strategy['ascending'] = list(map(lambda x: bool(0 if x == \"False\" else 1), strategy['ascending'].split(',')))\n",
    "    strategy['sortby'] = strategy['sortby'].split(',')\n",
    "    \n",
    "    print(strategy)\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyhard_strategies(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy):\n",
    "    \n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    \n",
    "    sample_size = 0 #contador de amostras utilizadas pela estratégia\n",
    "    performance_history = []\n",
    "    start = timer()\n",
    "    \n",
    "    strategy = config(strategy)\n",
    "    \n",
    "    # parte randomica inicial da estratégia\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)) + init_size, stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "    \n",
    "    sample_size = sample_size + len(X_train)\n",
    "\n",
    "    learner = ActiveLearner (\n",
    "        estimator= which_classifier(classifier), #cls,\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    "    )\n",
    "    \n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "\n",
    "    total_of_samples = 1\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    X_rawAndY_raw = np.column_stack([X_raw[idx_data[idx_bag][TRAIN]],y_raw[idx_data[idx_bag][TRAIN]]])\n",
    "    np.savetxt(\"data.csv\", X_rawAndY_raw, fmt='%i', delimiter=\",\")\n",
    "    \n",
    "    which_pyhard_measure(strategy['measure'])\n",
    "\n",
    "    !pyhard --no-isa\n",
    "\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    idx = list(df.sort_values(by=strategy['sortby'], ascending=strategy['ascending'])['instances'][:cost])\n",
    "\n",
    "    X_train = X_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "    y_train = y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    sample_size = cost\n",
    "    learner.teach(X_train, y_train)\n",
    "    uncertain_sample_score = learner.score(X_test, y_test)\n",
    "    performance_history.append(uncertain_sample_score)\n",
    "    \n",
    "    end = timer()\n",
    "    time_elapsed = end - start\n",
    "    \n",
    "    return { \"performance_history\": performance_history[-1], \n",
    "             \"time_elapsed\": time_elapsed,\n",
    "             \"classifier\": classifier,\n",
    "             \"sample_size\": sample_size / len(X_raw), # RETORNAR TODAS AS AMOSTRAS DE CADA PERFORMANCE OU SÓ DO ULTIMO\n",
    "             \"Strategy\": strategy['name']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando: 61_iris 5NN 0/5 H\n",
      "{'name': 'Lowest Harmfulness Sampling', 'measure': 'Harmfulness', 'sortby': ['feature_Harmfulness'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:02:14,889 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:02:14,897 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:02:14,907 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:02:14,907 - Building metadata.\n",
      "[INFO] 2021-04-26 21:02:19,740 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:02:19,743 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:02:19,743 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:02:19,745 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:02:20,432 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:02:20,433 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:02:21,130 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:02:21,130 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:02:21,131 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-26 21:02:21,182 - Total elapsed time: 6.3s\n",
      "[INFO] 2021-04-26 21:02:21,183 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 0/5 H\n",
      "Testando: 61_iris 5NN 0/5 U\n",
      "{'name': 'Highest Usefulness Sampling', 'measure': 'Usefulness', 'sortby': ['feature_Usefulness'], 'ascending': [False]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:03:11,420 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:03:11,436 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:03:11,466 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:03:11,466 - Building metadata.\n",
      "[INFO] 2021-04-26 21:03:21,828 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:03:21,831 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:03:21,831 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:03:21,833 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:03:22,665 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:03:22,665 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:03:23,471 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:03:23,472 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:03:23,473 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-26 21:03:23,533 - Total elapsed time: 12.2s\n",
      "[INFO] 2021-04-26 21:03:23,533 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 0/5 U\n",
      "Testando: 61_iris 5NN 0/5 H+U\n",
      "{'name': 'Lowest H, Highest U Sampling', 'measure': 'U+H', 'sortby': ['feature_Usefulness', 'feature_Harmfulness'], 'ascending': [False, True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:04:04,803 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:04:04,813 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:04:04,830 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:04:04,830 - Building metadata.\n",
      "[INFO] 2021-04-26 21:04:09,855 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:04:09,857 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:04:09,861 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:04:09,861 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:04:09,863 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:04:10,708 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:04:10,708 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:04:11,517 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:04:11,518 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:04:11,518 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-26 21:04:11,570 - Total elapsed time: 6.8s\n",
      "[INFO] 2021-04-26 21:04:11,570 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 0/5 H+U\n",
      "Testando: 61_iris 5NN 0/5 LSC\n",
      "{'name': 'Highest LSC Sampling', 'measure': 'LSC', 'sortby': ['feature_LSC'], 'ascending': [False]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:04:53,840 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:04:53,860 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:04:53,896 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:04:53,897 - Building metadata.\n",
      "[INFO] 2021-04-26 21:05:01,232 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-26 21:05:01,414 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:05:01,415 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:05:01,417 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:05:02,285 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:05:02,286 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:05:03,273 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:05:03,273 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:05:03,274 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-26 21:05:03,366 - Total elapsed time: 9.6s\n",
      "[INFO] 2021-04-26 21:05:03,367 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 0/5 LSC\n",
      "Testando: 61_iris 5NN 0/5 N2\n",
      "{'name': 'Lowest N2 Sampling', 'measure': 'N2', 'sortby': ['feature_N2'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:05:46,723 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:05:46,728 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:05:46,738 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:05:46,739 - Building metadata.\n",
      "[INFO] 2021-04-26 21:05:51,698 - Calculating measure 'N2'\n",
      "[INFO] 2021-04-26 21:05:51,950 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:05:51,950 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:05:51,952 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:05:52,761 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:05:52,762 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:05:53,565 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:05:53,565 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:05:53,566 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-26 21:05:53,627 - Total elapsed time: 6.9s\n",
      "[INFO] 2021-04-26 21:05:53,627 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 0/5 N2\n",
      "Testando: 61_iris 5NN 0/5 F3\n",
      "{'name': 'Lowest F3 Sampling', 'measure': 'F3', 'sortby': ['feature_F3'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:06:42,139 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:06:42,149 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:06:42,168 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:06:42,169 - Building metadata.\n",
      "[INFO] 2021-04-26 21:06:47,101 - Calculating measure 'F3'\n",
      "[INFO] 2021-04-26 21:06:47,275 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:06:47,276 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:06:47,278 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:06:48,103 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:06:48,103 - Evaluating testing fold #2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-26 21:06:48,916 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:06:48,916 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:06:48,916 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-26 21:06:48,970 - Total elapsed time: 6.8s\n",
      "[INFO] 2021-04-26 21:06:48,971 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 0/5 F3\n",
      "Testando: 61_iris 5NN 1/5 H\n",
      "{'name': 'Lowest Harmfulness Sampling', 'measure': 'Harmfulness', 'sortby': ['feature_Harmfulness'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:07:38,760 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:07:38,766 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:07:38,775 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:07:38,776 - Building metadata.\n",
      "[INFO] 2021-04-26 21:07:43,754 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:07:43,757 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:07:43,757 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:07:43,759 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:07:44,692 - Test fold mean accuracy: 0.9807692307692307\n",
      "[INFO] 2021-04-26 21:07:44,693 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:07:45,499 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:07:45,499 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:07:45,500 - Mean accuracy on test instances (iteration #1): 0.9615\n",
      "[INFO] 2021-04-26 21:07:45,566 - Total elapsed time: 6.8s\n",
      "[INFO] 2021-04-26 21:07:45,566 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 1/5 H\n",
      "Testando: 61_iris 5NN 1/5 U\n",
      "{'name': 'Highest Usefulness Sampling', 'measure': 'Usefulness', 'sortby': ['feature_Usefulness'], 'ascending': [False]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:08:26,298 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:08:26,302 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:08:26,317 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:08:26,317 - Building metadata.\n",
      "[INFO] 2021-04-26 21:08:31,054 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:08:31,058 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:08:31,059 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:08:31,060 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:08:31,872 - Test fold mean accuracy: 0.9807692307692307\n",
      "[INFO] 2021-04-26 21:08:31,872 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:08:32,568 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:08:32,568 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:08:32,569 - Mean accuracy on test instances (iteration #1): 0.9615\n",
      "[INFO] 2021-04-26 21:08:32,630 - Total elapsed time: 6.3s\n",
      "[INFO] 2021-04-26 21:08:32,630 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 1/5 U\n",
      "Testando: 61_iris 5NN 1/5 H+U\n",
      "{'name': 'Lowest H, Highest U Sampling', 'measure': 'U+H', 'sortby': ['feature_Usefulness', 'feature_Harmfulness'], 'ascending': [False, True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:09:20,890 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:09:20,896 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:09:20,916 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:09:20,917 - Building metadata.\n",
      "[INFO] 2021-04-26 21:09:25,789 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:09:25,795 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:09:25,798 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:09:25,798 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:09:25,800 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:09:26,628 - Test fold mean accuracy: 0.9807692307692307\n",
      "[INFO] 2021-04-26 21:09:26,629 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:09:27,324 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:09:27,324 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:09:27,326 - Mean accuracy on test instances (iteration #1): 0.9615\n",
      "[INFO] 2021-04-26 21:09:27,386 - Total elapsed time: 6.5s\n",
      "[INFO] 2021-04-26 21:09:27,386 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 1/5 H+U\n",
      "Testando: 61_iris 5NN 1/5 LSC\n",
      "{'name': 'Highest LSC Sampling', 'measure': 'LSC', 'sortby': ['feature_LSC'], 'ascending': [False]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:10:09,537 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:10:09,542 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:10:09,553 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:10:09,554 - Building metadata.\n",
      "[INFO] 2021-04-26 21:10:17,507 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-26 21:10:17,690 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:10:17,690 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:10:17,692 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:10:18,728 - Test fold mean accuracy: 0.9807692307692307\n",
      "[INFO] 2021-04-26 21:10:18,728 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:10:19,770 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:10:19,770 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:10:19,771 - Mean accuracy on test instances (iteration #1): 0.9615\n",
      "[INFO] 2021-04-26 21:10:19,848 - Total elapsed time: 10.3s\n",
      "[INFO] 2021-04-26 21:10:19,849 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 1/5 LSC\n",
      "Testando: 61_iris 5NN 1/5 N2\n",
      "{'name': 'Lowest N2 Sampling', 'measure': 'N2', 'sortby': ['feature_N2'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:11:08,577 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:11:08,584 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:11:08,618 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:11:08,618 - Building metadata.\n",
      "[INFO] 2021-04-26 21:11:16,200 - Calculating measure 'N2'\n",
      "[INFO] 2021-04-26 21:11:16,461 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:11:16,462 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:11:16,463 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:11:17,262 - Test fold mean accuracy: 0.9807692307692307\n",
      "[INFO] 2021-04-26 21:11:17,262 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:11:18,128 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:11:18,129 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:11:18,129 - Mean accuracy on test instances (iteration #1): 0.9615\n",
      "[INFO] 2021-04-26 21:11:18,189 - Total elapsed time: 9.6s\n",
      "[INFO] 2021-04-26 21:11:18,189 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 1/5 N2\n",
      "Testando: 61_iris 5NN 1/5 F3\n",
      "{'name': 'Lowest F3 Sampling', 'measure': 'F3', 'sortby': ['feature_F3'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:12:00,688 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:12:00,692 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:12:00,706 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:12:00,706 - Building metadata.\n",
      "[INFO] 2021-04-26 21:12:05,400 - Calculating measure 'F3'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-26 21:12:05,517 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:12:05,517 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:12:05,519 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:12:06,328 - Test fold mean accuracy: 0.9807692307692307\n",
      "[INFO] 2021-04-26 21:12:06,328 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:12:07,025 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:12:07,025 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:12:07,026 - Mean accuracy on test instances (iteration #1): 0.9615\n",
      "[INFO] 2021-04-26 21:12:07,084 - Total elapsed time: 6.4s\n",
      "[INFO] 2021-04-26 21:12:07,084 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 1/5 F3\n",
      "Testando: 61_iris 5NN 2/5 H\n",
      "{'name': 'Lowest Harmfulness Sampling', 'measure': 'Harmfulness', 'sortby': ['feature_Harmfulness'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:12:49,126 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:12:49,129 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:12:49,138 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:12:49,138 - Building metadata.\n",
      "[INFO] 2021-04-26 21:12:55,594 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:12:55,597 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:12:55,598 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:12:55,599 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:12:56,592 - Test fold mean accuracy: 0.9230769230769231\n",
      "[INFO] 2021-04-26 21:12:56,592 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:12:57,530 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:12:57,530 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:12:57,531 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:12:57,614 - Total elapsed time: 8.5s\n",
      "[INFO] 2021-04-26 21:12:57,614 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 2/5 H\n",
      "Testando: 61_iris 5NN 2/5 U\n",
      "{'name': 'Highest Usefulness Sampling', 'measure': 'Usefulness', 'sortby': ['feature_Usefulness'], 'ascending': [False]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:13:40,166 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:13:40,173 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:13:40,189 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:13:40,190 - Building metadata.\n",
      "[INFO] 2021-04-26 21:13:44,582 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:13:44,586 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:13:44,586 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:13:44,587 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:13:45,387 - Test fold mean accuracy: 0.9230769230769231\n",
      "[INFO] 2021-04-26 21:13:45,387 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:13:46,078 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:13:46,078 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:13:46,080 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:13:46,131 - Total elapsed time: 6.0s\n",
      "[INFO] 2021-04-26 21:13:46,132 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 2/5 U\n",
      "Testando: 61_iris 5NN 2/5 H+U\n",
      "{'name': 'Lowest H, Highest U Sampling', 'measure': 'U+H', 'sortby': ['feature_Usefulness', 'feature_Harmfulness'], 'ascending': [False, True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:14:24,516 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:14:24,521 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:14:24,532 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:14:24,532 - Building metadata.\n",
      "[INFO] 2021-04-26 21:14:28,972 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:14:28,974 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:14:28,981 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:14:28,981 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:14:28,983 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:14:29,679 - Test fold mean accuracy: 0.9230769230769231\n",
      "[INFO] 2021-04-26 21:14:29,679 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:14:30,378 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:14:30,378 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:14:30,379 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:14:30,449 - Total elapsed time: 6.0s\n",
      "[INFO] 2021-04-26 21:14:30,449 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 2/5 H+U\n",
      "Testando: 61_iris 5NN 2/5 LSC\n",
      "{'name': 'Highest LSC Sampling', 'measure': 'LSC', 'sortby': ['feature_LSC'], 'ascending': [False]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:15:14,471 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:15:14,478 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:15:14,490 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:15:14,490 - Building metadata.\n",
      "[INFO] 2021-04-26 21:15:18,875 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-26 21:15:18,964 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:15:18,964 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:15:18,965 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:15:19,656 - Test fold mean accuracy: 0.9230769230769231\n",
      "[INFO] 2021-04-26 21:15:19,656 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:15:20,352 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:15:20,352 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:15:20,353 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:15:20,396 - Total elapsed time: 5.9s\n",
      "[INFO] 2021-04-26 21:15:20,397 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 2/5 LSC\n",
      "Testando: 61_iris 5NN 2/5 N2\n",
      "{'name': 'Lowest N2 Sampling', 'measure': 'N2', 'sortby': ['feature_N2'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:15:58,832 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:15:58,847 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:15:58,875 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:15:58,875 - Building metadata.\n",
      "[INFO] 2021-04-26 21:16:06,824 - Calculating measure 'N2'\n",
      "[INFO] 2021-04-26 21:16:07,219 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:16:07,220 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:16:07,222 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:16:08,269 - Test fold mean accuracy: 0.9230769230769231\n",
      "[INFO] 2021-04-26 21:16:08,270 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:16:09,324 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:16:09,324 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:16:09,325 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:16:09,406 - Total elapsed time: 10.6s\n",
      "[INFO] 2021-04-26 21:16:09,406 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 2/5 N2\n",
      "Testando: 61_iris 5NN 2/5 F3\n",
      "{'name': 'Lowest F3 Sampling', 'measure': 'F3', 'sortby': ['feature_F3'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:16:54,936 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2021-04-26 21:16:54,966 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:16:55,009 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:16:55,010 - Building metadata.\n",
      "[INFO] 2021-04-26 21:17:06,249 - Calculating measure 'F3'\n",
      "[INFO] 2021-04-26 21:17:06,460 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:17:06,461 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:17:06,467 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:17:07,702 - Test fold mean accuracy: 0.9230769230769231\n",
      "[INFO] 2021-04-26 21:17:07,703 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:17:08,879 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:17:08,879 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:17:08,880 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:17:08,961 - Total elapsed time: 14.1s\n",
      "[INFO] 2021-04-26 21:17:08,962 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 2/5 F3\n",
      "Testando: 61_iris 5NN 3/5 H\n",
      "{'name': 'Lowest Harmfulness Sampling', 'measure': 'Harmfulness', 'sortby': ['feature_Harmfulness'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:17:53,396 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:17:53,404 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:17:53,435 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:17:53,435 - Building metadata.\n",
      "[INFO] 2021-04-26 21:18:00,249 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:18:00,255 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:18:00,255 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:18:00,256 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:18:01,088 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:18:01,088 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:18:01,904 - Test fold mean accuracy: 0.9038461538461539\n",
      "[INFO] 2021-04-26 21:18:01,904 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:18:01,905 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:18:01,955 - Total elapsed time: 8.6s\n",
      "[INFO] 2021-04-26 21:18:01,955 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 3/5 H\n",
      "Testando: 61_iris 5NN 3/5 U\n",
      "{'name': 'Highest Usefulness Sampling', 'measure': 'Usefulness', 'sortby': ['feature_Usefulness'], 'ascending': [False]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:18:40,190 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:18:40,196 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:18:40,210 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:18:40,211 - Building metadata.\n",
      "[INFO] 2021-04-26 21:18:44,694 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:18:44,701 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:18:44,702 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:18:44,703 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:18:45,507 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:18:45,508 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:18:46,318 - Test fold mean accuracy: 0.9038461538461539\n",
      "[INFO] 2021-04-26 21:18:46,318 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:18:46,318 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:18:46,373 - Total elapsed time: 6.2s\n",
      "[INFO] 2021-04-26 21:18:46,373 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 3/5 U\n",
      "Testando: 61_iris 5NN 3/5 H+U\n",
      "{'name': 'Lowest H, Highest U Sampling', 'measure': 'U+H', 'sortby': ['feature_Usefulness', 'feature_Harmfulness'], 'ascending': [False, True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:19:35,260 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:19:35,265 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:19:35,275 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:19:35,275 - Building metadata.\n",
      "[INFO] 2021-04-26 21:19:40,207 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:19:40,209 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:19:40,215 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:19:40,215 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:19:40,216 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:19:40,924 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:19:40,924 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:19:41,636 - Test fold mean accuracy: 0.9038461538461539\n",
      "[INFO] 2021-04-26 21:19:41,636 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:19:41,637 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:19:41,703 - Total elapsed time: 6.5s\n",
      "[INFO] 2021-04-26 21:19:41,703 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 3/5 H+U\n",
      "Testando: 61_iris 5NN 3/5 LSC\n",
      "{'name': 'Highest LSC Sampling', 'measure': 'LSC', 'sortby': ['feature_LSC'], 'ascending': [False]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:20:23,846 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:20:23,851 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:20:23,860 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:20:23,860 - Building metadata.\n",
      "[INFO] 2021-04-26 21:20:28,363 - Calculating measure 'LSC'\n",
      "[INFO] 2021-04-26 21:20:28,459 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:20:28,460 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:20:28,461 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:20:29,156 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:20:29,156 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:20:29,841 - Test fold mean accuracy: 0.9038461538461539\n",
      "[INFO] 2021-04-26 21:20:29,841 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:20:29,842 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:20:29,902 - Total elapsed time: 6.1s\n",
      "[INFO] 2021-04-26 21:20:29,902 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 3/5 LSC\n",
      "Testando: 61_iris 5NN 3/5 N2\n",
      "{'name': 'Lowest N2 Sampling', 'measure': 'N2', 'sortby': ['feature_N2'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:21:15,847 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:21:15,851 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:21:15,864 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:21:15,864 - Building metadata.\n",
      "[INFO] 2021-04-26 21:21:20,457 - Calculating measure 'N2'\n",
      "[INFO] 2021-04-26 21:21:20,668 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:21:20,668 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:21:20,672 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:21:21,467 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:21:21,468 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:21:22,253 - Test fold mean accuracy: 0.9038461538461539\n",
      "[INFO] 2021-04-26 21:21:22,254 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:21:22,255 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:21:22,313 - Total elapsed time: 6.5s\n",
      "[INFO] 2021-04-26 21:21:22,314 - Instance Hardness analysis finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passou: 61_iris 5NN 3/5 N2\n",
      "Testando: 61_iris 5NN 3/5 F3\n",
      "{'name': 'Lowest F3 Sampling', 'measure': 'F3', 'sortby': ['feature_F3'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:22:02,300 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:22:02,306 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:22:02,326 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:22:02,327 - Building metadata.\n",
      "[INFO] 2021-04-26 21:22:11,640 - Calculating measure 'F3'\n",
      "[INFO] 2021-04-26 21:22:11,927 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:22:11,927 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:22:11,930 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:22:13,576 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:22:13,576 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:22:15,174 - Test fold mean accuracy: 0.9038461538461539\n",
      "[INFO] 2021-04-26 21:22:15,175 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:22:15,176 - Mean accuracy on test instances (iteration #1): 0.9327\n",
      "[INFO] 2021-04-26 21:22:15,290 - Total elapsed time: 13.0s\n",
      "[INFO] 2021-04-26 21:22:15,290 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 3/5 F3\n",
      "Testando: 61_iris 5NN 4/5 H\n",
      "{'name': 'Lowest Harmfulness Sampling', 'measure': 'Harmfulness', 'sortby': ['feature_Harmfulness'], 'ascending': [True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:23:31,676 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:23:31,682 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:23:31,694 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:23:31,695 - Building metadata.\n",
      "[INFO] 2021-04-26 21:23:38,047 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:23:38,050 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:23:38,050 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:23:38,051 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:23:38,991 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:23:38,991 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:23:39,914 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:23:39,914 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:23:39,915 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-26 21:23:39,992 - Total elapsed time: 8.3s\n",
      "[INFO] 2021-04-26 21:23:39,992 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 4/5 H\n",
      "Testando: 61_iris 5NN 4/5 U\n",
      "{'name': 'Highest Usefulness Sampling', 'measure': 'Usefulness', 'sortby': ['feature_Usefulness'], 'ascending': [False]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:24:28,738 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:24:28,743 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:24:28,753 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:24:28,753 - Building metadata.\n",
      "[INFO] 2021-04-26 21:24:34,320 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:24:34,346 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:24:34,347 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:24:34,349 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:24:35,199 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:24:35,199 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:24:35,999 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:24:35,999 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:24:36,000 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-26 21:24:36,053 - Total elapsed time: 7.3s\n",
      "[INFO] 2021-04-26 21:24:36,053 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 4/5 U\n",
      "Testando: 61_iris 5NN 4/5 H+U\n",
      "{'name': 'Lowest H, Highest U Sampling', 'measure': 'U+H', 'sortby': ['feature_Usefulness', 'feature_Harmfulness'], 'ascending': [False, True]}\n",
      "run 'pyhard --help' to see all options.\n",
      "[INFO] 2021-04-26 21:25:47,087 - Configuration file: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/config.yaml'\n",
      "[INFO] 2021-04-26 21:25:47,092 - Reading input dataset: '/mnt/c/Users/ahmou/OneDrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/data.csv'\n",
      "[INFO] 2021-04-26 21:25:47,135 - Type of problem: 'classification'\n",
      "[INFO] 2021-04-26 21:25:47,136 - Building metadata.\n",
      "[INFO] 2021-04-26 21:25:53,545 - Calculating measure 'Harmfulness'\n",
      "[INFO] 2021-04-26 21:25:53,548 - Calculating measure 'Usefulness'\n",
      "[INFO] 2021-04-26 21:25:53,551 - Assessing performance of classifier 'random_forest'\n",
      "[INFO] 2021-04-26 21:25:53,551 - Estimating instance performance...\n",
      "[INFO] 2021-04-26 21:25:53,553 - Evaluating testing fold #1\n",
      "[INFO] 2021-04-26 21:25:54,353 - Test fold mean accuracy: 0.9423076923076923\n",
      "[INFO] 2021-04-26 21:25:54,354 - Evaluating testing fold #2\n",
      "[INFO] 2021-04-26 21:25:55,410 - Test fold mean accuracy: 0.9615384615384616\n",
      "[INFO] 2021-04-26 21:25:55,411 - Iteration 1/1 completed.\n",
      "[INFO] 2021-04-26 21:25:55,412 - Mean accuracy on test instances (iteration #1): 0.9519\n",
      "[INFO] 2021-04-26 21:25:55,459 - Total elapsed time: 8.4s\n",
      "[INFO] 2021-04-26 21:25:55,459 - Instance Hardness analysis finished.\n",
      "Passou: 61_iris 5NN 4/5 H+U\n",
      "Testando: 61_iris 5NN 4/5 LSC\n",
      "{'name': 'Highest LSC Sampling', 'measure': 'LSC', 'sortby': ['feature_LSC'], 'ascending': [False]}\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/bin/pyhard\", line 5, in <module>\n",
      "    from pyhard.cli import cli\n",
      "  File \"/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/pyhard/__init__.py\", line 3, in <module>\n",
      "    from .measures import ClassificationMeasures\n",
      "  File \"/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/pyhard/measures.py\", line 12, in <module>\n",
      "    from .base import BaseMeasures\n",
      "  File \"/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/pyhard/base.py\", line 6, in <module>\n",
      "    import holoviews as hv\n",
      "  File \"/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/holoviews/__init__.py\", line 11, in <module>\n",
      "    from . import util                                       # noqa (API import)\n",
      "  File \"/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/holoviews/util/__init__.py\", line 15, in <module>\n",
      "    from ..core import (\n",
      "  File \"/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/holoviews/core/__init__.py\", line 3, in <module>\n",
      "    from .boundingregion import *  # noqa (API import)\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 906, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1280, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1252, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1368, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1408, in _fill_cache\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'feature_LSC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/importing_libraries.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mph_strategy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpyhard_strategies_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testando: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_bag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mph_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyhard_strategies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_bag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mph_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mtotal_performance_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/importing_libraries.py\u001b[0m in \u001b[0;36mpyhard_strategies\u001b[0;34m(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost, strategy)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sortby'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ascending'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instances'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_bag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   5453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5454\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5455\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5457\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1682\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feature_LSC'"
     ]
    }
   ],
   "source": [
    "pyhard_strategies_names = ['H','U','H+U','LSC','N2','F3']\n",
    "\n",
    "for ds in datasets:\n",
    "    for classifier in classifiers:\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in range(n_splits):\n",
    "            for ph_strategy in pyhard_strategies_names:\n",
    "                tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" \" + ph_strategy)\n",
    "                result = pyhard_strategies(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost, ph_strategy)\n",
    "                result['dataset'] = ds[:-5]\n",
    "                total_performance_history.append(result)\n",
    "                tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" \" + ph_strategy)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_pyhard_measure(measure='LSC'):\n",
    "    import yaml\n",
    "    with open(r'config-template.yaml') as file:\n",
    "        configs_list = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "        if measure == 'LSC':\n",
    "            configs_list['measures_list'] = ['LSC']\n",
    "        elif measure == 'Harmfulness':\n",
    "            configs_list['measures_list'] = ['Harmfulness']\n",
    "        elif measure == 'Usefulness':\n",
    "            configs_list['measures_list'] = ['Usefulness']\n",
    "        elif measure == 'U+H':\n",
    "            configs_list['measures_list'] = ['Harmfulness','Usefulness']\n",
    "        elif measure == 'N2':\n",
    "            configs_list['measures_list'] = ['N2']\n",
    "        elif measure == 'F3':\n",
    "            configs_list['measures_list'] = ['F3']\n",
    "\n",
    "    with open(r'config.yaml', 'w') as file:\n",
    "        yaml.dump(configs_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_dataset(dataset = \"iris\", n_splits = 5):\n",
    "    \n",
    "    # Futuramente essa etapa será ajustada para receber qualquer dataset (ou lista com datasets)\n",
    "    if (dataset == \"iris\"):\n",
    "        data = load_iris()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "    \n",
    "    if (dataset == \"wine\"):\n",
    "        data = load_wine()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    if (dataset == \"digits\"):\n",
    "        data = load_digits()\n",
    "        X_raw = data['data']\n",
    "        y_raw = data['target']\n",
    "        \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_oml_dataset(dataset_id, n_splits = 5):\n",
    "    data = openml.datasets.get_dataset(dataset_id)\n",
    "    \n",
    "    X_raw, y_raw, categorical_indicator, attribute_names = data.get_data(\n",
    "    dataset_format=\"array\", target=data.default_target_attribute)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_raw)\n",
    "    y_raw = le.transform(y_raw)\n",
    "    \n",
    "    X_raw = np.nan_to_num(X_raw)\n",
    "    \n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    \n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_arff_dataset(dataset, n_splits = 5):\n",
    "   \n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    \n",
    "    data = arff.loadarff('datasets/luis/' + dataset)\n",
    "    data = pd.DataFrame(data[0])\n",
    "\n",
    "    X_raw = data[data.columns[:-1]].to_numpy()\n",
    "    y_raw = data[data.columns[-1]].to_numpy()\n",
    "    \n",
    "    lex = preprocessing.OrdinalEncoder()\n",
    "    lex.fit(X_raw)\n",
    "    X_raw = lex.transform(X_raw)\n",
    "        \n",
    "    ley = preprocessing.LabelEncoder()\n",
    "    ley.fit(y_raw)\n",
    "    y_raw = ley.transform(y_raw)\n",
    "    \n",
    "    # cross validation bags\n",
    "    data_cv = StratifiedShuffleSplit(n_splits= n_splits, train_size=0.7, random_state=0) #n_splits\n",
    "    data_cv.get_n_splits(X_raw,y_raw)\n",
    "    \n",
    "    # extraindo ids do data_cv\n",
    "    idx_data = []\n",
    "    for train_index, test_index in data_cv.split(X_raw, y_raw):\n",
    "            idx_data.append([train_index, test_index])\n",
    "\n",
    "    return X_raw, y_raw, idx_data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_classifier(classifier = '5NN'):\n",
    "    \n",
    "    if (classifier == '5NN'):\n",
    "        return KNeighborsClassifier(5)\n",
    "    elif (classifier == 'C4.5'):\n",
    "        return tree.DecisionTreeClassifier()\n",
    "    elif (classifier == 'NB'):\n",
    "        return GaussianNB()\n",
    "    elif (classifier == 'SVM'):\n",
    "        return SVC(probability=True, gamma='auto')\n",
    "    elif (classifier == 'RF'):\n",
    "        return RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datasets(dataset):\n",
    "    \n",
    "    data = arff.loadarff('./datasets/luis/' + dataset)\n",
    "    metadata = data[1]\n",
    "    data = pd.DataFrame(data[0])\n",
    "    \n",
    "    instances = len(data)\n",
    "    classes = len(data.iloc[:,-1].value_counts())\n",
    "    attributes = len(data.columns)- 1\n",
    "    nominal_attributes = str(metadata).count(\"nominal\")\n",
    "    \n",
    "    proportion = data.iloc[:,-1].value_counts()\n",
    "    proportion = proportion.map(lambda x: round(x/instances*100,2))\n",
    "\n",
    "    majority = max(proportion)\n",
    "    minority = min(proportion)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"name\": dataset[:-5],\n",
    "        \"instances\": instances,\n",
    "        \"classes\": classes,\n",
    "        \"attributes\": attributes,\n",
    "        \"nominal attributes\": nominal_attributes,\n",
    "        \"majority\": majority,\n",
    "        \"minority\": minority\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Teste\n",
    "\n",
    "total_performance_history = []\n",
    "\n",
    "classifiers = ['SVM']\n",
    "ds = '3_kr-vs-kp.arff'\n",
    "for classifier in classifiers:\n",
    "    X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "    for idx_bag in range(n_splits):\n",
    "        print(ds, classifier, \" \", idx_bag, \" \", n_splits, \" uncertain_sampling\")\n",
    "        result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "        result['dataset'] = ds\n",
    "        total_performance_history.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('./datasets/luis')\n",
    "classifiers = ['5NN', 'C4.5', 'NB','RF']\n",
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['61_iris.arff']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/importing_datasets.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/importing_datasets.py\u001b[0m in \u001b[0;36mfetch_datasets\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadarff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./datasets/luis/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arff' is not defined"
     ]
    }
   ],
   "source": [
    "metadata = []\n",
    "\n",
    "for ds in datasets:\n",
    "    metadata.append(fetch_datasets(ds))\n",
    "\n",
    "metadata = pd.DataFrame.from_dict(metadata)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for ds in tqdm(datasets):\n",
    "    for classifier in tqdm(classifiers):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" uncertain_sampling\")\n",
    "            result = uncertain_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" random sampling\")\n",
    "            result = random_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" query_by_committee\")\n",
    "            result = query_by_committee(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" exp error reduction\")\n",
    "            result = exp_error_reduction(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "        for idx_bag in range(n_splits):\n",
    "            #print(ds[:-5], \" \", classifier, \" \", idx_bag, \" \", n_splits, \" exp model change\")\n",
    "            result = exp_model_change(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Classifier:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Bag:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                    \n",
      "                                                 \n",
      "\n",
      "Bag:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Dataset:   0%|          | 0/1 [00:00<?, ?it/s]/s]\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                    \n",
      "                                                 \n",
      "\n",
      "Bag:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Dataset:   0%|          | 0/1 [00:00<?, ?it/s]/s]\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                    \n",
      "                                                 \n",
      "\n",
      "Bag:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Dataset:   0%|          | 0/1 [00:00<?, ?it/s]/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando: 61_iris 5NN 0/5 uncertain_sampling\n",
      "Passou: 61_iris 5NN 1/5 uncertain_sampling\n",
      "Testando: 61_iris 5NN 0/5 random_sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A                                    \n",
      "                                                 \n",
      "\n",
      "Bag:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Dataset:   0%|          | 0/1 [00:00<?, ?it/s]/s]\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                    \n",
      "                                                 \n",
      "\n",
      "Bag:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Dataset:   0%|          | 0/1 [00:00<?, ?it/s]/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passou: 61_iris 5NN 1/5 random_sampling\n",
      "Testando: 61_iris 5NN 0/5 query_by_committee\n",
      "\t Size of X_pool: 97\n",
      "\t Size of X_pool: 91\n",
      "\t Size of X_pool: 85\n",
      "\t Size of X_pool: 79\n",
      "\t Size of X_pool: 73\n",
      "\t Size of X_pool: 67\n",
      "\t Size of X_pool: 61\n",
      "\t Size of X_pool: 55\n",
      "\t Size of X_pool: 49\n",
      "\t Size of X_pool: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A                                    \n",
      "                                                 \n",
      "\n",
      "Bag:   0%|          | 0/5 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "Dataset:   0%|          | 0/1 [00:01<?, ?it/s]/s]\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                    \n",
      "                                                 \n",
      "\n",
      "Bag:   0%|          | 0/5 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "Dataset:   0%|          | 0/1 [00:01<?, ?it/s]/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passou: 61_iris 5NN 1/5 query_by_committee\n",
      "Testando: 61_iris 5NN 0/5 exp_error_reduction\n",
      "\t Size of X_pool: 45\n",
      "\t Size of X_pool: 40\n",
      "\t Size of X_pool: 35\n",
      "\t Size of X_pool: 30\n",
      "\t Size of X_pool: 25\n",
      "\t Size of X_pool: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bag:   0%|          | 0/5 [00:03<?, ?it/s]\n",
      "Classifier:   0%|          | 0/4 [00:03<?, ?it/s]\n",
      "Dataset:   0%|          | 0/1 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Size of X_pool: 15\n",
      "\t Size of X_pool: 10\n",
      "\t Size of X_pool: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/importing_libraries.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testando: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_bag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mtotal_performance_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/importing_libraries.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/active_learning/data/act_len labs/pyhard/importing_libraries.py\u001b[0m in \u001b[0;36mexp_error_reduction\u001b[0;34m(X_raw, y_raw, idx_data, idx_bag, classifier, init_size, cost)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0my_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_error_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mexp_er_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mperformance_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_er_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/modAL/models/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, **score_kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mscore\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \"\"\"\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscore_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \"\"\"\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mdata\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/ahmou/Onedrive/Documentos/ubuntu_wd/act_len/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    670\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[0;32m--> 672\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "functions = [\"uncertain_sampling\",\"random_sampling\",\"query_by_committee\",\"exp_error_reduction\",\"exp_model_change\"]\n",
    "parameters = \"(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\"\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in tqdm(classifiers,  desc =\"Classifier\"):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in tqdm(range(n_splits),  desc =\"Bag\"):\n",
    "            for func in functions:\n",
    "                tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag+1) + \"/\" + str(n_splits) + \" \" + func)\n",
    "                result = eval(func+parameters)\n",
    "                result['dataset'] = ds[:-5]\n",
    "                total_performance_history.append(result)\n",
    "                tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag+1) + \"/\" + str(n_splits) + \" \" + func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['RF']\n",
    "total_performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tqdm_datasets = tqdm(datasets, desc=\" Dataset: \"+ str(ds[:-5]))\n",
    "#tqdm_classifier = tqdm(classifiers, desc=\"Classifier: \"+ str(classifier))\n",
    "\n",
    "for ds in tqdm(datasets,  desc =\"Dataset\"):\n",
    "    for classifier in tqdm(classifiers,  desc =\"Classifier\"):\n",
    "        X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "        \n",
    "\n",
    "        #para cada i em idx_bag (\"n_splits\") (1 a 5)\n",
    "        for idx_bag in tqdm(range(n_splits),  desc =\"Bag\"):\n",
    "\n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_f3_sampling\")\n",
    "            result = lowest_f3_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_f3_sampling\")\n",
    "            \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_lsc_sampling\")\n",
    "            result = highest_lsc_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_lsc_sampling\")\n",
    "            \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_usefulness_sampling\")\n",
    "            result = highest_usefulness_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" highest_usefulness_sampling\")\n",
    "           \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_harmfulness_sampling\")\n",
    "            result = lowest_harmfulness_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_harmfulness_sampling\")\n",
    "            \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_h_highest_u_sampling\")\n",
    "            result = lowest_h_highest_u_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_h_highest_u_sampling\")\n",
    "  \n",
    "            tqdm.write(\"Testando: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_n2_sampling\")\n",
    "            result = lowest_n2_sampling(deepcopy(X_raw), deepcopy(y_raw), idx_data, idx_bag, classifier, k, cost)\n",
    "            result['dataset'] = ds[:-5]\n",
    "            total_performance_history.append(result)\n",
    "            tqdm.write(\"Passou: \" + str(ds[:-5]) + \" \" + str(classifier) + \" \" + str(idx_bag) + \"/\" + str(n_splits) + \" lowest_n2_sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_performance_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(total_performance_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('performance_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Strategy != \"Query by Committee\"].sort_values('performance_history', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Strategy == \"Expected Error Reduction\"].sort_values('time_elapsed', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df,\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=5), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df[(df.Strategy != \"Uncertain Sampling\") & (df.Strategy != \"Query by Committee\")],\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=3), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data= df[(df.Strategy == \"Uncertain Sampling\") | (df.Strategy == \"Query by Committee\")],\n",
    "    x=\"performance_history\", y=\"time_elapsed\",\n",
    "    hue=\"Strategy\", size=\"sample_size\",\n",
    "    palette=sns.color_palette(n_colors=2), sizes=(100, 300), alpha=0.3\n",
    ")\n",
    "g.ax.xaxis.grid(True, \"minor\", linewidth=.25)\n",
    "g.ax.yaxis.grid(True, \"minor\", linewidth=.25)\n",
    "_ = g.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "p_bar = tqdm(datalist)\n",
    "for dataset_id in p_bar:\n",
    "    X_raw, y_raw, idx_data, dataset_name = which_oml_dataset(dataset_id)\n",
    "    p_bar.set_description(f'\"{dataset_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"1465_breast-tissue.arff\"\n",
    "\n",
    "X_raw, y_raw, idx_data, dataset_name = which_arff_dataset(ds)\n",
    "   \n",
    "from modAL.uncertainty import classifier_uncertainty\n",
    "\n",
    "print(len(np.unique(y_raw)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_bag][TRAIN]], y_raw[idx_data[idx_bag][TRAIN]], train_size= len(np.unique(y_raw)), stratify = y_raw[idx_data[idx_bag][TRAIN]])\n",
    "print(y_train)\n",
    "\n",
    "learner = ActiveLearner (\n",
    "    estimator= which_classifier(classifier), #cls,\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    X_training = X_train, y_training = y_train # AL AJUSTA O CLASSIFIER \n",
    ")\n",
    "\n",
    "uncertain_sample_score = learner.score(X_test, y_test)\n",
    "\n",
    "total_of_samples = 1\n",
    "while (total_of_samples != cost):\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, train_size=0.03)\n",
    "\n",
    "    idx = np.random.choice(range(len(idx_data[idx_bag][TRAIN])), size=init_size, replace=False)\n",
    "    X_train, y_train = X_raw[idx_data[idx_bag][TRAIN][idx]], y_raw[idx_data[idx_bag][TRAIN][idx]]\n",
    "\n",
    "    if classifier_uncertainty(learner, X_train[0].reshape(1,-1)) > 0.2:\n",
    "        #print(\"IF\", learner.score(X_test, y_test))\n",
    "        learner.teach(X_train, y_train)\n",
    "        uncertain_sample_score = learner.score(X_test, y_test)\n",
    "        performance_history.append(uncertain_sample_score)\n",
    "    total_of_samples = total_of_samples + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size= len(np.unique(y_raw)) + init_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
